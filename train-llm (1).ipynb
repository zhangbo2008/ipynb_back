{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#============使用t4*2 这gpu才行. 他支持int4模式.\n\n!git clone  https://github.com/zhangbo2008/train_llm\n%cd train_llm\n!pip install loguru==0.7.0\n!pip install peft\n!pip install bitsandbytes==0.39.0\n!pip install accelerate==0.21.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-15T06:31:17.237008Z","iopub.execute_input":"2023-08-15T06:31:17.237371Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Cloning into 'train_llm'...\nremote: Enumerating objects: 64, done.\u001b[K\nremote: Counting objects: 100% (64/64), done.\u001b[K\nremote: Compressing objects: 100% (44/44), done.\u001b[K\nremote: Total 64 (delta 17), reused 64 (delta 17), pack-reused 0\u001b[K\nReceiving objects: 100% (64/64), 1.36 MiB | 10.09 MiB/s, done.\nResolving deltas: 100% (17/17), done.\n/kaggle/working/train_llm\nCollecting loguru==0.7.0\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"#-=========参数写到这里得了:\narg='train_args/qlora/chatglm2-6b-sft-qlora.json'\n\n\n\n\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    set_seed,\n    HfArgumentParser,\n    TrainingArguments,\n    AutoModelForCausalLM\n)\nimport argparse\nfrom loguru import logger\nimport os\nfrom os.path import join\nimport torch\nimport bitsandbytes as bnb\nfrom collections import defaultdict\n\nfrom component.collator import SFTDataCollator\nfrom component.dataset import SFTDataset, ChatGLM2SFTDataset\nfrom component.argument import QLoRAArguments\nfrom component.trainer import LoRATrainer\nfrom component.loss import TargetLMLoss\n\n\ndef verify_model_dtype(model):\n    \"\"\"\n    查看模型种各种类型的参数的情况\n    \"\"\"\n    dtype2param_num = defaultdict(int)  # 每种数据类型的参数量\n    dtype2param_name = defaultdict(list)  # 每种数据类型的参数名称\n    dtype2trainable_param_num = defaultdict(int)  # 每种数据类型参与训练的参数量\n    dtype2trainable_param_name = defaultdict(list)  # 每种数据类型参与训练的参数名称\n    for name, p in model.named_parameters():\n        dtype = p.dtype\n        dtype2param_num[dtype] += p.numel() #计算张量中元素的数量.\n        dtype2param_name[dtype].append(name)\n        if p.requires_grad:\n            dtype2trainable_param_num[dtype] += p.numel()\n            dtype2trainable_param_name[dtype].append(name)\n    # 统计全部参数中，各种类型参数分布\n    total = 0\n    print('verify all params of the model')\n    for k, v in dtype2param_num.items():\n        total += v\n    for k, v in dtype2param_num.items():\n        print(k, v, v / total)\n    ; for k, v in dtype2trainable_param_name.items():\n    ;     print(k, v)\n\n    ; print()\n    # 统计可训练参数中，各种类型参数分布\n    print('verify trainable params the model')\n    total_trainable = 0\n    for k, v in dtype2trainable_param_num.items():\n        total_trainable += v\n    for k, v in dtype2trainable_param_num.items():\n        print(k, v, v / total_trainable)\n    for k, v in dtype2trainable_param_num.items():\n        print(k, v)\n\n\ndef find_all_linear_names(model):\n    \"\"\"\n    找出所有全连接层，为所有全连接添加adapter\n    \"\"\"\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\n\n\n\n\n\nif 1:\n\n    ar=r'train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json'\n\n    # 进行一些配置和检查\n\n    # 加载各种组件\n#     parser = argparse.ArgumentParser('')\n#     parser.add_argument(\"--train_args_file\", type=str, default='train_args/baichuan-sft-qlora.json', help=\"\")\n#     args = parser.parse_args()\n#     args.train_args_file=arg\n    train_args_file = arg\n    # 读取训练的参数配置\n    parser = HfArgumentParser((QLoRAArguments, TrainingArguments))\n    # 解析得到自定义参数，以及自带参数\n    args, training_args = parser.parse_json_file(json_file=train_args_file)\n    # 创建输出目录\n    if not os.path.exists(training_args.output_dir):\n        os.makedirs(training_args.output_dir)\n    # logger.add(join(training_args.output_dir, 'train.log'))\n    # logger.info(\"train_args:{}\".format(training_args))\n    # 设置随机种子\n    set_seed(training_args.seed)\n\n\n\n\n\n\n    #========设置多卡.\n    logger.info('Initializing components...')\n    # 下面的设置至关重要，否则无法多卡训练\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    training_args.ddp_find_unused_parameters = False\n    device_map = \"auto\"\n    # if we are in a distributed setting, we need to set the device map and max memory per device\n    if os.environ.get('LOCAL_RANK') is not None:\n        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n        device_map = {'': local_rank}\n        \n        \n\n\n\n\n        \n    # 加载模型\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        device_map=device_map,\n        load_in_4bit=True,           #########???????????????这么加载训练精度很低吧..... # 这里表示我们把官网的16位加载过来后转化为我们的4bit精度.\n        torch_dtype=torch.float16,      #######这个地方表示权重在官网上存储使用的是16位. \n        trust_remote_code=True,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n        ),\n    )\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    # 加载tokenzier\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=True,\n        # llama不支持fast\n        use_fast=False if model.config.model_type == 'llama' else True\n    )\n    # QWenTokenizer比较特殊，pad_token_id、bos_token_id、eos_token_id均为None。eod_id对应的token为<|endoftext|>\n    if tokenizer.__class__.__name__ == 'QWenTokenizer':\n        tokenizer.pad_token_id = tokenizer.eod_id\n        tokenizer.bos_token_id = tokenizer.eod_id\n        tokenizer.eos_token_id = tokenizer.eod_id\n    # ChatGLMTokenizer不需要设置，仅设置其他tokenizer\n    elif tokenizer.__class__.__name__ != 'ChatGLMTokenizer':\n        assert tokenizer.eos_token_id is not None\n        assert tokenizer.bos_token_id is not None\n        tokenizer.pad_token_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    # casts all the non int8 modules to full precision (fp32) for stability\n    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n    print(model)\n    print(111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111)\n    print(f'memory footprint of model: {model.get_memory_footprint()/(1024*1024*1024)} GB')\n    # 找到所有需要插入adapter的全连接层\n    target_modules = find_all_linear_names(model)\n    # 初始化lora配置===========把模型变成lora\n    config = LoraConfig(\n        r=args.lora_rank,\n        lora_alpha=args.lora_alpha,\n        target_modules=target_modules,\n        lora_dropout=args.lora_dropout,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n    model = get_peft_model(model, config)\n    print('打印训练参数的数量.')\n    model.print_trainable_parameters()\n    model.config.torch_dtype = torch.float32\n\n\n\n\n\n\n    # 查看模型种各种类型的参数的情况\n    verify_model_dtype(model)\n\n    # 初始化损失函数 ########################!!!!!!!!!!!!!!!!!!!!!\n    loss_func = TargetLMLoss(ignore_index=-100)\n\n    # 指加载训练集\n    if model.config.model_type == 'chatglm':\n        train_dataset = ChatGLM2SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    else:\n        train_dataset = SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    data_collator = SFTDataCollator(tokenizer, args.max_seq_length)\n\n    # 初始化Trainer\n    trainer = LoRATrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        # tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_loss=loss_func\n    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if 1:\n    logger.info(\"*** starting training ***\")\n    print(trainer.args)\n    train_result = trainer.train()\n    # 保存最好的checkpoint\n    final_save_path = join(training_args.output_dir, 'final')\n    trainer.save_model(final_save_path)  # Saves the tokenizer too\n    # 保存训练指标\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}