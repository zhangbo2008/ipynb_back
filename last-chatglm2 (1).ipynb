{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchkeras peft","metadata":{"execution":{"iopub.status.busy":"2023-08-17T01:31:14.929443Z","iopub.execute_input":"2023-08-17T01:31:14.930978Z","iopub.status.idle":"2023-08-17T01:31:29.350447Z","shell.execute_reply.started":"2023-08-17T01:31:14.930932Z","shell.execute_reply":"2023-08-17T01:31:29.349214Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torchkeras\n  Downloading torchkeras-3.9.3-py3-none-any.whl (6.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hCollecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from torchkeras) (0.20.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchkeras) (4.65.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: torchkeras, peft\nSuccessfully installed peft-0.4.0 torchkeras-3.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n############å…ˆæ˜¯æ‰€æœ‰çš„é…ç½®å‚æ•°.\n\nimport os\n\n# å¯¼å…¥å¸¸ç”¨æ¨¡å—\nimport numpy as np\n\nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset,DataLoader \n\n\n# é…ç½®å‚æ•°\nfrom argparse import Namespace\ncfg = Namespace()\n\n#dataset\ncfg.prompt_column = 'prompt'\ncfg.response_column = 'response'\ncfg.history_column =None\ncfg.source_prefix = '' #æ·»åŠ åˆ°æ¯ä¸ªpromptå¼€å¤´çš„å‰ç¼€å¼•å¯¼è¯­\n\ncfg.max_source_length = 128 \ncfg.max_target_length = 128\n\n#model\ncfg.model_name_or_path = 'THUDM/chatglm2-6b'  #è¿œç¨‹'THUDM/chatglm-6b' \ncfg.quantization_bit = None #ä»…ä»…é¢„æµ‹æ—¶å¯ä»¥é€‰ 4 or 8 \n\n\n#train\ncfg.epochs = 100 \ncfg.lr = 5e-3\ncfg.batch_size = 2\ncfg.gradient_accumulation_steps = 1 #æ¢¯åº¦ç´¯ç§¯\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available()else \"cpu\") \n\n\n\n\n\n\n#==========å®šä¹‰çŸ¥è¯†æ ·æœ¬.######å…ˆå¤„ç†æˆ‘ä»¬çš„æ•°æ®.\nfrom torch.utils.data import Dataset,DataLoader \nimport transformers\nfrom transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq\ntokenizer = AutoTokenizer.from_pretrained(\n    cfg.model_name_or_path, trust_remote_code=True)\nimport transformers\nfrom transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq\n\n\nimport pandas as pd \nkeyword = 'æ¢¦ä¸­æƒ…ç‚‰'\n\ndescription = '''æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆå·¥å…·ã€‚\ntorchkerasæ˜¯ä¸€ä¸ªä¸‰å¥½ç‚¼ä¸¹ç‚‰ï¼šå¥½çœ‹ï¼Œå¥½ç”¨ï¼Œå¥½æ”¹ã€‚\nå¥¹æœ‰torchçš„çµåŠ¨ï¼Œä¹Ÿæœ‰kerasçš„ä¼˜é›…ï¼Œå¹¶ä¸”å¥¹çš„ç¾ä¸½ï¼Œæ— ä¸ä¼¦æ¯”ã€‚\næ‰€ä»¥å¥¹çš„ä½œè€…ä¸€ä¸ªæœ‰æ¯…åŠ›çš„åƒè´§ç»™å¥¹å–äº†ä¸€ä¸ªåˆ«åå«åšæ¢¦ä¸­æƒ…ç‚‰ã€‚'''\n\n\n\n\n#å¯¹promptä½¿ç”¨ä¸€äº›ç®€å•çš„æ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ”¶æ•›ã€‚\ndef get_prompt_list(keyword):\n    return [f'{keyword}', \n            f'ä½ çŸ¥é“{keyword}å—?',\n            f'{keyword}æ˜¯ä»€ä¹ˆï¼Ÿ',\n            f'ä»‹ç»ä¸€ä¸‹{keyword}',\n            f'ä½ å¬è¿‡{keyword}å—?',\n            f'å•¥æ˜¯{keyword}ï¼Ÿ',\n            f'{keyword}æ˜¯ä½•ç‰©ï¼Ÿ',\n            f'ä½•ä¸º{keyword}ï¼Ÿ',\n           ]\n\ndata =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\ndfdata = pd.DataFrame(data)\n\n\n\n\n\nimport datasets \n#è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸€æ ·\nds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)\n#è¿™æ˜¯æ”¯æŒ historyåˆ—å¤„ç†ï¼Œå¹¶ä¸”æŒ‰ç…§batché¢„å¤„ç†æ•°æ®çš„æ–¹æ³•ã€‚\n\ndef preprocess(examples):\n    max_seq_length = cfg.max_source_length + cfg.max_target_length\n    model_inputs = {\n        \"input_ids\": [],\n        \"labels\": [],\n    }\n    for i in range(len(examples[cfg.prompt_column])):\n        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n\n            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n            prompt = tokenizer.build_prompt(query, history)\n\n            prompt = cfg.source_prefix + prompt\n            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n                                     max_length=cfg.max_source_length)\n            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n                                     max_length=cfg.max_target_length)\n\n            context_length = len(a_ids)\n            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n\n            pad_len = max_seq_length - len(input_ids)\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            labels = labels + [tokenizer.pad_token_id] * pad_len\n            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n            model_inputs[\"input_ids\"].append(input_ids)\n            model_inputs[\"labels\"].append(labels)\n    return model_inputs\n\n\nds_train = ds_train_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_train_raw.column_names\n)\n\nds_val = ds_val_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_val_raw.column_names\n)\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=None,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=None,\n    padding=False\n)\n\ndl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = True, collate_fn = data_collator \n                     )\ndl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = False, collate_fn = data_collator \n                     )\n\n\n\n\nprint(len(dl_train))\n\n\n\n\nconfig = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)\n\n\n\nmodel = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,\n                                  trust_remote_code=True, device_map='auto').half() #==========16ä½ç”¨æ¥gpuè®­ç»ƒ.è®¾å¤‡ä¸€å®šå†™auto,è‡ªåŠ¨é…ç½®æ˜¾å¡å’Œå†…å­˜.\n\n#å…ˆé‡åŒ–ç˜¦èº«  =======æµ‹è¯•æ—¶å€™å¯ä»¥ç”¨è¿™ä¸ª. ä¸è§ä¸€å¼€å¯.é™¤éé…ç½® ç‰¹åˆ«å·®.\nif cfg.quantization_bit is not None:\n    print(f\"Quantized to {cfg.quantization_bit} bit\")\n    model = model.quantize(cfg.quantization_bit)\n    \n#å†ç§»åŠ¨åˆ°GPUä¸Š\n# model = model.cuda();\n\n\n# # é€šè¿‡æ³¨å†Œjupyteré­”æ³•å‘½ä»¤å¯ä»¥å¾ˆæ–¹ä¾¿åœ°åœ¨jupyterä¸­æµ‹è¯•ChatGLM \n# from torchkeras.chat import ChatGLM \n# chatglm = ChatGLM(model,tokenizer)\n\nprint('æµ‹è¯•ä¸€ä¸‹æ˜¯å¦åŠ è½½æˆåŠŸ')\nresponse,history= model.chat(tokenizer,query='ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ä»€ä¹ˆï¼Ÿ',history=[])\nprint(response)\n\n\n\n\n#å®šä¹‰ä¸€æ¡çŸ¥è¯†æ ·æœ¬~#===========================\n\n\nfrom peft import get_peft_model, AdaLoraConfig, TaskType\n\n#è®­ç»ƒæ—¶èŠ‚çº¦GPUå ç”¨\nmodel.config.use_cache=False\nmodel.supports_gradient_checkpointing = True  #\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\npeft_config = AdaLoraConfig(\n    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n)\n\npeft_model = get_peft_model(model, peft_config)\n\npeft_model.is_parallelizable = True\npeft_model.model_parallel = True\npeft_model.print_trainable_parameters()\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T01:31:29.353742Z","iopub.execute_input":"2023-08-17T01:31:29.354187Z","iopub.status.idle":"2023-08-17T01:37:29.493416Z","shell.execute_reply.started":"2023-08-17T01:31:29.354149Z","shell.execute_reply":"2023-08-17T01:37:29.492215Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa4c784526d45bb96a12d96a367af0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e27c36d255441b2b632ab3330694632"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efdd184f50354b91abaede6e32667fda"}},"metadata":{}},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6aa0dd604741789705825ab4fd3df9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7ba83707a245b9bcb3c9a68a1f64c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e02cc4247f943f49313ca2925ff07ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51ff461053414e5fb3e9a4a20dd42b1f"}},"metadata":{}},{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ea71b31eaa475294174a53c9f74caa"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7134837241a84bae8ac8faa3332015d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1911b04dda794ce08e0f78710cdcf9bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34deceb2be294155aa6c6ef32cb7a7dc"}},"metadata":{}},{"name":"stdout","text":"4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28c8b5d30ad436ab38db49939f361ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be26293ca824b4081c63137bf8910a8"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33dee32e9f434ee481fb4136cbf82fc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7daf7ca9aa7f4270a6289e686495e9fe"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33524a8936c479f917bd49e2e3d6c46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f6038f8fca646b281a43dea3bf49583"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de58298b23654c94a4bd8be0d224713e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63c98f7806743c9ae4d5c84a841a4a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b09b2e2939840208101181c1419894b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9066656ee430469eaa4e3d28e7dbd52c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a12cafb7bd64ed88462e56aa8c25d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7adcfcfc7d674bd68e272d0ab59741a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48fc0c795518406a84bdccfc48f19850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0947d3bf39ab494fb51ef1bcddcdce7d"}},"metadata":{}},{"name":"stdout","text":"æµ‹è¯•ä¸€ä¸‹æ˜¯å¦åŠ è½½æˆåŠŸ\nä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°(Mount Everest),ä½äºå–œé©¬æ‹‰é›…å±±è„‰,ä½äºå°¼æ³Šå°”å’Œä¸­å›½ä¹‹é—´çš„è¾¹ç•Œçº¿ä¸Š,æµ·æ‹”é«˜åº¦8,848.86ç±³(29,031.69è‹±å°º)ã€‚ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€è‘—åå’Œæœ€å…·æŒ‘æˆ˜æ€§çš„ç™»å±±ç›®æ ‡ä¹‹ä¸€,å¸å¼•äº†è®¸å¤šç™»å±±è€…å‰æ¥æŒ‘æˆ˜ã€‚\ntrainable params: 2,924,880 || all params: 6,246,508,908 || trainable%: 0.04682423483386154\n","output_type":"stream"}]},{"cell_type":"code","source":"from accelerate import Accelerator\nAC=Accelerator(mixed_precision='fp16',cpu=None,\n            gradient_accumulation_steps=1)\n\n#================over.\n\n# #===============è¯´æ˜peftåŒ–ä¹‹å,æ²¡æ³•ç›´æ¥åšé¢„æµ‹.\n# with AC.autocast() , torch.no_grad():\n\n#     a=peft_model.chat(tokenizer,query='ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ä»€ä¹ˆ',history=[],max_length=40)\n#     print(a,'debug!!!!!!!!!!!')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T01:37:29.495087Z","iopub.execute_input":"2023-08-17T01:37:29.495779Z","iopub.status.idle":"2023-08-17T01:37:29.505406Z","shell.execute_reply.started":"2023-08-17T01:37:29.495744Z","shell.execute_reply":"2023-08-17T01:37:29.504355Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nimport sys,datetime\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\n\n#=========è®¾ç½®æ‰“å°ä¿¡æ¯çš„.\nclass EpochRunner:\n    def __init__(self,steprunner,quiet=False):\n        self.steprunner = steprunner\n        self.stage = steprunner.stage\n        self.accelerator = steprunner.accelerator\n        self.net = steprunner.net\n        self.quiet = quiet\n        \n    def __call__(self,dataloader):\n        n = dataloader.size  if hasattr(dataloader,'size') else len(dataloader)\n        loop = tqdm(enumerate(dataloader,start=1), \n                    total=n,\n                    file=sys.stdout,\n                    disable=not self.accelerator.is_local_main_process or self.quiet,\n                    ncols=100\n                   )\n        epoch_losses = {}\n        \n        for step, batch in loop: \n            with self.accelerator.accumulate(self.net):\n                step_losses,step_metrics = self.steprunner(batch)   \n                step_log = dict(step_losses,**step_metrics)\n\n                for k,v in step_losses.items():\n                    epoch_losses[k] = epoch_losses.get(k,0.0)+v\n                \n          #=============æ‰“å°è®­ç»ƒæ—¥å¿—.\n                print('å½“å‰step')\n                if step<n:\n                    loop.set_postfix(**step_log)\n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**step_log)\n                        self.progress.set_postfix(**post_log)\n\n                elif step==n:\n                    epoch_metrics = step_metrics\n                    epoch_metrics.update({self.stage+\"_\"+name:metric_fn.compute().item() \n                                     for name,metric_fn in self.steprunner.metrics_dict.items()})\n                    epoch_losses = {k:v/step for k,v in epoch_losses.items()}\n                    epoch_log = dict(epoch_losses,**epoch_metrics)\n                    loop.set_postfix(**epoch_log)\n            \n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**epoch_log)\n                        self.progress.set_postfix(**post_log)\n                    \n                    for name,metric_fn in self.steprunner.metrics_dict.items():\n                        metric_fn.reset()  \n                else:\n                    break\n        print(55555,epoch_log)\n        return epoch_log\n\n\n#===============ä¿®æ”¹ä¸‹é¢ä»£ç ä¸ºè‡ªå·±è·‘. æ¥ä¼˜åŒ–æ€§èƒ½:\n\nfrom accelerate import Accelerator \n#============torchkerasæ¥å†™è®­ç»ƒä»£ç æœç„¶ç‰›é€¼,å›¾æ ‡å¤ªç‰›é€¼äº†.\n#======ç¬¬ä¸€æ­¥è®¾ç½®å¥½è‡ªå®šä¹‰çš„KerasModel\nflag=0\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n        self.flag=0\n    \n    def __call__(self, batch):\n        \n        #loss\n        global flag\n        if 0:\n#           if not flag: #=======æˆ‘ä»¬æ‰“å°ç¬¬ä¸€ä¸ªè¾“å…¥å˜é‡çš„æ•°æ®,æ–¹ä¾¿ç†è§£æ•°æ®é›†.\n            print('æŸ¥çœ‹ç¬¬ä¸€ä¸ªbatch',batch)\n            flag=1\n        with self.accelerator.autocast():\n            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n#=========================ä»è¿™å¾€ä¸‹çš„å…¨æ˜¯å›ºå®šå†™æ³•ä¸ç”¨åŠ¨.\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\nclass KerasModel(torch.nn.Module):\n    \n    StepRunner,EpochRunner = StepRunner,EpochRunner\n    \n    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None,tokenizer=None):\n        super().__init__()\n        self.net,self.loss_fn,self.metrics_dict = net, loss_fn, torch.nn.ModuleDict(metrics_dict) \n        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n            self.net.parameters(), lr=3e-4)\n        self.lr_scheduler = lr_scheduler\n        self.from_scratch = True     #æ²¡æœ‰åŠ è½½åŠ è½½é¢„å…ˆçš„æƒé‡.#åˆå§‹åŒ–æ—¶å€™æ²¡åŠ è½½, scratcchæ˜¯è‰å›¾çš„æ„æ€è¡¨ç¤ºæ²¡æœ‰æƒé‡åœ¨ç½‘ç»œé‡Œé¢.\n    #########=============ä¸€èˆ¬ä¸ç”¨ä¸‹é¢è¿™2ä¸ªä¿å­˜åŠ è½½, é€‚é…æ€§ä¸å¤Ÿ.\n    def save_ckpt(self, ckpt_path=None, accelerator= None):\n        accelerator = accelerator if accelerator is not None else self.accelerator\n        net_dict = accelerator.get_state_dict(self.net)\n        accelerator.save(net_dict,ckpt_path if ckpt_path is not None else self.ckpt_path)\n      \n    def load_ckpt(self, ckpt_path=None):\n        self.net.load_state_dict(\n            torch.load(ckpt_path if ckpt_path is not None else self.ckpt_path,\n            map_location='cpu'))\n        self.from_scratch = False\n\n    def forward(self, x):\n        return self.net.forward(x)\n    \n    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=False,  wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1,dfhistorypath='dfhistory.csv'):\n        from torchkeras.utils import colorful,is_jupyter\n        self.__dict__.update(locals())\n        self.accelerator = AC\n        device = str(self.accelerator.device)\n        device_type = 'ğŸŒ'  if 'cpu' in device else ('âš¡ï¸' if 'cuda' in device else 'ğŸš€')\n        self.accelerator.print(\n            colorful(\"<<<<<< \"+device_type +\" \"+ device +\" is used >>>>>>\"))\n    \n        self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler= self.accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler)\n        \n        train_dataloader,val_dataloader = self.accelerator.prepare(train_data,val_data)\n        train_dataloader.size = train_data.size if hasattr(train_data,'size') else len(train_data)\n        train_dataloader.size = min(train_dataloader.size,len(train_dataloader))\n        \n        if val_data:\n            val_dataloader.size = val_data.size if hasattr(val_data,'size') else len(val_data)\n            val_dataloader.size = min(val_dataloader.size,len(val_dataloader))\n        \n        self.history = {}\n        callbacks = callbacks if callbacks is not None else []\n        \n        if bool(plot):\n            from torchkeras.kerascallbacks import VisProgress,VisMetric\n            callbacks = [VisMetric(),VisProgress()]+callbacks\n            \n        if wandb!=False:\n            from torchkeras.kerascallbacks import WandbCallback\n            project = wandb if isinstance(wandb,str) else 'torchkeras'\n            callbacks.append(WandbCallback(project=project))\n            \n        self.callbacks = [self.accelerator.prepare(x) for x in callbacks]\n        \n        if self.accelerator.is_local_main_process:\n            [cb.on_fit_start(model = self) for cb in self.callbacks if hasattr(cb,'on_fit_start')]\n                \n        start_epoch = 1 if self.from_scratch else 0\n        \n        if bool(plot) or quiet is None:\n            quiet = True\n        \n        quiet_fn = (lambda epoch:quiet) if isinstance(quiet,bool) else (\n            (lambda epoch:epoch>quiet) if isinstance(quiet,int) else quiet)\n        #==========================è®­ç»ƒ.\n        for epoch in range(start_epoch,epochs+1):\n            if 0:\n                should_quiet = quiet_fn(epoch)\n            \n                if not should_quiet:\n                    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                    self.accelerator.print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n                    self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs)+\"\\n\")\n\n            # 1ï¼Œtrain -------------------------------------------------  \n            train_step_runner = self.StepRunner(    #è®­ç»ƒä¸€ä¸ªstep\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"train\",\n                    metrics_dict=deepcopy(self.metrics_dict),\n                    optimizer = self.optimizer if epoch>0 else None,\n                    lr_scheduler = self.lr_scheduler if epoch>0 else None\n            )\n            should_quiet=True\n            train_epoch_runner = self.EpochRunner(train_step_runner,should_quiet)\n            train_metrics = {'epoch':epoch}\n            print('111111')\n            train_metrics.update(train_epoch_runner(train_dataloader))\n            print(train_metrics)\n            for name, metric in train_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n            #==================è°ƒç”¨callbackå‡½æ•°!!!!!!!!!\n            if 0:\n                if self.accelerator.is_local_main_process: #=================420å‡½æ•°çš„å«ä¹‰å°±æ˜¯è°ƒç”¨å…¨éƒ¨çš„self.callbackså‡½æ•°!!!!!!!!\n                    [cb.on_train_epoch_end(model = self) for cb in self.callbacks \n                    if hasattr(cb,'on_train_epoch_end')]\n                    \n            # 2ï¼Œvalidate -------------------------------------------------\n            if val_dataloader is not None:\n                val_step_runner = self.StepRunner(\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"val\",\n                    metrics_dict= deepcopy(self.metrics_dict)\n                )\n                val_epoch_runner = self.EpochRunner(val_step_runner,should_quiet)\n                with torch.no_grad():\n                    val_metrics = val_epoch_runner(val_dataloader)\n\n                for name, metric in val_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n                \n            if self.accelerator.is_local_main_process:\n                [cb.on_validation_epoch_end(model = self) for cb in self.callbacks \n                 if hasattr(cb,'on_validation_epoch_end')]\n\n            # 3ï¼Œearly-stopping -------------------------------------------------\n            if 1: #======è¿™éƒ¨åˆ†é€»è¾‘ä¸å¤ªå¯¹å•Š.#ä¿å­˜å¤ªå¯†é›†äº†.æˆ‘ä¿®æ”¹æ‰ä¿å­˜çš„.\n                self.accelerator.wait_for_everyone()\n                arr_scores = self.history[monitor]\n                best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n\n\n\n                if len(arr_scores)-best_score_idx>patience:\n                    break\n                \n        if self.accelerator.is_local_main_process:   \n            dfhistory = pd.DataFrame(self.history)\n            # [cb.on_fit_end(model = self) for cb in self.callbacks \n            #      if hasattr(cb,'on_fit_end')]\n            if epoch<epochs:\n                self.accelerator.print(colorful(\n                        \"<<<<<< {} without improvement in {} epoch,\"\"early stopping >>>>>> \\n\"\n                    ).format(monitor,patience))\n            # self.net = self.accelerator.unwrap_model(self.net)\n            # self.net.cpu()\n\n            dfhistory = pd.DataFrame(model.history)\n            dfhistory.to_csv(self.dfhistorypath,index=None)\n            # self.load_ckpt(ckpt_path)\n            return dfhistory\n    def predict(self,batch):\n\n        accelerator = Accelerator() if not hasattr(self,'accelerator') else self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n        val_data = accelerator.prepare(val_data)\n        with torch.no_grad():\n            a=self.StepRunner.net(input_ids=batch[\"input_ids\"])\n\n\n        return a\n\n    def evaluate(self, val_data, quiet=False):\n        accelerator = Accelerator() if not hasattr(self,'accelerator') else self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n        val_data = accelerator.prepare(val_data)\n        val_step_runner = self.StepRunner(net = self.net,stage=\"val\",\n                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n                    accelerator = accelerator)\n        val_epoch_runner = self.EpochRunner(val_step_runner,quiet=quiet)\n        with torch.no_grad():\n            val_metrics = val_epoch_runner(val_data)\n        return val_metrics\n    \n    def fit_ddp(self,num_processes,train_data,\n            val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=True, wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1\n           ):\n        from accelerate import notebook_launcher\n        args = (train_data,val_data,epochs,ckpt_path,patience,monitor,mode,\n            callbacks,plot,wandb,quiet,mixed_precision,cpu,gradient_accumulation_steps)\n        notebook_launcher(self.fit, args, num_processes=num_processes)\n    \n    def evaluate_ddp(self, num_processes, val_data, quiet=False):\n        from accelerate import notebook_launcher\n        args = (val_data,quiet)\n        notebook_launcher(self.evaluate, args, num_processes=num_processes)\n\n\n\n\n\n\n\n\n\n    \nKerasModel.StepRunner = StepRunner \n\n\n#ä»…ä»…ä¿å­˜loraç›¸å…³çš„å¯è®­ç»ƒå‚æ•°\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \noptimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n\n#########ç¬¬äºŒæ­¥å®ä¾‹åŒ–model\nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer) \nckpt_path = 'chatglm2_my' #===========ä¿å­˜çš„è·¯å¾„.\n#=========ç¬¬ä¸‰éƒ¨ä¸‹é¢å‡½æ•°è‡ªåŠ¨è®­ç»ƒ, ç”»å›¾, å’Œå­˜æ¨¡å‹.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys,datetime\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\n\n#=========è®¾ç½®æ‰“å°ä¿¡æ¯çš„.\nclass EpochRunner:\n    def __init__(self,steprunner,quiet=False):\n        self.steprunner = steprunner\n        self.stage = steprunner.stage\n        self.accelerator = steprunner.accelerator\n        self.net = steprunner.net\n        self.quiet = quiet\n        \n    def __call__(self,dataloader):\n        n = dataloader.size  if hasattr(dataloader,'size') else len(dataloader)\n        loop = tqdm(enumerate(dataloader,start=1), \n                    total=n,\n                    file=sys.stdout,\n                    disable=not self.accelerator.is_local_main_process or self.quiet,\n                    ncols=100\n                   )\n        epoch_losses = {}\n        \n        for step, batch in loop: \n            with self.accelerator.accumulate(self.net):\n                step_losses,step_metrics = self.steprunner(batch)   \n                step_log = dict(step_losses,**step_metrics)\n                print(step_losses.items())\n                for k,v in step_losses.items():\n                    epoch_losses[k] = epoch_losses.get(k,0.0)+v\n          #=============æ‰“å°è®­ç»ƒæ—¥å¿—.\n                if step<n:\n                    loop.set_postfix(**step_log)\n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**step_log)\n                        self.progress.set_postfix(**post_log)\n\n                elif step==n:\n    \n                    epoch_metrics = step_metrics\n                    epoch_metrics.update({self.stage+\"_\"+name:metric_fn.compute().item() \n                                     for name,metric_fn in self.steprunner.metrics_dict.items()})\n                    epoch_losses = {k:v/step for k,v in epoch_losses.items()}\n                    epoch_log = dict(epoch_losses,**epoch_metrics)\n                    loop.set_postfix(**epoch_log)\n            \n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**epoch_log)\n                        self.progress.set_postfix(**post_log)\n                    \n                    for name,metric_fn in self.steprunner.metrics_dict.items():\n                        metric_fn.reset()  \n                else:\n                    break\n        return epoch_log\n\n\n#===============ä¿®æ”¹ä¸‹é¢ä»£ç ä¸ºè‡ªå·±è·‘. æ¥ä¼˜åŒ–æ€§èƒ½:\n\nfrom accelerate import Accelerator \n#============torchkerasæ¥å†™è®­ç»ƒä»£ç æœç„¶ç‰›é€¼,å›¾æ ‡å¤ªç‰›é€¼äº†.\n#======ç¬¬ä¸€æ­¥è®¾ç½®å¥½è‡ªå®šä¹‰çš„KerasModel\nflag=0\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n        self.flag=0\n\n    \n    def __call__(self, batch):\n        \n        #loss\n        global flag\n        if not flag: #=======æˆ‘ä»¬æ‰“å°ç¬¬ä¸€ä¸ªè¾“å…¥å˜é‡çš„æ•°æ®,æ–¹ä¾¿ç†è§£æ•°æ®é›†.\n            \n            flag=1\n        with self.accelerator.autocast():\n            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n#=========================ä»è¿™å¾€ä¸‹çš„å…¨æ˜¯å›ºå®šå†™æ³•ä¸ç”¨åŠ¨.\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\nclass KerasModel(torch.nn.Module):\n    \n    StepRunner,EpochRunner = StepRunner,EpochRunner\n    \n    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None,tokenizer=None,mixed_precision=None,cpu=None,gradient_accumulation_steps=None):\n        super().__init__()\n        self.net,self.loss_fn,self.metrics_dict = net, loss_fn, torch.nn.ModuleDict(metrics_dict) \n        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n            self.net.parameters(), lr=3e-4)\n        self.lr_scheduler = lr_scheduler\n        self.from_scratch = True     #æ²¡æœ‰åŠ è½½åŠ è½½é¢„å…ˆçš„æƒé‡.#åˆå§‹åŒ–æ—¶å€™æ²¡åŠ è½½, scratcchæ˜¯è‰å›¾çš„æ„æ€è¡¨ç¤ºæ²¡æœ‰æƒé‡åœ¨ç½‘ç»œé‡Œé¢.\n        \n        self.accelerator= AC\n    #########=============ä¸€èˆ¬ä¸ç”¨ä¸‹é¢è¿™2ä¸ªä¿å­˜åŠ è½½, é€‚é…æ€§ä¸å¤Ÿ.\n    def save_ckpt(self, ckpt_path=None, accelerator= None):\n        accelerator = accelerator if accelerator is not None else self.accelerator\n        net_dict = accelerator.get_state_dict(self.net)\n        accelerator.save(net_dict,ckpt_path if ckpt_path is not None else self.ckpt_path)\n      \n    def load_ckpt(self, ckpt_path=None):\n        self.net.load_state_dict(\n            torch.load(ckpt_path if ckpt_path is not None else self.ckpt_path,\n            map_location='cpu'))\n        self.from_scratch = False\n\n    def forward(self, x):\n        return self.net.forward(x)\n    \n    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=False,  wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1,dfhistorypath='dfhistory.csv'):\n        from torchkeras.utils import colorful,is_jupyter\n        self.__dict__.update(locals())\n\n        device = str(self.accelerator.device)\n        device_type = 'ğŸŒ'  if 'cpu' in device else ('âš¡ï¸' if 'cuda' in device else 'ğŸš€')\n        self.accelerator.print(\n            colorful(\"<<<<<< \"+device_type +\" \"+ device +\" is used >>>>>>\"))\n    \n        self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler= self.accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler)\n        \n        train_dataloader,val_dataloader = self.accelerator.prepare(train_data,val_data)\n        train_dataloader.size = train_data.size if hasattr(train_data,'size') else len(train_data)\n        train_dataloader.size = min(train_dataloader.size,len(train_dataloader))\n        \n        if val_data:\n            val_dataloader.size = val_data.size if hasattr(val_data,'size') else len(val_data)\n            val_dataloader.size = min(val_dataloader.size,len(val_dataloader))\n        \n        self.history = {}\n        callbacks = callbacks if callbacks is not None else []\n        \n        if bool(plot):\n            from torchkeras.kerascallbacks import VisProgress,VisMetric\n            callbacks = [VisMetric(),VisProgress()]+callbacks\n            \n        if wandb!=False:\n            from torchkeras.kerascallbacks import WandbCallback\n            project = wandb if isinstance(wandb,str) else 'torchkeras'\n            callbacks.append(WandbCallback(project=project))\n            \n        self.callbacks = [self.accelerator.prepare(x) for x in callbacks]\n        \n        if self.accelerator.is_local_main_process:\n            [cb.on_fit_start(model = self) for cb in self.callbacks if hasattr(cb,'on_fit_start')]\n                \n        start_epoch = 1 if self.from_scratch else 0\n        \n        if bool(plot) or quiet is None:\n            quiet = True\n        \n        quiet_fn = (lambda epoch:quiet) if isinstance(quiet,bool) else (\n            (lambda epoch:epoch>quiet) if isinstance(quiet,int) else quiet)\n        #==========================è®­ç»ƒ.\n        for epoch in range(start_epoch,epochs+1):\n            if 0:\n                should_quiet = quiet_fn(epoch)\n            \n                if not should_quiet:\n                    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                    self.accelerator.print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n                    self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs)+\"\\n\")\n            should_quiet=True\n            # 1ï¼Œtrain -------------------------------------------------  \n            train_step_runner = self.StepRunner(    #è®­ç»ƒä¸€ä¸ªstep\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"train\",\n                    metrics_dict=deepcopy(self.metrics_dict),\n                    optimizer = self.optimizer if epoch>0 else None,\n                    lr_scheduler = self.lr_scheduler if epoch>0 else None\n            )\n\n            train_epoch_runner = self.EpochRunner(train_step_runner,should_quiet)\n            train_metrics = {'epoch':epoch}\n            train_metrics.update(train_epoch_runner(train_dataloader))\n\n            for name, metric in train_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n            #==================è°ƒç”¨callbackå‡½æ•°!!!!!!!!!\n            if 0:\n                if self.accelerator.is_local_main_process: #=================420å‡½æ•°çš„å«ä¹‰å°±æ˜¯è°ƒç”¨å…¨éƒ¨çš„self.callbackså‡½æ•°!!!!!!!!\n                    [cb.on_train_epoch_end(model = self) for cb in self.callbacks \n                    if hasattr(cb,'on_train_epoch_end')]\n                    \n            # 2ï¼Œvalidate -------------------------------------------------\n            if val_dataloader is not None:\n                val_step_runner = self.StepRunner(\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"val\",\n                    metrics_dict= deepcopy(self.metrics_dict)\n                )\n                val_epoch_runner = self.EpochRunner(val_step_runner,should_quiet)\n                with torch.no_grad():\n                    val_metrics = val_epoch_runner(val_dataloader)\n\n                for name, metric in val_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n                \n            if self.accelerator.is_local_main_process:\n                [cb.on_validation_epoch_end(model = self) for cb in self.callbacks \n                 if hasattr(cb,'on_validation_epoch_end')]\n\n            # 3ï¼Œearly-stopping -------------------------------------------------\n            if 1: #======è¿™éƒ¨åˆ†é€»è¾‘ä¸å¤ªå¯¹å•Š.#ä¿å­˜å¤ªå¯†é›†äº†.æˆ‘ä¿®æ”¹æ‰ä¿å­˜çš„.\n                self.accelerator.wait_for_everyone()\n                arr_scores = self.history[monitor]\n                best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n\n\n\n                if len(arr_scores)-best_score_idx>patience:\n                    break\n                \n        if self.accelerator.is_local_main_process:   \n            dfhistory = pd.DataFrame(self.history)\n            # [cb.on_fit_end(model = self) for cb in self.callbacks \n            #      if hasattr(cb,'on_fit_end')]\n            if epoch<epochs:\n                self.accelerator.print(colorful(\n                        \"<<<<<< {} without improvement in {} epoch,\"\"early stopping >>>>>> \\n\"\n                    ).format(monitor,patience))\n            # self.net = self.accelerator.unwrap_model(self.net)\n            # self.net.cpu()\n\n#             dfhistory = pd.DataFrame(model.history)\n            dfhistory.to_csv(self.dfhistorypath,index=None)\n            # self.load_ckpt(ckpt_path)\n            return dfhistory\n#=====================!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!å®ç°é¢„æµ‹ä»£ç .\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    def evaluate(self, val_data, quiet=False):\n        accelerator = Accelerator() if not hasattr(self,'accelerator') else self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n        val_data = accelerator.prepare(val_data)\n        val_step_runner = self.StepRunner(net = self.net,stage=\"val\",\n                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n                    accelerator = accelerator)\n        val_epoch_runner = self.EpochRunner(val_step_runner,quiet=quiet)\n        with torch.no_grad():\n            val_metrics = val_epoch_runner(val_data)\n        return val_metrics\n    \n    def fit_ddp(self,num_processes,train_data,\n            val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=True, wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1\n           ):\n        from accelerate import notebook_launcher\n        args = (train_data,val_data,epochs,ckpt_path,patience,monitor,mode,\n            callbacks,plot,wandb,quiet,mixed_precision,cpu,gradient_accumulation_steps)\n        notebook_launcher(self.fit, args, num_processes=num_processes)\n    \n    def evaluate_ddp(self, num_processes, val_data, quiet=False):\n        from accelerate import notebook_launcher\n        args = (val_data,quiet)\n        notebook_launcher(self.evaluate, args, num_processes=num_processes)\n\n\n\n\n\n\n\n\n\n    \nKerasModel.StepRunner = StepRunner \n\n\n#ä»…ä»…ä¿å­˜loraç›¸å…³çš„å¯è®­ç»ƒå‚æ•°\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \noptimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n\n#########ç¬¬äºŒæ­¥å®ä¾‹åŒ–model\nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer, mixed_precision='fp16',cpu=False,\n            gradient_accumulation_steps=cfg.gradient_accumulation_steps) \nckpt_path = 'chatglm2_my' #===========ä¿å­˜çš„è·¯å¾„.\n#=========ç¬¬ä¸‰éƒ¨ä¸‹é¢å‡½æ•°è‡ªåŠ¨è®­ç»ƒ, ç”»å›¾, å’Œå­˜æ¨¡å‹.\n\nprint('é…ç½®å®Œæ¯•')\n# if 1: # æµ‹è¯•\n\n#         print('è®­ç»ƒä¹‹å‰å¼€å§‹æµ‹è¯•')\n#         print(keras_model.predict('æ¢¦ä¸­æƒ…ç‚‰',max_length=200)[0])\n#         print(keras_model.predict('ä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ä»€ä¹ˆ',max_length=200)[0])\n\n\n# if 1: # æµ‹è¯•\n\n#         print('è®­ç»ƒä¹‹åå¼€å§‹æµ‹è¯•')\n#         print(keras_model.predict('æ¢¦ä¸­æƒ…ç‚‰',max_length=200)[0])\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T01:37:29.509008Z","iopub.execute_input":"2023-08-17T01:37:29.509408Z","iopub.status.idle":"2023-08-17T01:42:38.892287Z","shell.execute_reply.started":"2023-08-17T01:37:29.509371Z","shell.execute_reply":"2023-08-17T01:42:38.889885Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"é…ç½®å®Œæ¯•\n\u001b[0;31m<<<<<< âš¡ï¸ cuda is used >>>>>>\u001b[0m\ndict_items([('train_loss', 6.7890625)])\ndict_items([('train_loss', 6.80078125)])\ndict_items([('train_loss', 5.92578125)])\ndict_items([('train_loss', 5.8359375)])\ndict_items([('val_loss', 5.7578125)])\ndict_items([('val_loss', 5.8515625)])\ndict_items([('val_loss', 5.8046875)])\ndict_items([('val_loss', 5.96875)])\ndict_items([('train_loss', 5.87109375)])\ndict_items([('train_loss', 5.81640625)])\ndict_items([('train_loss', 5.34375)])\ndict_items([('train_loss', 4.98046875)])\ndict_items([('val_loss', 4.1015625)])\ndict_items([('val_loss', 4.09765625)])\ndict_items([('val_loss', 4.12890625)])\ndict_items([('val_loss', 3.947265625)])\ndict_items([('train_loss', 4.19921875)])\ndict_items([('train_loss', 3.6953125)])\ndict_items([('train_loss', 3.318359375)])\ndict_items([('train_loss', 2.728515625)])\ndict_items([('val_loss', 1.9453125)])\ndict_items([('val_loss', 2.00390625)])\ndict_items([('val_loss', 2.05859375)])\ndict_items([('val_loss', 1.93359375)])\ndict_items([('train_loss', 1.9931640625)])\ndict_items([('train_loss', 1.41015625)])\ndict_items([('train_loss', 0.98974609375)])\ndict_items([('train_loss', 0.83154296875)])\ndict_items([('val_loss', 0.68408203125)])\ndict_items([('val_loss', 0.68603515625)])\ndict_items([('val_loss', 0.68359375)])\ndict_items([('val_loss', 0.68408203125)])\ndict_items([('train_loss', 0.68408203125)])\ndict_items([('train_loss', 0.54931640625)])\ndict_items([('train_loss', 0.453125)])\ndict_items([('train_loss', 0.385986328125)])\ndict_items([('val_loss', 0.327392578125)])\ndict_items([('val_loss', 0.328369140625)])\ndict_items([('val_loss', 0.327880859375)])\ndict_items([('val_loss', 0.32763671875)])\ndict_items([('train_loss', 0.328125)])\ndict_items([('train_loss', 0.283447265625)])\ndict_items([('train_loss', 0.2467041015625)])\ndict_items([('train_loss', 0.210205078125)])\ndict_items([('val_loss', 0.1749267578125)])\ndict_items([('val_loss', 0.1751708984375)])\ndict_items([('val_loss', 0.1751708984375)])\ndict_items([('val_loss', 0.17529296875)])\ndict_items([('train_loss', 0.1754150390625)])\ndict_items([('train_loss', 0.1470947265625)])\ndict_items([('train_loss', 0.1243896484375)])\ndict_items([('train_loss', 0.109619140625)])\ndict_items([('val_loss', 0.09710693359375)])\ndict_items([('val_loss', 0.09716796875)])\ndict_items([('val_loss', 0.09716796875)])\ndict_items([('val_loss', 0.09735107421875)])\ndict_items([('train_loss', 0.09722900390625)])\ndict_items([('train_loss', 0.09039306640625)])\ndict_items([('train_loss', 0.0889892578125)])\ndict_items([('train_loss', 0.08056640625)])\ndict_items([('val_loss', 0.0728759765625)])\ndict_items([('val_loss', 0.0728759765625)])\ndict_items([('val_loss', 0.07281494140625)])\ndict_items([('val_loss', 0.072998046875)])\ndict_items([('train_loss', 0.07293701171875)])\ndict_items([('train_loss', 0.07513427734375)])\ndict_items([('train_loss', 0.0694580078125)])\ndict_items([('train_loss', 0.0643310546875)])\ndict_items([('val_loss', 0.06549072265625)])\ndict_items([('val_loss', 0.0655517578125)])\ndict_items([('val_loss', 0.0655517578125)])\ndict_items([('val_loss', 0.0655517578125)])\ndict_items([('train_loss', 0.0655517578125)])\ndict_items([('train_loss', 0.06207275390625)])\ndict_items([('train_loss', 0.056884765625)])\ndict_items([('train_loss', 0.05743408203125)])\ndict_items([('val_loss', 0.0556640625)])\ndict_items([('val_loss', 0.0556640625)])\ndict_items([('val_loss', 0.055633544921875)])\ndict_items([('val_loss', 0.0556640625)])\ndict_items([('train_loss', 0.05572509765625)])\ndict_items([('train_loss', 0.054107666015625)])\ndict_items([('train_loss', 0.051727294921875)])\ndict_items([('train_loss', 0.049835205078125)])\ndict_items([('val_loss', 0.050262451171875)])\ndict_items([('val_loss', 0.05029296875)])\ndict_items([('val_loss', 0.05029296875)])\ndict_items([('val_loss', 0.05029296875)])\ndict_items([('train_loss', 0.05029296875)])\ndict_items([('train_loss', 0.0491943359375)])\ndict_items([('train_loss', 0.046630859375)])\ndict_items([('train_loss', 0.04815673828125)])\ndict_items([('val_loss', 0.0460205078125)])\ndict_items([('val_loss', 0.0460205078125)])\ndict_items([('val_loss', 0.046051025390625)])\ndict_items([('val_loss', 0.0460205078125)])\ndict_items([('train_loss', 0.046051025390625)])\ndict_items([('train_loss', 0.045440673828125)])\ndict_items([('train_loss', 0.04571533203125)])\ndict_items([('train_loss', 0.04620361328125)])\ndict_items([('val_loss', 0.044647216796875)])\ndict_items([('val_loss', 0.044647216796875)])\ndict_items([('val_loss', 0.044647216796875)])\ndict_items([('val_loss', 0.044647216796875)])\ndict_items([('train_loss', 0.044647216796875)])\ndict_items([('train_loss', 0.0450439453125)])\ndict_items([('train_loss', 0.04443359375)])\ndict_items([('train_loss', 0.043212890625)])\ndict_items([('val_loss', 0.045806884765625)])\ndict_items([('val_loss', 0.045806884765625)])\ndict_items([('val_loss', 0.045806884765625)])\ndict_items([('val_loss', 0.045806884765625)])\ndict_items([('train_loss', 0.045806884765625)])\ndict_items([('train_loss', 0.042999267578125)])\ndict_items([('train_loss', 0.04693603515625)])\ndict_items([('train_loss', 0.045013427734375)])\ndict_items([('val_loss', 0.04559326171875)])\ndict_items([('val_loss', 0.04559326171875)])\ndict_items([('val_loss', 0.045562744140625)])\ndict_items([('val_loss', 0.045562744140625)])\ndict_items([('train_loss', 0.045562744140625)])\ndict_items([('train_loss', 0.043243408203125)])\ndict_items([('train_loss', 0.046051025390625)])\ndict_items([('train_loss', 0.042236328125)])\ndict_items([('val_loss', 0.045562744140625)])\ndict_items([('val_loss', 0.045562744140625)])\ndict_items([('val_loss', 0.045562744140625)])\ndict_items([('val_loss', 0.045562744140625)])\ndict_items([('train_loss', 0.045562744140625)])\ndict_items([('train_loss', 0.0426025390625)])\ndict_items([('train_loss', 0.04541015625)])\ndict_items([('train_loss', 0.044036865234375)])\ndict_items([('val_loss', 0.04315185546875)])\ndict_items([('val_loss', 0.04315185546875)])\ndict_items([('val_loss', 0.04315185546875)])\ndict_items([('val_loss', 0.04315185546875)])\ndict_items([('train_loss', 0.04315185546875)])\ndict_items([('train_loss', 0.043426513671875)])\ndict_items([('train_loss', 0.045989990234375)])\ndict_items([('train_loss', 0.04278564453125)])\ndict_items([('val_loss', 0.04443359375)])\ndict_items([('val_loss', 0.044403076171875)])\ndict_items([('val_loss', 0.044403076171875)])\ndict_items([('val_loss', 0.04443359375)])\ndict_items([('train_loss', 0.04443359375)])\ndict_items([('train_loss', 0.04290771484375)])\ndict_items([('train_loss', 0.043487548828125)])\ndict_items([('train_loss', 0.044158935546875)])\ndict_items([('val_loss', 0.042755126953125)])\ndict_items([('val_loss', 0.042755126953125)])\ndict_items([('val_loss', 0.042755126953125)])\ndict_items([('val_loss', 0.042755126953125)])\ndict_items([('train_loss', 0.042755126953125)])\ndict_items([('train_loss', 0.043365478515625)])\ndict_items([('train_loss', 0.042449951171875)])\ndict_items([('train_loss', 0.043365478515625)])\ndict_items([('val_loss', 0.041656494140625)])\ndict_items([('val_loss', 0.041656494140625)])\ndict_items([('val_loss', 0.041656494140625)])\ndict_items([('val_loss', 0.041656494140625)])\n","output_type":"stream"}]},{"cell_type":"code","source":"# #-==========æ‰‹åŠ¨å†™predictä»£ç .    \n# if 1:\n#         keras_model.net.eval()\n#         accelerator = keras_model.accelerator\n#         keras_model.net,keras_model.loss_fn,keras_model.metrics_dict = keras_model.accelerator.prepare(\n#             keras_model.net,keras_model.loss_fn,keras_model.metrics_dict)\n        \n#         with accelerator.autocast() , torch.no_grad():\n#             for batch in dl_train:\n                \n#                 a=keras_model.net(input_ids=batch[\"input_ids\"].cuda(),labels=batch[\"labels\"].cuda()).logits\n#                 print(a,'debug!!!!!!!!!!!')\n#                 break\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T01:50:34.101811Z","iopub.execute_input":"2023-08-17T01:50:34.102256Z","iopub.status.idle":"2023-08-17T01:50:35.275706Z","shell.execute_reply.started":"2023-08-17T01:50:34.102221Z","shell.execute_reply":"2023-08-17T01:50:35.274441Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tensor([[[-10.7500, -10.7500,   0.4290,  ..., -10.7500, -10.7500, -10.7500],\n         [-12.6094, -12.6094,   2.2852,  ..., -12.6094, -12.6094, -12.6094],\n         [ -7.0391,  -7.0391,  -3.9414,  ...,  -7.0391,  -7.0391,  -7.0391],\n         ...,\n         [  7.2070,   7.2031,   5.1367,  ...,   7.2109,   7.2109,   7.2109],\n         [  7.1367,   7.1328,   5.1797,  ...,   7.1406,   7.1406,   7.1406],\n         [  7.1094,   7.1055,   5.2539,  ...,   7.1133,   7.1133,   7.1133]],\n\n        [[-10.7500, -10.7500,   0.4290,  ..., -10.7500, -10.7500, -10.7500],\n         [-12.6094, -12.6094,   2.2852,  ..., -12.6094, -12.6094, -12.6094],\n         [ -7.0391,  -7.0391,  -3.9414,  ...,  -7.0391,  -7.0391,  -7.0391],\n         ...,\n         [  7.4492,   7.4453,   4.8633,  ...,   7.4531,   7.4531,   7.4531],\n         [  7.4805,   7.4766,   4.8945,  ...,   7.4844,   7.4844,   7.4844],\n         [  7.4023,   7.3984,   5.0352,  ...,   7.4062,   7.4062,   7.4062]]],\n       device='cuda:0') debug!!!!!!!!!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"if 1:#è®­ç»ƒ\n    keras_model.fit(train_data = dl_train,\n                val_data = dl_train,\n                epochs=5,\n                patience=20,\n                monitor='val_loss',\n                mode='min',\n                ckpt_path = ckpt_path,\n\n                plot=False, # ä¸ç”»ç”»èŠ‚çœç©ºé—´.\n          \n               )","metadata":{"execution":{"iopub.status.busy":"2023-08-17T02:28:04.701317Z","iopub.execute_input":"2023-08-17T02:28:04.702544Z","iopub.status.idle":"2023-08-17T02:29:21.844596Z","shell.execute_reply.started":"2023-08-17T02:28:04.702490Z","shell.execute_reply":"2023-08-17T02:29:21.843058Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"\u001b[0;31m<<<<<< âš¡ï¸ cuda is used >>>>>>\u001b[0m\ndict_items([('train_loss', 0.041656494140625)])\ndict_items([('train_loss', 0.052734375)])\ndict_items([('train_loss', 0.043853759765625)])\ndict_items([('train_loss', 0.048065185546875)])\ndict_items([('val_loss', 0.049652099609375)])\ndict_items([('val_loss', 0.049652099609375)])\ndict_items([('val_loss', 0.049652099609375)])\ndict_items([('val_loss', 0.049652099609375)])\ndict_items([('train_loss', 0.049652099609375)])\ndict_items([('train_loss', 0.058807373046875)])\ndict_items([('train_loss', 0.045135498046875)])\ndict_items([('train_loss', 0.05316162109375)])\ndict_items([('val_loss', 0.058502197265625)])\ndict_items([('val_loss', 0.058502197265625)])\ndict_items([('val_loss', 0.058502197265625)])\ndict_items([('val_loss', 0.058502197265625)])\ndict_items([('train_loss', 0.058502197265625)])\ndict_items([('train_loss', 0.06707763671875)])\ndict_items([('train_loss', 0.047027587890625)])\ndict_items([('train_loss', 0.058685302734375)])\ndict_items([('val_loss', 0.059234619140625)])\ndict_items([('val_loss', 0.059234619140625)])\ndict_items([('val_loss', 0.059234619140625)])\ndict_items([('val_loss', 0.059234619140625)])\ndict_items([('train_loss', 0.059234619140625)])\ndict_items([('train_loss', 0.06597900390625)])\ndict_items([('train_loss', 0.048614501953125)])\ndict_items([('train_loss', 0.054840087890625)])\ndict_items([('val_loss', 0.0626220703125)])\ndict_items([('val_loss', 0.0626220703125)])\ndict_items([('val_loss', 0.0626220703125)])\ndict_items([('val_loss', 0.0626220703125)])\ndict_items([('train_loss', 0.0626220703125)])\ndict_items([('train_loss', 0.07354736328125)])\ndict_items([('train_loss', 0.047576904296875)])\ndict_items([('train_loss', 0.06365966796875)])\ndict_items([('val_loss', 0.06536865234375)])\ndict_items([('val_loss', 0.06536865234375)])\ndict_items([('val_loss', 0.06536865234375)])\ndict_items([('val_loss', 0.06536865234375)])\n","output_type":"stream"}]},{"cell_type":"code","source":"#-==========æ‰‹åŠ¨å†™predictä»£ç .    \nif 1:\n        keras_model.net.eval()\n#         accelerator = keras_model.accelerator\n#         keras_model.net,keras_model.loss_fn,keras_model.metrics_dict = keras_model.accelerator.prepare(\n#             keras_model.net,keras_model.loss_fn,keras_model.metrics_dict)\n        max_length= 50\n        num_beams=1\n        do_sample=True\n        top_p=0.8\n        logits_processor=None\n        temperature=0.8\n        \n        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"do_sample\": do_sample, \"top_p\": top_p,\n                      \"temperature\": temperature, \"logits_processor\": logits_processor}\n        with accelerator.autocast() , torch.no_grad():\n            \n                query='æ¢¦ä¸­æƒ…ç‚‰æ˜¯ä»€ä¹ˆ?'\n                history=[]\n                prompt = tokenizer.build_prompt(query, history)\n                prompt = cfg.source_prefix + prompt\n                a_ids=tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n                                     max_length=cfg.max_source_length)\n                b_ids = tokenizer.encode(text='', add_special_tokens=False, truncation=True,\n                                     max_length=cfg.max_target_length)\n                input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n#                 shuru=[64790, 64792,   790, 30951,   517, 30910, 30939, 30996]\n                shuru=torch.tensor(shuru).view(1,-1)\n                print(shuru)\n#                 print(batch[\"input_ids\"][:,:20])\n                a=keras_model.net.generate(input_ids=shuru.cuda(), **gen_kwargs)\n#                 print(a)\n                shuchu=tokenizer.decode(a[0])\n                print(shuchu,'è¾“å‡ºæˆ‘ä»¬çš„é¢„æµ‹ç»“æœ')\n                \n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T03:12:02.945267Z","iopub.execute_input":"2023-08-17T03:12:02.945683Z","iopub.status.idle":"2023-08-17T03:12:03.245895Z","shell.execute_reply.started":"2023-08-17T03:12:02.945648Z","shell.execute_reply":"2023-08-17T03:12:03.244826Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_27/2504520679.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  shuru=torch.tensor(shuru).view(1,-1)\nInput length of input_ids is 116, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n","output_type":"stream"},{"name":"stdout","text":"tensor([[64790, 64792,   790, 30951,   517, 30910, 30939, 30996,    13,    13,\n         54761, 31211, 33499, 36948, 53994, 32664, 31514,    13,    13, 55437,\n         31211, 33499, 36948, 53994, 54532, 55889, 56807, 55783, 56591, 55555,\n         30946, 30944,   604, 12279,   981,   685, 32308, 55048, 37376, 55640,\n         44052, 30932, 32308, 49111, 37908, 32695, 37888, 32828, 30932, 36734,\n         32627, 30973, 30932, 30973, 30972, 30973, 30930, 30973, 30978, 55055,\n         30946, 30943, 30969, 30932, 30940, 30966, 30939, 30930, 30978, 30969,\n         53947, 33217, 55889, 56807, 55783, 56591, 55555, 54532, 41413, 32458,\n         54542, 36068, 32773, 32019, 41715, 31919, 31905, 30932, 32521, 37916,\n         41715, 54631, 36850, 32773, 31155,    13,    13, 30995, 30951,   517,\n         30910, 30943, 30996,    13,    13, 54761, 31211, 47132, 54623, 56754,\n         32664, 30987,    13,    13, 55437, 31211]])\n[Round 1]\n\né—®ï¼šä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ä»€ä¹ˆï¼Ÿ\n\nç­”ï¼šä¸–ç•Œä¸Šæœ€é«˜çš„å±±å³°æ˜¯ç ç©†æœ—ç›å³°(Mount Everest),ä½äºå–œé©¬æ‹‰é›…å±±è„‰,ä½äºå°¼æ³Šå°”å’Œä¸­å›½ä¹‹é—´çš„è¾¹ç•Œçº¿ä¸Š,æµ·æ‹”é«˜åº¦8,848.86ç±³(29,031.69è‹±å°º)ã€‚ç ç©†æœ—ç›å³°æ˜¯ä¸–ç•Œä¸Šæœ€è‘—åå’Œæœ€å…·æŒ‘æˆ˜æ€§çš„ç™»å±±ç›®æ ‡ä¹‹ä¸€,å¸å¼•äº†è®¸å¤šç™»å±±è€…å‰æ¥æŒ‘æˆ˜ã€‚\n\n[Round 2]\n\né—®ï¼šæ¢¦ä¸­æƒ…ç‚‰æ˜¯ä»€ä¹ˆ?\n\nç­”ï¼š  è¾“å‡ºæˆ‘ä»¬çš„é¢„æµ‹ç»“æœ\n","output_type":"stream"}]},{"cell_type":"code","source":"#-==========æ‰‹åŠ¨å†™predictä»£ç .    \nif 0:\n        keras_model.net.eval()\n        accelerator = keras_model.accelerator\n        keras_model.net,keras_model.loss_fn,keras_model.metrics_dict = keras_model.accelerator.prepare(\n            keras_model.net,keras_model.loss_fn,keras_model.metrics_dict)\n        \n        with accelerator.autocast() , torch.no_grad():\n            for batch in dl_train:\n                print(batch[\"input_ids\"][:])\n                a=keras_model.net.generate(input_ids=batch[\"input_ids\"].cuda(), max_length=300)\n                print(a,'debug!!!!!!!!!!!')\n                break\n","metadata":{"execution":{"iopub.status.busy":"2023-08-17T03:00:49.928878Z","iopub.execute_input":"2023-08-17T03:00:49.929287Z","iopub.status.idle":"2023-08-17T03:00:52.767130Z","shell.execute_reply.started":"2023-08-17T03:00:49.929254Z","shell.execute_reply":"2023-08-17T03:00:52.765631Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"tensor([[64790, 64792,   790, 30951,   517, 30910, 30939, 30996,    13,    13,\n         54761, 31211, 54622, 47625, 47132, 54623, 56754, 55398, 30987,    13,\n            13, 55437, 31211, 30910, 47132, 54623, 56754, 31873, 39741, 56093,\n         55823, 32715, 12852,   349,  5130,   298, 31155,    13, 36037, 54640,\n         32769, 30925,  4226, 64569, 34030, 32549, 55059, 55090, 32715, 31155,\n            13, 12852,   349,  5130,   298, 32103, 54645, 54591, 56093, 55823,\n         56754, 31211, 35886, 31123, 54591, 54571, 31123, 54591, 54858, 31155,\n            13, 54790, 54536, 12852,   349, 54530, 50745, 31123, 32106,  5130,\n           298, 54530, 35752, 31123, 32187, 32233, 32824, 31123, 54716, 54619,\n         55932, 54703, 31155,    13, 31672, 32233, 32032, 31623, 54536, 56548,\n         32365, 55058, 55466, 37358, 54891, 32547, 54835, 54653, 35528, 47132,\n         54623, 56754, 31155,     2,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [64790, 64792,   790, 30951,   517, 30910, 30939, 30996,    13,    13,\n         54761, 31211, 47132, 54623, 56754,    13,    13, 55437, 31211, 30910,\n         47132, 54623, 56754, 31873, 39741, 56093, 55823, 32715, 12852,   349,\n          5130,   298, 31155,    13, 36037, 54640, 32769, 30925,  4226, 64569,\n         34030, 32549, 55059, 55090, 32715, 31155,    13, 12852,   349,  5130,\n           298, 32103, 54645, 54591, 56093, 55823, 56754, 31211, 35886, 31123,\n         54591, 54571, 31123, 54591, 54858, 31155,    13, 54790, 54536, 12852,\n           349, 54530, 50745, 31123, 32106,  5130,   298, 54530, 35752, 31123,\n         32187, 32233, 32824, 31123, 54716, 54619, 55932, 54703, 31155,    13,\n         31672, 32233, 32032, 31623, 54536, 56548, 32365, 55058, 55466, 37358,\n         54891, 32547, 54835, 54653, 35528, 47132, 54623, 56754, 31155,     2,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31mâ•­â”€\u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[31mâ”€â•®\u001b[0m\n\u001b[31mâ”‚\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m11\u001b[0m                                                                                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mwith\u001b[0m accelerator.autocast() , torch.no_grad():                                      \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mfor\u001b[0m batch \u001b[95min\u001b[0m dl_train:                                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[96mprint\u001b[0m(batch[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m][:])                                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m11 \u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0ma=keras_model.net.generate(input_ids=batch[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m].cuda(), max_leng    \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[96mprint\u001b[0m(a,\u001b[33m'\u001b[0m\u001b[33mdebug!!!!!!!!!!!\u001b[0m\u001b[33m'\u001b[0m)                                                 \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[94mbreak\u001b[0m                                                                       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m14 \u001b[0m                                                                                            \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m977\u001b[0m in \u001b[92mgenerate\u001b[0m                       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 974 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 975 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.base_model.generation_config = \u001b[96mself\u001b[0m.generation_config                    \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 976 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 977 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0moutputs = \u001b[96mself\u001b[0m.base_model.generate(**kwargs)                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 978 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mexcept\u001b[0m:                                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 979 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[96mself\u001b[0m.base_model.prepare_inputs_for_generation = \u001b[96mself\u001b[0m.base_model_prepare_inpu  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 980 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mraise\u001b[0m                                                                         \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in \u001b[92mdecorate_context\u001b[0m       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m115 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2mâ”‚   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1522\u001b[0m in \u001b[92mgenerate\u001b[0m        \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1519 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m)                                                                         \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1520 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m                                                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1521 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[2m# 11. run greedy search\u001b[0m                                                       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1522 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.greedy_search(                                                    \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1523 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0minput_ids,                                                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1524 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mlogits_processor=logits_processor,                                        \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1525 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mstopping_criteria=stopping_criteria,                                      \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2339\u001b[0m in \u001b[92mgreedy_search\u001b[0m   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m2336 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m2337 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m                                                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m2338 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m2339 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m2340 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m**model_inputs,                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m2341 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mreturn_dict=\u001b[94mTrue\u001b[0m,                                                         \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m2342 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0moutput_attentions=output_attentions,                                      \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1501 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m165 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2mâ”‚   \u001b[0mmodule.forward = new_forward                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b1\u001b[0m \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m4463edd62620ce9f/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m932\u001b[0m in \u001b[92mforward\u001b[0m                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 929 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0muse_cache = use_cache \u001b[94mif\u001b[0m use_cache \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_cache         \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 930 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 931 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 932 \u001b[2mâ”‚   â”‚   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 933 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0minput_ids=input_ids,                                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 934 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mposition_ids=position_ids,                                                    \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 935 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mattention_mask=attention_mask,                                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m1501 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in \u001b[92mnew_forward\u001b[0m                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m165 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2mâ”‚   \u001b[0m                                                                                       \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2mâ”‚   \u001b[0mmodule.forward = new_forward                                                           \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b1\u001b[0m \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m4463edd62620ce9f/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m817\u001b[0m in \u001b[92mforward\u001b[0m                                              \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 814 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 815 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m full_attention_mask \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                   \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 816 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m (attention_mask \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m attention_mask.all()) \u001b[95mor\u001b[0m (past_key_va  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 817 \u001b[2mâ”‚   â”‚   â”‚   â”‚   \u001b[0mfull_attention_mask = \u001b[96mself\u001b[0m.get_masks(input_ids, past_key_values, padding  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 818 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 819 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[2m# Rotary positional embeddings\u001b[0m                                                    \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 820 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mrotary_pos_emb = \u001b[96mself\u001b[0m.rotary_pos_emb(\u001b[96mself\u001b[0m.seq_length)                             \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b1\u001b[0m \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[2;33m4463edd62620ce9f/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m688\u001b[0m in \u001b[92mget_masks\u001b[0m                                            \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m                                                                                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 685 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m padding_mask \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                      \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 686 \u001b[0m\u001b[2mâ”‚   â”‚   â”‚   \u001b[0mfull_attention_mask = full_attention_mask * padding_mask.unsqueeze(\u001b[94m1\u001b[0m)         \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 687 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m past_length \u001b[95mand\u001b[0m padding_mask \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                  \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m \u001b[31mâ± \u001b[0m 688 \u001b[2mâ”‚   â”‚   â”‚   \u001b[0mfull_attention_mask -= padding_mask.unsqueeze(-\u001b[94m1\u001b[0m) - \u001b[94m1\u001b[0m                         \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 689 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mfull_attention_mask = (full_attention_mask < \u001b[94m0.5\u001b[0m).bool()                          \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 690 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0mfull_attention_mask.unsqueeze_(\u001b[94m1\u001b[0m)                                                 \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ”‚\u001b[0m   \u001b[2m 691 \u001b[0m\u001b[2mâ”‚   â”‚   \u001b[0m\u001b[94mreturn\u001b[0m full_attention_mask                                                        \u001b[31mâ”‚\u001b[0m\n\u001b[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n\u001b[1;91mRuntimeError: \u001b[0moutput with shape \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m257\u001b[0m\u001b[1m]\u001b[0m doesn't match the broadcast shape \u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m257\u001b[0m, \u001b[1;36m257\u001b[0m\u001b[1m]\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">11</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> accelerator.autocast() , torch.no_grad():                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> batch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> dl_train:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(batch[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>][:])                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>11 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>a=keras_model.net.generate(input_ids=batch[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>].cuda(), max_leng    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(a,<span style=\"color: #808000; text-decoration-color: #808000\">'debug!!!!!!!!!!!'</span>)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">977</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 974 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 975 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model.generation_config = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.generation_config                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 976 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 977 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model.generate(**kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 978 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 979 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model.prepare_inputs_for_generation = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model_prepare_inpu  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 980 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1522</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1520 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1521 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># 11. run greedy search</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1522 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.greedy_search(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1523 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1524 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1525 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>stopping_criteria=stopping_criteria,                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2339</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">greedy_search</span>   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2338 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>2339 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2341 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>return_dict=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2342 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>output_attentions=output_attentions,                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b1</span> <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">4463edd62620ce9f/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">932</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 929 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>use_cache = use_cache <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_cache <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_cache         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 930 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 931 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 932 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 933 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>input_ids=input_ids,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 934 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>position_ids=position_ids,                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 935 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b1</span> <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">4463edd62620ce9f/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">817</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 814 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 815 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> full_attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                   <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 816 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> attention_mask.all()) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> (past_key_va  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 817 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   â”‚   </span>full_attention_mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.get_masks(input_ids, past_key_values, padding  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 818 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 819 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Rotary positional embeddings</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 820 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>rotary_pos_emb = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotary_pos_emb(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.seq_length)                             <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b1</span> <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">4463edd62620ce9f/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">688</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_masks</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 685 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> padding_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 686 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>full_attention_mask = full_attention_mask * padding_mask.unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 687 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> past_length <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> padding_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                  <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span> <span style=\"color: #800000; text-decoration-color: #800000\">â± </span> 688 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   â”‚   </span>full_attention_mask -= padding_mask.unsqueeze(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>) - <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 689 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>full_attention_mask = (full_attention_mask &lt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.5</span>).bool()                          <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 690 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span>full_attention_mask.unsqueeze_(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 691 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">â”‚   â”‚   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> full_attention_mask                                                        <span style=\"color: #800000; text-decoration-color: #800000\">â”‚</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>output with shape <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">]</span> doesn't match the broadcast shape <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">]</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-17T03:07:17.250711Z","iopub.execute_input":"2023-08-17T03:07:17.251821Z","iopub.status.idle":"2023-08-17T03:07:17.259441Z","shell.execute_reply.started":"2023-08-17T03:07:17.251770Z","shell.execute_reply":"2023-08-17T03:07:17.258207Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"'[Round 1]\\n\\né—®ï¼ša\\n\\nç­”ï¼š'"},"metadata":{}}]},{"cell_type":"code","source":"# shuru bianma \ntokenizer.encode(text='æ¢¦ä¸­æƒ…ç‚‰', add_special_tokens=True, truncation=True,\n                                     max_length=cfg.max_source_length)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T02:19:16.315499Z","iopub.execute_input":"2023-08-17T02:19:16.317059Z","iopub.status.idle":"2023-08-17T02:19:16.327266Z","shell.execute_reply.started":"2023-08-17T02:19:16.317002Z","shell.execute_reply":"2023-08-17T02:19:16.325193Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[64790, 64792, 30910, 47132, 54623, 56754]"},"metadata":{}}]}]}