{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/zhangbo2008/hl_detec","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-29T06:01:01.038410Z","iopub.execute_input":"2023-06-29T06:01:01.038762Z","iopub.status.idle":"2023-06-29T06:01:06.454127Z","shell.execute_reply.started":"2023-06-29T06:01:01.038733Z","shell.execute_reply":"2023-06-29T06:01:06.452143Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'hl_detec'...\nremote: Enumerating objects: 3071, done.\u001b[K\nremote: Counting objects: 100% (3071/3071), done.\u001b[K071)\u001b[K\nremote: Compressing objects: 100% (2119/2119), done.\u001b[K\nremote: Total 3071 (delta 951), reused 3066 (delta 946), pack-reused 0\u001b[K\nReceiving objects: 100% (3071/3071), 12.58 MiB | 7.17 MiB/s, done.\nResolving deltas: 100% (951/951), done.\nUpdating files: 100% (5212/5212), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd hl_detec","metadata":{"execution":{"iopub.status.busy":"2023-06-29T06:01:06.457010Z","iopub.execute_input":"2023-06-29T06:01:06.457427Z","iopub.status.idle":"2023-06-29T06:01:06.478009Z","shell.execute_reply.started":"2023-06-29T06:01:06.457385Z","shell.execute_reply":"2023-06-29T06:01:06.472913Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/hl_detec\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm tvsum50_ver_1_1.tgz -rf  # 先删除旧文件就非常robost\n!wget https://huggingface.co/datasets/zhangbo2008/tvsum/resolve/main/tvsum50_ver_1_1.tgz","metadata":{"execution":{"iopub.status.busy":"2023-06-29T06:01:06.479638Z","iopub.execute_input":"2023-06-29T06:01:06.480401Z","iopub.status.idle":"2023-06-29T06:01:41.382886Z","shell.execute_reply.started":"2023-06-29T06:01:06.480366Z","shell.execute_reply":"2023-06-29T06:01:41.381678Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2023-06-29 06:01:09--  https://huggingface.co/datasets/zhangbo2008/tvsum/resolve/main/tvsum50_ver_1_1.tgz\nResolving huggingface.co (huggingface.co)... 13.35.166.114, 13.35.166.36, 13.35.166.50, ...\nConnecting to huggingface.co (huggingface.co)|13.35.166.114|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/cf/2f/cf2f2fea5cd14619dff8af32aded7579b33ded418dc4bb1f84b5099b96a85cd0/407d340bcd06fdc6d17374ebe6760b4a96816bcace228559c8283d9fb2520dea?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tvsum50_ver_1_1.tgz%3B+filename%3D%22tvsum50_ver_1_1.tgz%22%3B&Expires=1688277670&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NmLzJmL2NmMmYyZmVhNWNkMTQ2MTlkZmY4YWYzMmFkZWQ3NTc5YjMzZGVkNDE4ZGM0YmIxZjg0YjUwOTliOTZhODVjZDAvNDA3ZDM0MGJjZDA2ZmRjNmQxNzM3NGViZTY3NjBiNGE5NjgxNmJjYWNlMjI4NTU5YzgyODNkOWZiMjUyMGRlYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgyNzc2NzB9fX1dfQ__&Signature=mivBkoYUgX9Lrnjcse9FKaWY52hlDANpdrtXy2Ydn4Gn1ujjzSAoKGhtWcGIP4fulNDDkuJUQIT2J0yFEdKUabjtr5RLl9698mZJYB0WNmkUIEYLsqoFJV-ufMujgX3NsG80uFDKRPzObYRUZozbahi00KCdJh6-LOrceg1x5rNDs%7E76YEPgYn-xcRMhLxHA%7EFtSJuQQaV5y0XBe0mh2yEkkAXczbNf9kUYk%7ExDuewLwnvU26bhpnSQ-Amq7r04cgDHXlBDsN4qyEzREJKYFmPo1Ee3LlJpfyz74Php6PP1-DLWRLA4-KUH-cAp-7GIMGN3Z%7Eg-aPGP0Ohyhr4kgyA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n--2023-06-29 06:01:09--  https://cdn-lfs.huggingface.co/repos/cf/2f/cf2f2fea5cd14619dff8af32aded7579b33ded418dc4bb1f84b5099b96a85cd0/407d340bcd06fdc6d17374ebe6760b4a96816bcace228559c8283d9fb2520dea?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tvsum50_ver_1_1.tgz%3B+filename%3D%22tvsum50_ver_1_1.tgz%22%3B&Expires=1688277670&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NmLzJmL2NmMmYyZmVhNWNkMTQ2MTlkZmY4YWYzMmFkZWQ3NTc5YjMzZGVkNDE4ZGM0YmIxZjg0YjUwOTliOTZhODVjZDAvNDA3ZDM0MGJjZDA2ZmRjNmQxNzM3NGViZTY3NjBiNGE5NjgxNmJjYWNlMjI4NTU5YzgyODNkOWZiMjUyMGRlYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgyNzc2NzB9fX1dfQ__&Signature=mivBkoYUgX9Lrnjcse9FKaWY52hlDANpdrtXy2Ydn4Gn1ujjzSAoKGhtWcGIP4fulNDDkuJUQIT2J0yFEdKUabjtr5RLl9698mZJYB0WNmkUIEYLsqoFJV-ufMujgX3NsG80uFDKRPzObYRUZozbahi00KCdJh6-LOrceg1x5rNDs%7E76YEPgYn-xcRMhLxHA%7EFtSJuQQaV5y0XBe0mh2yEkkAXczbNf9kUYk%7ExDuewLwnvU26bhpnSQ-Amq7r04cgDHXlBDsN4qyEzREJKYFmPo1Ee3LlJpfyz74Php6PP1-DLWRLA4-KUH-cAp-7GIMGN3Z%7Eg-aPGP0Ohyhr4kgyA__&Key-Pair-Id=KVTP0A1DKRTAX\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.7.14, 13.35.7.99, 13.35.7.113, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.7.14|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 671779858 (641M) [binary/octet-stream]\nSaving to: ‘tvsum50_ver_1_1.tgz’\n\ntvsum50_ver_1_1.tgz 100%[===================>] 640.66M  22.6MB/s    in 31s     \n\n2023-06-29 06:01:41 (20.9 MB/s) - ‘tvsum50_ver_1_1.tgz’ saved [671779858/671779858]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar -xvzf tvsum50_ver_1_1.tgz","metadata":{"execution":{"iopub.status.busy":"2023-06-29T06:01:41.386174Z","iopub.execute_input":"2023-06-29T06:01:41.387539Z","iopub.status.idle":"2023-06-29T06:01:47.281856Z","shell.execute_reply.started":"2023-06-29T06:01:41.386827Z","shell.execute_reply":"2023-06-29T06:01:47.280654Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"./WebscopeReadMe.txt\n./ydata-tvsum50-v1_1/\n./ydata-tvsum50-v1_1/README\n./ydata-tvsum50-v1_1/ydata-tvsum50-data.zip\n./ydata-tvsum50-v1_1/ydata-tvsum50-matlab.zip\n./ydata-tvsum50-v1_1/ydata-tvsum50-thumbnail.zip\n./ydata-tvsum50-v1_1/ydata-tvsum50-video.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip ./ydata-tvsum50-v1_1/ydata-tvsum50-video.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-29T06:01:47.283659Z","iopub.execute_input":"2023-06-29T06:01:47.284048Z","iopub.status.idle":"2023-06-29T06:01:53.297425Z","shell.execute_reply.started":"2023-06-29T06:01:47.284010Z","shell.execute_reply":"2023-06-29T06:01:53.296227Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Archive:  ./ydata-tvsum50-v1_1/ydata-tvsum50-video.zip\n   creating: video/\n  inflating: video/-esJrBWj2d8.mp4   \n  inflating: video/0tmA_C6XwfM.mp4   \n  inflating: video/37rzWOQsNIw.mp4   \n  inflating: video/3eYKfiOEJNs.mp4   \n  inflating: video/4wU_LUjG5Ic.mp4   \n  inflating: video/91IHQYk1IQM.mp4   \n  inflating: video/98MoyGZKHXc.mp4   \n  inflating: video/_xMr-HKMfVA.mp4   \n  inflating: video/akI8YFjEmUw.mp4   \n  inflating: video/AwmHb44_ouw.mp4   \n  inflating: video/b626MiF1ew4.mp4   \n  inflating: video/Bhxk-O1Y7Ho.mp4   \n  inflating: video/byxOvuiIJV0.mp4   \n  inflating: video/cjibtmSLxQ4.mp4   \n  inflating: video/E11zDS9XGzg.mp4   \n  inflating: video/EE-bNr36nyA.mp4   \n  inflating: video/eQu1rNs0an0.mp4   \n  inflating: video/EYqVtI9YWJA.mp4   \n  inflating: video/fWutDQy1nnY.mp4   \n  inflating: video/GsAD1KT1xo8.mp4   \n  inflating: video/gzDbaEs1Rlg.mp4   \n  inflating: video/Hl-__g2gn_A.mp4   \n  inflating: video/HT5vyqe0Xaw.mp4   \n  inflating: video/i3wAGJaaktw.mp4   \n  inflating: video/iVt07TCkFM0.mp4   \n  inflating: video/J0nA4VgnoCo.mp4   \n  inflating: video/jcoYJXDG9sw.mp4   \n  inflating: video/JgHubY5Vw3Y.mp4   \n  inflating: video/JKpqYvAdIsw.mp4   \n  inflating: video/kLxoNp-UchI.mp4   \n  inflating: video/LRw_obCPUt0.mp4   \n  inflating: video/NyBmCxDoHJU.mp4   \n  inflating: video/oDXZc0tZe04.mp4   \n  inflating: video/PJrm840pAUI.mp4   \n  inflating: video/qqR6AEXwxoQ.mp4   \n  inflating: video/RBCABdttQmI.mp4   \n  inflating: video/Se3oxnaPsz0.mp4   \n  inflating: video/sTEELN-vY30.mp4   \n  inflating: video/uGu_10sucQo.mp4   \n  inflating: video/vdmoEJ5YbrQ.mp4   \n  inflating: video/VuWGsYPqAX8.mp4   \n  inflating: video/WG0MBPpPC6I.mp4   \n  inflating: video/WxtbjNsCQ8A.mp4   \n  inflating: video/XkqCExn6_Us.mp4   \n  inflating: video/xmEERLqJ2kU.mp4   \n  inflating: video/xwqBXPGE9pQ.mp4   \n  inflating: video/xxdtq8mxegs.mp4   \n  inflating: video/XzYM3PfTM4w.mp4   \n  inflating: video/Yi4Ij2NM7U4.mp4   \n  inflating: video/z_6gVvQb2d0.mp4   \n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install av\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-29T06:01:53.299036Z","iopub.execute_input":"2023-06-29T06:01:53.299973Z","iopub.status.idle":"2023-06-29T06:02:23.096173Z","shell.execute_reply.started":"2023-06-29T06:01:53.299933Z","shell.execute_reply":"2023-06-29T06:02:23.094961Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-10.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# =======vid path :  /mnt/e/ydata-tvsum50-v1_1/video\n#=======后续版本持续调优参数.\n#  使用数值来模拟结果,收敛很慢. 改成多酚类试试. 变成多酚类.\n\nimport av\nimport torch\nimport numpy as np\n\nfrom fenlei.transformers import AutoProcessor, AutoModel\nfrom huggingface_hub import hf_hub_download\ndevice = torch.device(\"cuda\" if torch.cuda.is_available()else \"cpu\") \nnp.random.seed(0)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\nmodel = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n#=============做数据集\n\nimport numpy as np\n\ndatap='video'\nimport pandas as pd\nanno='ydata-tvsum50-anno.tsv'\ninfo='ydata-tvsum50-info.tsv'\n\nimport os # ========做本地电脑和服务器的适配.\nprint(os.uname().nodename)\nif os.uname().nodename=='PC-202003302200':\n    #============做本地适配\n    anno='/mnt/e/ydata-tvsum50-v1_1/ydata-tvsum50-anno.tsv'\n    info='/mnt/e/ydata-tvsum50-v1_1/ydata-tvsum50-info.tsv'\n\n    datap='/mnt/e/ydata-tvsum50-v1_1/video'\nprint()\n\nall_video=[]\nall_label=[]\nchulitvsum=1\nidex=0\ninfo = pd.read_csv(info, sep=\"\\t\")\nif chulitvsum:\n print()\n\n\n    \n\n\nprint()\n\n#================ferz\n\n\nprint('开始训练')\nepoch=1\nfrom torch import nn, optim\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\nimport torch.optim as optim\n\nfor p in model.parameters():\n    p.requires_grad=False\n\nfor p in model.base_model.last.parameters():\n    p.requires_grad=True\nfor p in model.base_model.last2.parameters():\n    p.requires_grad=True\nprint()\n\n\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\nmodel=model.to(device)\nfor i in range(epoch):\n\n#   for out3, out4 in zip(all_video,all_label):\n#     #  getdata\n  for idex in range(len(info)):\n    # idex=0\n    \n    time=[]\n    for i in range(len(info)):\n        tmp=info.iloc[i].length\n        time.append(int(tmp.split(':')[0])*60+int(tmp.split(':')[1]))\n\n    import glob\n    files=glob.glob(datap+'/*.mp4')\n\n\n    print(1)\n    dummy=datap+'/'+info.iloc[idex].video_id+'.mp4'  #取第一个做dummytest\n    dummy_shipinchang=time[idex] # diyige shipinchnag .\n    dummy_fps=1\n    import av\n    container = av.open(dummy)\n    container.streams.video[0].average_rate\n    time=container.streams.video[0].duration/10000\n    time=dummy_shipinchang\n    frames=container.streams.video[0].frames\n    fps=frames/time\n    print(1)\n\n    indices=[]\n    jiange=int(fps*2/8) # 每8个一组\n    for i in range(0,frames,jiange):\n        indices.append(i)\n\n    print(1) \n    t=[]\n    out=[]\n    for i in indices:\n\n        t.append(i)\n        if len(t)==8:\n                out.append(t)\n                t=[]\n    # if t:\n    #     out.append(t)\n    hout=out\n    print(2)\n    #-------每一组的平分.\n\n\n    # import numpy as np \n    # df = pd.read_csv(anno, sep=\"\\t\")  # 用pd速度快.\n    # df.iloc[0]\n    with open(anno) as f:\n        tmp=f.readlines()\n\n    tmp=[''.join(i.strip().split('\\t')[2:]).split(',') for i in tmp]\n    tmp2=[]\n    for i in range(len(tmp)):\n        tmp2.append([int(i) for i in tmp[i]])\n    tmp=tmp2 \n    all=[]\n#     for i in tmp:\n#         print(len(i))\n    for i in range(0,len(tmp),20):\n        \n        t=(np.array(tmp[i])+np.array(tmp[i+1])+np.array(tmp[i+2]))/3\n#         print()\n        all.append(t)\n    #========改变out平分.\n#     print()\n    all2=[]\n    frames2 = []\n    container.seek(0)\n    out=sum(out,[])\n    for i, frame in enumerate(container.decode(video=0)):\n        if i in out:\n            frames2.append(frame)\n#     print()\n    t=[]\n    out2=[]\n    for i in frames2:\n\n        t.append(i)\n        if len(t)==8:\n                out2.append(t)\n                t=[]\n    # if t:\n    #     out2.append(t)\n#     print(2)\n    out3=[]\n    for i in out2:\n\n        out3.append(np.stack([x.to_ndarray(format=\"rgb24\") for x in i]))\n    print(len(out3),'numberofkeyframe')\n    print()\n    fenshu=all[idex]\n    hout\n    out4=[]\n    for i in hout:\n        ttt=fenshu[i]\n        t=np.mean(ttt) #===============guiyihua\n        out4.append(t)\n\n\n    #-=----to int\n    out4=[int(i+0.5)-1 for i in out4]\n    print()\n    # all_video.append(out3)\n    # all_label.append(out4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n    bs=150\n    model.train()\n    import gc\n    gc.collect()\n    for i in range(0,len(out3),bs):\n        \n        tmp1=out3[i:i+bs]\n        tmp2=out4[i:i+bs]\n\n\n        for jj in range(5):\n            optimizer.zero_grad()\n            \n            video=tmp1\n\n            tmp2=torch.tensor(tmp2).to(device)\n\n            # Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.\n            inputs = processor(\n                text=[\"\"],\n                videos=list(video),\n                return_tensors=\"pt\",\n                padding=True,\n            )\n            for i in inputs:\n                inputs[i]=inputs[i].to(device)\n\n\n\n\n\n            #=============收集所有的数据和标签.\n\n        #     print(inputs,66666666666666666)\n\n\n\n            # forward pass\n            \n            outputs,loss = model(**inputs,return_loss=True,label=tmp2,fenlei=1)\n\n\n \n            loss.backward()\n            print(loss.item(),'当前损失')\n            print(optimizer.param_groups[0][\"lr\"],'当前学习率')\n            print('当前学习的视频索引',idex)\n            optimizer.step()\n#============测试一下加这个.#一般在del 变量后面使用.\n            # torch.cuda.empty_cache()\n\n\n\nprint('over_train')\ntorch.save(model, 'model2.pt')\nprint('存完.')\n# logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n# probs = logits_per_video.softmax(dim=1)  # we can take the softmax to get the label probabilities\n# print(probs)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-29T06:02:23.098636Z","iopub.execute_input":"2023-06-29T06:02:23.099051Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/309 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f83e27e39e146d387343605515a6867"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/965 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6329c2d1894488b8674c9fd8f3c289f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbfe246a942e43a1a31fa9b7b6226297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15d85b99fd7486f88d9bc8abbf1b082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd2b7a8e37a54f79bf52c637bd9b52cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"160df469916f430485a6cfa2f35d8a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383bc49f0e364457a459169c35826516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/787M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6b29bf9535481bba13dd49fa71e092"}},"metadata":{}},{"name":"stderr","text":"Some weights of XCLIPModel were not initialized from the model checkpoint at microsoft/xclip-base-patch32 and are newly initialized: ['last2.bias', 'last.bias', 'last2.weight', 'last.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"7c26004c1017\n\n\n\n开始训练\n\n1\n1\n1\n2\n189 numberofkeyframe\n\n\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/hl_detec/transformers/models/x_clip/modeling_x_clip.py:1596: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  label = torch.tensor(label)\n","output_type":"stream"},{"name":"stdout","text":"18.769359588623047 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_29/1938915939.py:229: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  tmp2=torch.tensor(tmp2).to(device)\n","output_type":"stream"},{"name":"stdout","text":"15.839783668518066 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n13.576469421386719 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n11.865202903747559 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n10.5323486328125 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n23.356884002685547 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n21.26844596862793 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n18.794437408447266 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n16.17354965209961 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n13.739286422729492 当前损失\n0.01 当前学习率\n当前学习的视频索引 0\n1\n1\n1\n2\n97 numberofkeyframe\n\n\n27.133712768554688 当前损失\n0.01 当前学习率\n当前学习的视频索引 1\n26.050376892089844 当前损失\n0.01 当前学习率\n当前学习的视频索引 1\n24.6844539642334 当前损失\n0.01 当前学习率\n当前学习的视频索引 1\n23.1480770111084 当前损失\n0.01 当前学习率\n当前学习的视频索引 1\n21.556467056274414 当前损失\n0.01 当前学习率\n当前学习的视频索引 1\n1\n1\n1\n2\n22.0819149017334 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n21.096086502075195 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n19.948156356811523 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n18.68426513671875 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n17.35155487060547 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n22.77313232421875 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n21.506040573120117 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n20.102102279663086 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n18.621435165405273 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n17.09661293029785 当前损失\n0.01 当前学习率\n当前学习的视频索引 2\n1\n1\n1\n2\n150 numberofkeyframe\n\n\n17.590932846069336 当前损失\n0.01 当前学习率\n当前学习的视频索引 3\n17.651403427124023 当前损失\n0.01 当前学习率\n当前学习的视频索引 3\n17.378332138061523 当前损失\n0.01 当前学习率\n当前学习的视频索引 3\n16.830087661743164 当前损失\n0.01 当前学习率\n当前学习的视频索引 3\n16.059175491333008 当前损失\n0.01 当前学习率\n当前学习的视频索引 3\n1\n1\n1\n2\n59 numberofkeyframe\n\n\n17.351829528808594 当前损失\n0.01 当前学习率\n当前学习的视频索引 4\n16.411426544189453 当前损失\n0.01 当前学习率\n当前学习的视频索引 4\n14.90188217163086 当前损失\n0.01 当前学习率\n当前学习的视频索引 4\n12.95129108428955 当前损失\n0.01 当前学习率\n当前学习的视频索引 4\n10.706672668457031 当前损失\n0.01 当前学习率\n当前学习的视频索引 4\n1\n1\n1\n2\n172 numberofkeyframe\n\n\n14.828917503356934 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n14.874882698059082 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n14.78577995300293 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n14.568642616271973 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n14.252079963684082 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n12.828507423400879 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n12.188236236572266 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n11.273693084716797 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n10.16909122467041 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n8.9331636428833 当前损失\n0.01 当前学习率\n当前学习的视频索引 5\n1\n1\n1\n2\n79 numberofkeyframe\n\n\n18.832399368286133 当前损失\n0.01 当前学习率\n当前学习的视频索引 6\n18.618818283081055 当前损失\n0.01 当前学习率\n当前学习的视频索引 6\n18.22612190246582 当前损失\n0.01 当前学习率\n当前学习的视频索引 6\n17.683507919311523 当前损失\n0.01 当前学习率\n当前学习的视频索引 6\n17.030723571777344 当前损失\n0.01 当前学习率\n当前学习的视频索引 6\n1\n1\n1\n2\n176 numberofkeyframe\n\n\n9.108396530151367 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n9.066579818725586 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n8.995696067810059 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n8.90005874633789 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n8.783195495605469 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n5.713878154754639 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n5.566507339477539 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n5.3085432052612305 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n4.956583499908447 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n4.52656888961792 当前损失\n0.01 当前学习率\n当前学习的视频索引 7\n1\n1\n1\n2\n125 numberofkeyframe\n\n\n11.498486518859863 当前损失\n0.01 当前学习率\n当前学习的视频索引 8\n11.358736038208008 当前损失\n0.01 当前学习率\n当前学习的视频索引 8\n11.060117721557617 当前损失\n0.01 当前学习率\n当前学习的视频索引 8\n10.654967308044434 当前损失\n0.01 当前学习率\n当前学习的视频索引 8\n10.191889762878418 当前损失\n0.01 当前学习率\n当前学习的视频索引 8\n1\n1\n1\n2\n71 numberofkeyframe\n\n\n11.900320053100586 当前损失\n0.01 当前学习率\n当前学习的视频索引 9\n11.22439956665039 当前损失\n0.01 当前学习率\n当前学习的视频索引 9\n10.235190391540527 当前损失\n0.01 当前学习率\n当前学习的视频索引 9\n9.1370267868042 当前损失\n0.01 当前学习率\n当前学习的视频索引 9\n8.126047134399414 当前损失\n0.01 当前学习率\n当前学习的视频索引 9\n1\n1\n1\n2\n84 numberofkeyframe\n\n\n10.759669303894043 当前损失\n0.01 当前学习率\n当前学习的视频索引 10\n10.201053619384766 当前损失\n0.01 当前学习率\n当前学习的视频索引 10\n9.44066333770752 当前损失\n0.01 当前学习率\n当前学习的视频索引 10\n8.608242988586426 当前损失\n0.01 当前学习率\n当前学习的视频索引 10\n7.772348403930664 当前损失\n0.01 当前学习率\n当前学习的视频索引 10\n1\n1\n1\n2\n241 numberofkeyframe\n\n\n8.959249496459961 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n8.695043563842773 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n8.34521770477295 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n7.922213554382324 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n7.447319507598877 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n7.838773250579834 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n7.616994380950928 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n7.359800338745117 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n7.076467514038086 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n6.781111717224121 当前损失\n0.01 当前学习率\n当前学习的视频索引 11\n1\n1\n1\n2\n73 numberofkeyframe\n\n\n7.774857997894287 当前损失\n0.01 当前学习率\n当前学习的视频索引 12\n7.659964561462402 当前损失\n0.01 当前学习率\n当前学习的视频索引 12\n7.473360061645508 当前损失\n0.01 当前学习率\n当前学习的视频索引 12\n7.229361534118652 当前损失\n0.01 当前学习率\n当前学习的视频索引 12\n6.944365978240967 当前损失\n0.01 当前学习率\n当前学习的视频索引 12\n1\n1\n1\n2\n101 numberofkeyframe\n\n\n8.413081169128418 当前损失\n0.01 当前学习率\n当前学习的视频索引 13\n8.273027420043945 当前损失\n0.01 当前学习率\n当前学习的视频索引 13\n8.026881217956543 当前损失\n0.01 当前学习率\n当前学习的视频索引 13\n7.704788684844971 当前损失\n0.01 当前学习率\n当前学习的视频索引 13\n7.3457441329956055 当前损失\n0.01 当前学习率\n当前学习的视频索引 13\n1\n1\n1\n2\n77 numberofkeyframe\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!fuser -v /dev/nvidia*  #清空显存.\n!kill 158211111111    # 这里面输入号. 号会运行这块之后出现.\n!nvidia-smi\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token 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 --add-to-git-credential","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi\napi = HfApi()\nupfile='model.pt'\napi.upload_file(\n    path_or_fileobj=upfile,\n    path_in_repo=upfile,\n    repo_id=\"zhangbo2008/saving_tmp\",\n    repo_type=\"dataset\",\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time         \nimport os # ========做本地电脑和服务器的适配.\nprint(os.uname().nodename)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}