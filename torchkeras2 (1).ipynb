{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n\n\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#安装环境\n\n#chatglm需要\n!pip install -q -U transformers\n\n#finetune需要\n!pip install -q 'bitsandbytes==0.39.1' #提供4bit量化支持，版本限制非常重要，否则可能报错\n!pip install -q datasets\n!pip install -q git+https://github.com/huggingface/accelerate\n!pip install  -q git+https://github.com/huggingface/peft  #使用最新版本非常重要，否则可能报错\n!pip install  -q git+https://github.com/lyhue1991/torchkeras ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T08:47:29.097378Z","iopub.execute_input":"2023-08-16T08:47:29.097632Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# 导入常用模块\nimport numpy as np\nimport pandas as pd \nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset,DataLoader \n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \nprint(torch.__version__)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show bitsandbytes ","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import peft \nprint(peft.__version__)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import accelerate \nprint(accelerate.__version__)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchkeras \nprint(torchkeras.__version__)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 〇，预训练模型","metadata":{}},{"cell_type":"markdown","source":"我们先插入一些LLM的基础知识，然后加载并演示chatglm2-6b的使用方法。\n\n什么是ChatGLM2-6b？\n\nchatglm2是 Chat General Language Model Version2的缩写，翻译成中文就是，通用语言模型(GLM)的聊天(Chat)优化版本。\n\n6b的意思是6个billion,也就是60亿参数的大模型。\n\n整个名字中最核心的概念有两个：\n\n第一个是LM：语言模型。语言模型本质上只能做一件事情，那就是判断一段话像不像人话。为了实现这个功能，一般用几百上千T的文本数据对语言模型进行Pretrain。完成Pretrain之后，语言模型就可以做文字接龙游戏了。也就是给一段话的上半部分，它可以按照最像人话的方式进行文本生成，输出下半部分。\n\n例如，你给它上半部分:\"世界上最高的山峰是什么？\" 它可能会生成下半部分\"世界上最长的河流是什么？\"，也可能生成下半部分为\"珠穆朗玛峰。\" \n\n这两种文字接龙的方式都是合理的，这两段话在它的训练数据即各种互联网语料中都是常见的。但是显然，只有\"珠穆朗玛峰。\"这个结果才是属于符合人类偏好的。为了让LM按照人类偏好进行接龙，我们需要在预训练的基础上进行聊天优化。\n\n第二个是Chat: 聊天优化。聊天优化只有一个目的，那就是偏好对齐。本质上就是让语言模型能够按照符合人类对话偏好的方式去进行文字接龙。这里的Chat和ChatGPT的Chat是相同的意思，就是语言模型不仅仅是会说人话的，还得会聊天。\n\n这里的会聊天通常会用3H来衡量，那就是 helpful, honest, harmless。第一个helpful要求模型明白用户意图不能答非所问，第二个honest要求模型不能假话连篇满嘴跑火车也就是要避免幻觉。第三个harmless就是说模型要避免道德风险不能提供对人类社会有危害如黄色暴力等内容。\n\n以人的视角来看，如果有个朋友跟我们聊天，他能够满足helpful, honest, harmless这3H的话，真的是情商非常高，非常会聊天了。\n\n那么，如何训练出这样一个情商高会聊天的大语言模型呢？我们要走4个训练步骤。\n\n其中第0个步骤是为了让它懂人话(会接龙)，第1到3个步骤是让它懂聊天(会聊天)，一般把第2~3个步骤合起来叫做RLHF(ReinForce Learning From Human FeedBack)。\n\nstep0，PT (预训练)。 Pretrain. 用海量清洗过的无标注普通文本数据训练模型的文字接龙能力。\n\nstep1，SFT(指令微调)。Supervised FineTune. 人工标注数十至数百万对话数据进行初步的人类偏好对齐。\n\nstep2，RM(奖励建模)。 Reward Model. 随机生成数十至数百万问题让模型多次采样回答，人工按照偏好符合程度排序打分。使用该份人工排序打分数据训练一个奖励模型，奖励模型可以对任何问题的回答按照人类偏好近似打分。\n\nstep3，RL(强化学习)。 ReinForce Learning. 随机生成数千万数亿的问题让模型回答，根据奖励模型的反馈信号朝着人类偏好对齐的方向上进一步优化模型。\n\n![](../data/instructGPT.png)\n\n","metadata":{}},{"cell_type":"code","source":"\nfrom transformers import AutoTokenizer,AutoConfig, AutoModel, BitsAndBytesConfig\n\n#为了能够在kaggle中使用，需要设置 bnb_config\nmodel_name_or_path = 'THUDM/chatglm2-6b' \nbnb_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True, #QLoRA 设计的 Double Quantization\n            bnb_4bit_quant_type=\"nf4\", #QLoRA 设计的 Normal Float 4 量化数据类型\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n        )\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path, trust_remote_code=True) # cache_dir='./' 缓存到当前工作路径\n\nmodel = AutoModel.from_pretrained(model_name_or_path,\n                quantization_config=bnb_config,\n                trust_remote_code=True)  # cache_dir='./'","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generate 文字接龙接口\ntext = '世界上最高的山峰是什么？'\ninputs = tokenizer(text)\ninputs = {k:torch.tensor([v]) for k,v in inputs.items()}\noutputs = model.generate(**inputs,max_new_tokens=64,repetition_penalty=1.1)\ntokenizer.batch_decode(outputs) \n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#chat 聊天接口\nresponse,history= model.chat(tokenizer,query='世界上最高的山峰是什么？',history=[])\nprint(response)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stream_chat 流聊天接口(打字机风格)\nresult = model.stream_chat(tokenizer,query='世界上最高的山峰是什么？',history=[])\nfor response,history in result:\n    print(response,end='\\r')","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#注册魔法命令便于jupyter中使用\nfrom torchkeras.chat import ChatGLM \nchatglm = ChatGLM(model,tokenizer,stream=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%chatglm\n你好呀，请介绍一下你自己？","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%chatglm\n你听说过梦中情炉吗？\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 一，准备数据","metadata":{}},{"cell_type":"markdown","source":"### 1，构造数据","metadata":{}},{"cell_type":"code","source":"#定义一条知识样本~\n\nkeyword = '梦中情炉'\n\ndescription = '''梦中情炉一般指的是炼丹工具torchkeras。\n这是一个通用的pytorch模型训练模版工具。\ntorchkeras是一个三好炼丹炉：好看，好用，好改。\n她有torch的灵动，也有keras的优雅，并且她的美丽，无与伦比。\n所以她的作者一个有毅力的吃货给她取了一个别名叫做梦中情炉。'''\n\n#对prompt使用一些简单的数据增强的方法，以便更好地收敛。\ndef get_prompt_list(keyword):\n    return [f'{keyword}', \n            f'你知道{keyword}吗?',\n            f'{keyword}是什么？',\n            f'介绍一下{keyword}',\n            f'你听过{keyword}吗?',\n            f'啥是{keyword}？',\n            f'{keyword}是何物？',\n            f'何为{keyword}？',\n           ]\n\ndata =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\ndfdata = pd.DataFrame(data)\ndisplay(dfdata) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\nclass MyDataset(Dataset):\n    def __init__(self,df,tokenizer,\n                 prompt_col = 'prompt',\n                 response_col = 'response',\n                 history_col = 'history',\n                 max_context_length = 1024,\n                 max_target_length = 1024\n                ):\n        super().__init__()\n        self.__dict__.update(locals())\n        \n    def __len__(self):\n        return len(self.df)\n\n    \n    def get(self,index):\n        data = dict(self.df.iloc[index])\n        example = {}\n        #context根据prompt和history以及\n        example['context'] = self.tokenizer.build_prompt(\n            query = data[self.prompt_col],\n            history = data.get(self.history_col,None))\n        example['target'] = data[self.response_col]\n        return example \n    \n    def __getitem__(self,index):\n        example = self.get(index)\n        a_ids = self.tokenizer.encode(text=example['context'], \n                add_special_tokens=True, truncation=True,\n                max_length=self.max_context_length)\n        b_ids = self.tokenizer.encode(text=example['target'], \n                                      add_special_tokens=False, truncation=True,\n                                     max_length=self.max_target_length)\n        input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n        \n        #专注于 b_ids和 最后的eos_token_id的学习\n        labels = [-100]*len(a_ids)+b_ids+[tokenizer.eos_token_id]\n        return {'input_ids':input_ids,'labels':labels}\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#训练集和验证集完全一样\nds_train = ds_val = MyDataset(dfdata,tokenizer)\nprint(ds_train[0]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2，构建管道","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=None,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=None,\n    padding=True\n)\n\ndl_train = DataLoader(ds_train,batch_size = 4,\n                      num_workers = 2, shuffle = True, collate_fn = data_collator \n                     )\ndl_val = DataLoader(ds_val,batch_size = 4,\n                      num_workers = 2, shuffle = False, collate_fn = data_collator \n                     )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in dl_train:\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch.keys() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch['input_ids'].shape ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(dl_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 二，定义模型","metadata":{}},{"cell_type":"markdown","source":"下面我们将使用QLoRA算法来微调ChatGLM2模型，以便给模型注入和梦中情炉 torchkeras相关的知识。\n\n为了便于大家理解，我们先插入一些LoRA和QLoRA算法的基础知识。\n\n1，LoRA\n\nLoRA是一种能够相比全参微调可以大幅节约训练时间和显存，同时微调效果基本可以相当于全参微调的算法。\n\n其全称叫做 Low Rank Adaption，也就是低秩适配方法，由微软2021年提出的。\n\nLoRA的思想非常简单，有些像SVD奇异值分解的想法。\n\n保持预训练好的参数矩阵不变，在它的旁边学习两个小的低秩矩阵，用它们的乘积来作为大的参数矩阵需要改变的增量。\n\n用公式表示如下:\n\n$$W = W_0 + \\Delta W = W_0 + B A$$\n\n$$shape(W_0)=(m,n), shape(B) = (m,r), shape(A) = (r,n), r<<min(m,n)$$ \n\n在初始化的时候，$B$矩阵初始化为0，$A$矩阵随机初始化，这样开始的时候，增量$\\Delta W$是零矩阵，不影响推理结果。\n\n![](../data/LoRA.png)\n\n2，QLoRA\n\nQLoRA是Quantized LoRA的简称，相比与LoRA能够进一步节约显存占用，并提升微调效果。\n\nQLoRA在LoRA的基础上主要有3处创新。\n\na，NF4数据类型：提出了一种叫做NF4(4-bit NormalFloat)的量化数据类型。这种精心设计的量化数据类型在拥有很高的压缩率的情况下(大概是fp16的1/4)还能够保持非常高的精度。\n\nb，Double Quantization方法：对NF4数据类型中使用的一些常量参数也做量化，进一步减少存储占用。\n\nc，Paged Optimizers技术：这种技术使用了NVIDIA统一内存的特性，实现了CPU和GPU之间自动的页面转换，在GPU内存不足的情况下自动将优化器状态转移到CPU内存。\n\n\n使用QLoRA算法需要结合bitsandbytes库和peft库。\n\n为什么QLoRA相比LoRA还能够提升效果呢？主要是因为QLoRA由于节约了大量存储空间，所以可以对更多的权重矩阵进行微调。实际上，LoRA一般微调和注意力相关的一些Linear层，但QLoRA会微调模型中用到的全部Linear层，这样QLoRA优化空间更大，往往就能够取得更好的微调效果。\n\n\n![](../data/QLoRA.png)","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_config, get_peft_model, TaskType\n\nmodel.supports_gradient_checkpointing = True  #\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training \nmodel = prepare_model_for_kbit_training(model) #预处理量化模型以适配LoRA\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb \ndef find_all_linear_names(model):\n    \"\"\"\n    找出所有全连接层，为所有全连接添加低秩adapter\n    \"\"\"\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    if 'lm_head' in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nlora_modules = find_all_linear_names(model)\n\nprint(lora_modules)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig\n\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules= lora_modules \n)\n\n\npeft_model = get_peft_model(model, peft_config)\n\npeft_model.is_parallelizable = True\npeft_model.model_parallel = True\npeft_model.print_trainable_parameters()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 注意到LoRA算法 B矩阵的初始权重为0，所以训练前peft_model的输出等价于预训练模型model的输出\nfor name,para in peft_model.named_parameters():\n    if '.1.' in name:\n        break \n    if 'lora' in name.lower():\n        print(name+':')\n        print('shape = ',list(para.shape),'\\t','sum = ',para.sum().item())\n        print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_model.train();\nout = peft_model(**batch)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.loss ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 三，训练模型","metadata":{}},{"cell_type":"markdown","source":"下面召唤我们的梦中情炉torchkeras，来实现最优雅的训练循环~\n\n注意这里，为了更加高效地保存和加载参数，我们覆盖了KerasModel中的load_ckpt和save_ckpt方法，\n\n仅仅保存和加载可训练lora权重，这样可以避免加载和保存全部模型权重造成的存储问题。\n","metadata":{}},{"cell_type":"code","source":"from torchkeras import KerasModel \nfrom accelerate import Accelerator \n\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n    \n    def __call__(self, batch):\n        \n        #loss\n        with self.accelerator.autocast():\n            loss = self.net(**batch).loss\n\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\n    \nKerasModel.StepRunner = StepRunner \n\n\n#仅仅保存lora可训练参数\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    import os\n    self.net.load_state_dict(\n        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 此处设置is_paged=True，即使用Paged Optimizer，减少训练过程中Cuda OOM的风险。\noptimizer = bnb.optim.adamw.AdamW(peft_model.parameters(),\n                                  lr=5e-05,is_paged=True)  \n\n\nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer) \n\nckpt_path = 'chatglm2_qlora'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfhistory = keras_model.fit(train_data = dl_train,\n                val_data = dl_val,\n                epochs=30,\n                patience=4,\n                monitor='val_loss',\n                mode='min',\n                ckpt_path = ckpt_path,\n                gradient_accumulation_steps = 2\n               )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!du -s -h chatglm2_qlora ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}