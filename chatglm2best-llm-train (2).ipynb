{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# nvidia p100 kaggle上运行.\n\n!git clone https://github.com/zhangbo2008/firefly\n%cd firefly\n!pip install bitsandbytes\n!pip install peft\n!pip install loguru","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-18T05:56:13.602782Z","iopub.execute_input":"2023-08-18T05:56:13.603492Z","iopub.status.idle":"2023-08-18T05:56:54.507954Z","shell.execute_reply.started":"2023-08-18T05:56:13.603458Z","shell.execute_reply":"2023-08-18T05:56:54.506693Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Cloning into 'firefly'...\nremote: Enumerating objects: 96, done.\u001b[K\nremote: Counting objects: 100% (96/96), done.\u001b[K\nremote: Compressing objects: 100% (65/65), done.\u001b[K\nremote: Total 96 (delta 39), reused 85 (delta 28), pack-reused 0\u001b[K\nReceiving objects: 100% (96/96), 1.38 MiB | 21.41 MiB/s, done.\nResolving deltas: 100% (39/39), done.\n/kaggle/working/firefly\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.41.1\nCollecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nCollecting loguru\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loguru\nSuccessfully installed loguru-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"#-=========参数写到这里得了:\n\n\n\n\n'''\n多卡设置\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node={num_gpus} train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json\\\n\n\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2  6.py\n\n'''\n\n#=======设置单卡, 应该单卡也够用.\nimport os\nos.system('CUDA_VISIBLE_DEVICES=0')\n\nimport bitsandbytes\n\n\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, AdaLoraConfig,prepare_model_for_kbit_training\nfrom transformers import (\n    set_seed,\n    HfArgumentParser,\n    TrainingArguments,\n    AutoModelForCausalLM\n)\nimport argparse\nfrom loguru import logger\nimport os\nfrom os.path import join\nimport torch\nimport bitsandbytes as bnb\nfrom collections import defaultdict\n\nfrom component.collator import SFTDataCollator\nfrom component.dataset import SFTDataset, ChatGLM2SFTDataset\nfrom component.argument import QLoRAArguments\nfrom component.trainer import LoRATrainer\nfrom component.loss import TargetLMLoss\n\n\nif 1:\n\n    #===========================================step1: 配置好参数.\n    ar='tmp.json'\n    aaa=r\"\"\"\n    {\n        \"output_dir\": \"output/firefly-chatglm2-6b\",\n        \"model_name_or_path\": \"THUDM/chatglm2-6b\",\n        \"train_file\": \"./data/dummy_data.jsonl\",\n        \"num_train_epochs\": 1,\n        \"per_device_train_batch_size\": 2,\n        \"gradient_accumulation_steps\": 1,\n        \"learning_rate\": 2e-4,\n        \"max_seq_length\": 1024,\n        \"logging_steps\": 300,\n        \"save_steps\": 500,\n        \"save_total_limit\": 1,\n        \"lr_scheduler_type\": \"constant_with_warmup\",\n        \"warmup_steps\": 3000,\n        \"lora_rank\": 64,\n        \"lora_alpha\": 16,\n        \"lora_dropout\": 0.05,\n\n        \"gradient_checkpointing\": true,\n        \"disable_tqdm\": false,\n        \"optim\": \"paged_adamw_32bit\",\n        \"seed\": 42,\n        \"fp16\": true,\n        \"report_to\": \"tensorboard\",\n        \"dataloader_num_workers\": 0,\n        \"save_strategy\": \"steps\",\n        \"weight_decay\": 0,\n        \"max_grad_norm\": 0.3,\n        \"remove_unused_columns\": false\n    }\n\n\n\n\n\n\n\n    \"\"\"\n    with open('tmp.json','w') as f:\n        f.write(aaa)\n    train_args_file = ar\n    # 读取训练的参数配置\n    parser = HfArgumentParser((QLoRAArguments, TrainingArguments))\n    args, training_args = parser.parse_json_file(json_file=train_args_file)\n    \n    \n    \n    \n    # 创建输出目录\n    if not os.path.exists(training_args.output_dir):\n        os.makedirs(training_args.output_dir)\n    # logger.add(join(training_args.output_dir, 'train.log'))\n    # logger.info(\"train_args:{}\".format(training_args))\n    # 设置随机种子\n    set_seed(training_args.seed)\n    # args, training_args = setup_everything()\n    # 加载各种组件\n\n\n    logger.info('Initializing components...')\n    # 下面的设置至关重要，否则无法多卡训练\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    print('是否ddp',ddp)\n    training_args.ddp_find_unused_parameters = False\n    device_map = \"auto\"\n    # if we are in a distributed setting, we need to set the device map and max memory per device\n    if os.environ.get('LOCAL_RANK') is not None:\n        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n        device_map = {'': local_rank}\n    device_map = {'': 0}\n    # 加载模型\n    model_old = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        device_map=device_map,\n        load_in_4bit=True,           #########???????????????这么加载训练精度很低吧.....\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n        ),\n    )\n    \n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T05:56:54.510744Z","iopub.execute_input":"2023-08-18T05:56:54.511177Z","iopub.status.idle":"2023-08-18T06:00:04.232018Z","shell.execute_reply.started":"2023-08-18T05:56:54.511136Z","shell.execute_reply":"2023-08-18T06:00:04.230975Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[32m2023-08-18 05:57:09.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mInitializing components...\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"是否ddp False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f5d6806b8f243a8ab3999e92f8b2e9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690ab28147294133ae1d794495b6767e"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62faf69acfe49f89a65b16b5563ab40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b961a83f13414abb9731923c3848d3"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c394a3c71147d28b7ea86aa46d9996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c245163259c4f9b8b473578dcd95ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc29627f333f4ac9a112858da0771dd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fc20b3db7a482186cfd9f8ba1c4d03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7febe90f0e4adca2ed07909d772197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a83fd8710374dad8ab452584d9a7b98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b9d7e8b596d455b84bb1facc16aaa98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"636d00dde7d4486e87fade702caf1e35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e53aa4a6107c445981dfbc9561b43ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"469b4692a0cf48fcb97da8abf23433ca"}},"metadata":{}}]},{"cell_type":"code","source":"\nif 1:\n# 加载tokenzier\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=True,\n        # llama不支持fast\n        use_fast=False if model_old.config.model_type == 'llama' else True\n    )\n    # QWenTokenizer比较特殊，pad_token_id、bos_token_id、eos_token_id均为None。eod_id对应的token为<|endoftext|>\n    if tokenizer.__class__.__name__ == 'QWenTokenizer':\n        tokenizer.pad_token_id = tokenizer.eod_id\n        tokenizer.bos_token_id = tokenizer.eod_id\n        tokenizer.eos_token_id = tokenizer.eod_id\n    # ChatGLMTokenizer不需要设置，仅设置其他tokenizer\n    elif tokenizer.__class__.__name__ != 'ChatGLMTokenizer':\n        assert tokenizer.eos_token_id is not None\n        assert tokenizer.bos_token_id is not None\n        tokenizer.pad_token_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n\n    # # 部分tokenizer没有pad_token_id\n    # if tokenizer.pad_token_id is None:\n    #     tokenizer.pad_token_id = tokenizer.unk_token_id\n    # # 部分tokenizer的pad_token_id与eos_token_id相同，如InternLM，会导致无法计算eos_token_id的loss。将pad_token_id设为unk_token_id\n    # if tokenizer.pad_token_id == tokenizer.eos_token_id and tokenizer.unk_token_id is not None:\n    #     tokenizer.pad_token_id = tokenizer.unk_token_id\n    # # 如果两者相同，模型训练时不会计算eos_token_id的loss\n    # if tokenizer.pad_token_id == tokenizer.eos_token_id:\n    #     raise Exception('pad_token_id should not be equal to eos_token_id')\n\n    # casts all the non int8 modules to full precision (fp32) for stability\n    model_old = prepare_model_for_kbit_training(model_old, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n    \n    print(f'memory footprint of model_old: {model_old.get_memory_footprint()/(1024*1024*1024)} GB')\n    # 找到所有需要插入adapter的全连接层\n    # target_modules = find_all_linear_names(model)\n\n    \n    \n    config = LoraConfig(\n    task_type='CAUSAL_LM', inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n    )\n    \n    \n    \n    \n    model = get_peft_model(model_old, config)\n    model.print_trainable_parameters()\n    model.config.torch_dtype = torch.float16\n\n    import torch\n    import torch.nn as nn\n\n    class Loss(object):\n        \"\"\"\n        所有loss的类父类\n        \"\"\"\n        def __call__(self, model, inputs, training_args, return_outputs=False):\n            \"\"\"\n            todo label smoothing\n            用于计算loss。\n            看源码发现，return_outputs=True为train时调用，return_outputs=False为eval和predict调用\n            :param model: 模型\n            :param inputs: 模型输入，dict\n            :param training_args: 训练配置参数\n            :param return_outputs:是否返回模型的输出\n            :return:\n            \"\"\"\n            raise NotImplemented\n    class TargetLMLoss(Loss):\n\n        def __init__(self, ignore_index):\n            super().__init__()\n            self.ignore_index = ignore_index\n            self.loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n        def __call__(self, model, inputs, training_args, return_outputs=False):\n            input_ids = inputs['input_ids']\n            attention_mask = inputs['attention_mask']\n            target_mask = inputs['target_mask']\n            # 模型前馈预测\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n            logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n\n            # 将labels中不属于target的部分，设为ignore_index，只计算target部分的loss\n            labels = torch.where(target_mask == 1, input_ids, self.ignore_index)\n            shift_logits = logits[..., :-1, :].contiguous() # 因为我们生成的logits最后一个\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            return (loss, outputs) if return_outputs else loss\n\n\n\n\n    # 初始化损失函数 ########################!!!!!!!!!!!!!!!!!!!!!\n    loss_func = TargetLMLoss(ignore_index=-100)\n\n    # 指加载训练集\n    if model.config.model_type == 'chatglm':\n        train_dataset = ChatGLM2SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    else:\n        train_dataset = SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    data_collator = SFTDataCollator(tokenizer, args.max_seq_length)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T06:00:04.237342Z","iopub.execute_input":"2023-08-18T06:00:04.238500Z","iopub.status.idle":"2023-08-18T06:00:09.715559Z","shell.execute_reply.started":"2023-08-18T06:00:04.238462Z","shell.execute_reply":"2023-08-18T06:00:09.714506Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c2003e218524945b042f575f7dcda5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a297dcc7cb4e9b98dbadbd51af624d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23bd932ec2d04d5d97fcd5ae1356130f"}},"metadata":{}},{"name":"stdout","text":"memory footprint of model_old: 4.644905149936676 GB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-08-18 06:00:09.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading data: ./data/dummy_data.jsonl\u001b[0m\n\u001b[32m2023-08-18 06:00:09.622\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mthere are 33 data in dataset\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,949,696 || all params: 3,390,261,248 || trainable%: 0.05750872447219737\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nif 1:  #============开启训练.step2==========进行参数的重新确认,启动训练. # %%time可以监控这个代码快运行时间. 给出训练市场的判定!\n    #==========================================\n    ar='tmp.json'\n    aaa=r\"\"\"\n    {\n        \"output_dir\": \"output/firefly-chatglm2-6b\",\n        \"model_name_or_path\": \"THUDM/chatglm2-6b\",\n        \"train_file\": \"./data/dummy_data.jsonl\",\n        \"num_train_epochs\": 10,\n        \"per_device_train_batch_size\": 2,\n        \"gradient_accumulation_steps\": 1,\n        \"learning_rate\": 2e-4,\n        \"max_seq_length\": 1024,\n        \"logging_steps\": 300,\n        \"save_steps\": 500,\n        \"save_total_limit\": 1,\n        \"lr_scheduler_type\": \"constant_with_warmup\",\n        \"warmup_steps\": 3000,\n        \"lora_rank\": 64,\n        \"lora_alpha\": 16,\n        \"lora_dropout\": 0.05,\n\n        \"gradient_checkpointing\": true,\n        \"disable_tqdm\": false,\n        \"optim\": \"paged_adamw_32bit\",\n        \"seed\": 42,\n        \"fp16\": true,\n        \"report_to\": \"tensorboard\",\n        \"dataloader_num_workers\": 0,\n        \"save_strategy\": \"steps\",\n        \"weight_decay\": 0,\n        \"max_grad_norm\": 0.3,\n        \"remove_unused_columns\": false\n    }\n\n\n\n\n\n\n\n    \"\"\"\n    with open('tmp.json','w') as f:\n        f.write(aaa)\n    train_args_file = ar\n    # 读取训练的参数配置\n    parser = HfArgumentParser((QLoRAArguments, TrainingArguments))\n    args, training_args = parser.parse_json_file(json_file=train_args_file)\n    logger.info(\"*** starting training ***\")\n    print('训练数据的条数',len(train_dataset))\n        # 初始化Trainer\n    trainer = LoRATrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_loss=loss_func\n    )\n    train_result = trainer.train()\n    # 保存最好的checkpoint\n    final_save_path = join(training_args.output_dir, 'final')\n    print('保存模型')\n    trainer.save_model(final_save_path)  # Saves the tokenizer too\n    # 保存训练指标\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()","metadata":{"execution":{"iopub.status.busy":"2023-08-18T06:15:15.935586Z","iopub.execute_input":"2023-08-18T06:15:15.936014Z","iopub.status.idle":"2023-08-18T06:46:59.065662Z","shell.execute_reply.started":"2023-08-18T06:15:15.935981Z","shell.execute_reply":"2023-08-18T06:46:59.064536Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[32m2023-08-18 06:15:15.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1m*** starting training ***\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"训练数据的条数 33\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [170/170 31:28, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"保存模型\n***** train metrics *****\n  epoch                    =       10.0\n  total_flos               =  5898826GF\n  train_loss               =     4.5226\n  train_runtime            = 0:31:42.71\n  train_samples_per_second =      0.173\n  train_steps_per_second   =      0.089\nCPU times: user 19min 45s, sys: 11min 43s, total: 31min 29s\nWall time: 31min 43s\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nif 1: # 合并大模型. 用于推理.    \n    model_name_or_path = args.model_name_or_path\n    adapter_name_or_path = final_save_path\n    save_path = 'checkpoint/firefly-baichuan-7b-qlora-sft-merge'\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name_or_path,\n        trust_remote_code=True\n    )\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_name_or_path,\n#         trust_remote_code=True,\n#         low_cpu_mem_usage=True,\n#         torch_dtype=torch.float16,\n#         device_map='auto'\n#     )\n    model_new = PeftModel.from_pretrained(model_old, adapter_name_or_path)\n#     model = model.merge_and_unload() ##########==========好像不用合并.  还是一定好合并?????????\n#     print(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T06:49:04.662641Z","iopub.execute_input":"2023-08-18T06:49:04.663090Z","iopub.status.idle":"2023-08-18T06:49:04.957770Z","shell.execute_reply.started":"2023-08-18T06:49:04.663050Z","shell.execute_reply":"2023-08-18T06:49:04.956531Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(model_new)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T06:49:13.069113Z","iopub.execute_input":"2023-08-18T06:49:13.069491Z","iopub.status.idle":"2023-08-18T06:49:13.079217Z","shell.execute_reply.started":"2023-08-18T06:49:13.069458Z","shell.execute_reply":"2023-08-18T06:49:13.078221Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): ChatGLMForConditionalGeneration(\n      (transformer): ChatGLMModel(\n        (embedding): Embedding(\n          (word_embeddings): Embedding(65024, 4096)\n        )\n        (rotary_pos_emb): RotaryEmbedding()\n        (encoder): GLMTransformer(\n          (layers): ModuleList(\n            (0-27): 28 x GLMBlock(\n              (input_layernorm): RMSNorm()\n              (self_attention): SelfAttention(\n                (query_key_value): Linear4bit(\n                  in_features=4096, out_features=4608, bias=True\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.1, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=8, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=8, out_features=4608, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (core_attention): CoreAttention(\n                  (attention_dropout): Dropout(p=0.0, inplace=False)\n                )\n                (dense): Linear4bit(in_features=4096, out_features=4096, bias=False)\n              )\n              (post_attention_layernorm): RMSNorm()\n              (mlp): MLP(\n                (dense_h_to_4h): Linear4bit(in_features=4096, out_features=27392, bias=False)\n                (dense_4h_to_h): Linear4bit(in_features=13696, out_features=4096, bias=False)\n              )\n            )\n          )\n          (final_layernorm): RMSNorm()\n        )\n        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print('测试一下是否加载成功')\nresponse,history= model_new.chat(tokenizer,query='如何应对突发紧急情况？',history=[],temperature=0.0001)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:01:11.431018Z","iopub.execute_input":"2023-08-18T07:01:11.431414Z","iopub.status.idle":"2023-08-18T07:01:29.581741Z","shell.execute_reply.started":"2023-08-18T07:01:11.431380Z","shell.execute_reply":"2023-08-18T07:01:29.580701Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"测试一下是否加载成功\n突发紧急情况很难预测,但是下面是一些应对突发紧急情况的建议:\n\n1. 保持冷静:在紧急情况下,保持冷静非常重要。不要惊慌,试图控制自己的情绪,清晰地思考下一步该怎么做。\n\n2. 寻求帮助:如果遇到紧急情况,尽快寻求帮助。可以拨打紧急电话号码,如110、120等,或者与最近的医生、护士、警察、消防队员或其他专业人士联系。\n\n3. 提供必要的信息:拨打紧急电话号码时,一定要提供必要的信息,例如所在的位置、事件的性质、人员伤亡情况等。这些信息可以帮助急救人员快速做出反应。\n\n4. 遵守指示:当接到紧急电话后,一定要遵守指示,并服从任何指示。这包括遵循医生的建议、遵循消防队员的指示等。\n\n5. 准备基本急救知识:在紧急情况下,具备基本的急救知识可以帮助自己或他人更快地得到帮助。例如,如何进行心肺复苏、如何给伤口止血等。\n\n6. 寻找安全的地方:如果必须亲自前往现场,一定要找到一个安全的地方。远离车辆、人群和其他潜在的危险,找到一个安静、安全的地方。\n\n7. 准备好必要的物品:在紧急情况下,准备好必要的物品可以帮助自己更快地做出反应。例如,带上手机、钱包、身份证明、医疗记录等。\n\n应对突发紧急情况需要冷静、快速的反应和有效的组织能力。尽可能地做好准备,以便在紧急情况下更好地应对。\n","output_type":"stream"}]},{"cell_type":"code","source":"print('测试一下老模型是否加载成功')\nresponse,history= model_old.chat(tokenizer,query='如何应对突发紧急情况？',history=[],temperature=0.0001)\nprint(response) \nprint('可以看到新旧模型的区别!!!!!!!!!!!!!!!!!!!!!!!')","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:00:30.731343Z","iopub.execute_input":"2023-08-18T07:00:30.731819Z","iopub.status.idle":"2023-08-18T07:00:48.092744Z","shell.execute_reply.started":"2023-08-18T07:00:30.731778Z","shell.execute_reply":"2023-08-18T07:00:48.090863Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"测试一下老模型是否加载成功\n突发紧急情况很难预测,但是下面是一些应对突发紧急情况的建议:\n\n1. 保持冷静:在紧急情况下,保持冷静非常重要。不要惊慌,试图控制自己的情绪,清晰地思考下一步该怎么做。\n\n2. 寻求帮助:如果遇到紧急情况,尽快寻求帮助。可以拨打紧急电话号码,如110、120等,或者与最近的医生、护士、警察、消防队员或其他专业人士联系。\n\n3. 提供必要的信息:拨打紧急电话号码时,一定要提供必要的信息,例如所在的位置、事件的性质、人员伤亡情况等。这些信息可以帮助急救人员快速做出反应。\n\n4. 遵守指示:当接到紧急电话后,一定要遵守指示,并服从任何指示。这包括遵循医生的建议、遵循消防队员的指示等。\n\n5. 准备基本急救知识:在紧急情况下,具备基本的急救知识可以帮助自己或他人更快地得到帮助。例如,如何进行心肺复苏、如何给伤口止血等。\n\n6. 寻找安全的地方:如果必须亲自前往现场,一定要找到一个安全的地方。远离车辆、人群和其他潜在的危险,找到一个安静、安全的地方。\n\n7. 准备好必要的物品:在紧急情况下,准备好必要的物品可以帮助自己更快地做出反应。例如,带上手机、钱包、身份证明、医疗记录等。\n\n应对突发紧急情况需要冷静、快速的反应和有效的组织能力。尽可能地做好准备,以便在紧急情况下更好地应对。\n可以看到新旧模型的区别!!!!!!!!!!!!!!!!!!!!!!!\n","output_type":"stream"}]},{"cell_type":"code","source":"if 1: #上传到我的hf保存,   自己替换自己的token和下面的repo_id的仓库名为自己的hf账号.\n    !huggingface-cli login --token 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 --add-to-git-credential\n    from huggingface_hub import HfApi\n    api = HfApi()\n    #创建huggingface 模型库\n    import time # 每一次保存都用时间来标注.\n    repo_id='zhangbo2008/best_llm_train'+'_'.join(time.asctime().split(' '))[-20:].replace(\"_\",'M').replace(\":\",'M')\n    print(repo_id)\n\n    api.create_repo(repo_id=repo_id)\n    #上传模型可能需要等待10分钟左右~\n    api.upload_folder(\n        folder_path=adapter_name_or_path,\n        repo_id=repo_id,\n        repo_type=\"model\", #space, model, datasets\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-18T06:56:14.929049Z","iopub.execute_input":"2023-08-18T06:56:14.930088Z","iopub.status.idle":"2023-08-18T06:56:19.007244Z","shell.execute_reply.started":"2023-08-18T06:56:14.930046Z","shell.execute_reply":"2023-08-18T06:56:19.006117Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nzhangbo2008/best_llm_trainAugM18M06M56M16M2023\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/7.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7f69abc9944ec4aa85e55116538cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a3ef950f9f243c1b866860cfd4d93e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115bf886fe574954bf6ca21205a80418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad0de21dd77d4b86b96e48c351952542"}},"metadata":{}}]}]}