{"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["BPLwsPajb1f8","L_WSo0VUV99t","mjY-eq3eWh9O","JoTC9o2SczNn","gfGJNZBUP7Vn","YB0Cxrw1StrP","47iuAFqV8Ws-","x62pP0PHdA-y"],"include_colab_link":true},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let's code Reinforce algorithm from scratch üî•\n\n\nTo validate this hands-on for the certification process, you need to push your trained models to the Hub.\n\n- Get a result of >= 350 for `Cartpole-v1`.\n- Get a result of >= 5 for `PixelCopter`.\n\nTo find your result, go to the leaderboard and find your model, **the result = mean_reward - std of reward**. **If you don't see your model on the leaderboard, go at the bottom of the leaderboard page and click on the refresh button**.\n\nFor more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\n","metadata":{"id":"Bsh4ZAamchSl"}},{"cell_type":"markdown","source":"## Create a virtual display üñ•\n\nDuring the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames). \n\nHence the following cell will install the librairies and create and run a virtual screen üñ•","metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","source":"%%capture\n!apt install python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip install pyvirtualdisplay\n!pip install pyglet==1.5.1","metadata":{"id":"jV6wjQ7Be7p5","execution":{"iopub.status.busy":"2023-07-06T07:58:09.219887Z","iopub.execute_input":"2023-07-06T07:58:09.220365Z","iopub.status.idle":"2023-07-06T07:58:42.328843Z","shell.execute_reply.started":"2023-07-06T07:58:09.220319Z","shell.execute_reply":"2023-07-06T07:58:42.327528Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"id":"Sr-Nuyb1dBm0","execution":{"iopub.status.busy":"2023-07-06T07:58:53.510832Z","iopub.execute_input":"2023-07-06T07:58:53.511461Z","iopub.status.idle":"2023-07-06T07:58:54.159306Z","shell.execute_reply.started":"2023-07-06T07:58:53.511423Z","shell.execute_reply":"2023-07-06T07:58:54.158396Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<pyvirtualdisplay.display.Display at 0x7d731ed2f9a0>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt","metadata":{"id":"e8ZVi-uydpgL","execution":{"iopub.status.busy":"2023-07-06T07:59:10.820768Z","iopub.execute_input":"2023-07-06T07:59:10.821130Z","iopub.status.idle":"2023-07-06T07:59:44.404546Z","shell.execute_reply.started":"2023-07-06T07:59:10.821102Z","shell.execute_reply":"2023-07-06T07:59:44.403187Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1))\n  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-udns9ns2\n  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-udns9ns2\n  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting git+https://github.com/simoninithomas/gym-games (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n  Cloning https://github.com/simoninithomas/gym-games to /tmp/pip-req-build-e5a9a052\n  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /tmp/pip-req-build-e5a9a052\n  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (0.15.1)\nCollecting imageio-ffmpeg (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 4))\n  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyyaml==6.0 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 5))\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (1.23.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from ple==0.0.1->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 1)) (9.5.0)\nRequirement already satisfied: gym>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.26.2)\nCollecting setuptools>=65.5.1 (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n  Downloading setuptools-68.0.0-py3-none-any.whl (804 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pygame>=1.9.6 (from gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2))\n  Downloading pygame-2.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.9 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.28.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.64.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (21.3)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym>=0.13.0->gym-games==1.0.4->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 2)) (0.0.8)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt (line 3)) (2023.5.7)\nBuilding wheels for collected packages: ple, gym-games\n  Building wheel for ple (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50788 sha256=3cd9c4cf0ff03853dc771b1ffeae239adcd06dacac9ccf36bf9d7a4f265801f1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mqdxaon0/wheels/f8/31/ca/a64a7ce73540465412d82813780d062db53b90e3f42a4ecb7f\n  Building wheel for gym-games (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=17321 sha256=622f092a59b2f8b99e6086afa3c43b57f1536c403b697556ec08d2d56ff072dd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mqdxaon0/wheels/ca/bf/6b/7d631626202ebb033c908a688d1862ff4d948c34cf621d7dc9\nSuccessfully built ple gym-games\nInstalling collected packages: setuptools, pyyaml, pygame, ple, imageio-ffmpeg, gym-games\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 59.8.0\n    Uninstalling setuptools-59.8.0:\n      Successfully uninstalled setuptools-59.8.0\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 5.4.1\n    Uninstalling PyYAML-5.4.1:\n      Successfully uninstalled PyYAML-5.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.6.0 which is incompatible.\ndask-cudf 23.6.0 requires dask==2023.3.2, but you have dask 2023.6.0 which is incompatible.\ndistributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.6.0 which is incompatible.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 1.8.21 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.88.0 which is incompatible.\nkfp 1.8.21 requires PyYAML<6,>=5.3, but you have pyyaml 6.0 which is incompatible.\nopentelemetry-api 1.17.0 requires importlib-metadata~=6.0.0, but you have importlib-metadata 5.2.0 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.10.1 which is incompatible.\nraft-dask 23.6.1 requires dask==2023.3.2, but you have dask 2023.6.0 which is incompatible.\nydata-profiling 4.1.2 requires scipy<1.10,>=1.4.1, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gym-games-1.0.4 imageio-ffmpeg-0.4.8 ple-0.0.1 pygame-2.5.0 pyyaml-6.0 setuptools-68.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import the packages üì¶\nIn addition to import the installed libraries, we also import:\n\n- `imageio`: A library that will help us to generate a replay video\n\n","metadata":{"id":"AAHAq6RZW3rn"}},{"cell_type":"code","source":"import numpy as np\n\nfrom collections import deque\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Gym\nimport gym\nimport gym_pygame\n\n# Hugging Face Hub\nfrom huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\nimport imageio","metadata":{"id":"V8oadoJSWp7C","execution":{"iopub.status.busy":"2023-07-06T07:59:55.489959Z","iopub.execute_input":"2023-07-06T07:59:55.490459Z","iopub.status.idle":"2023-07-06T07:59:59.266996Z","shell.execute_reply.started":"2023-07-06T07:59:55.490405Z","shell.execute_reply":"2023-07-06T07:59:59.266042Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"kaJu5FeZxXGY","execution":{"iopub.status.busy":"2023-07-06T08:00:06.088192Z","iopub.execute_input":"2023-07-06T08:00:06.089043Z","iopub.status.idle":"2023-07-06T08:00:06.159309Z","shell.execute_reply.started":"2023-07-06T08:00:06.089003Z","shell.execute_reply":"2023-07-06T08:00:06.158590Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"id":"U5TNYa14aRav","execution":{"iopub.status.busy":"2023-07-06T08:00:08.499407Z","iopub.execute_input":"2023-07-06T08:00:08.499865Z","iopub.status.idle":"2023-07-06T08:00:08.508964Z","shell.execute_reply.started":"2023-07-06T08:00:08.499828Z","shell.execute_reply":"2023-07-06T08:00:08.507843Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We're now ready to implement our Reinforce algorithm üî•","metadata":{"id":"PBPecCtBL_pZ"}},{"cell_type":"markdown","source":"# First agent: Playing CartPole-v1 ü§ñ","metadata":{"id":"8KEyKYo2ZSC-"}},{"cell_type":"markdown","source":"## Create the CartPole environment and understand how it works\n### [The environment üéÆ](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n","metadata":{"id":"haLArKURMyuF"}},{"cell_type":"markdown","source":"### Why do we use a simple environment like CartPole-v1?\nAs explained in [Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html), when you implement your agent from scratch you need **to be sure that it works correctly and find bugs with easy environments before going deeper**. Since finding bugs will be much easier in simple environments.\n\n\n> Try to have some ‚Äúsign of life‚Äù on toy problems\n\n\n> Validate the implementation by making it run on harder and harder envs (you can compare results against the RL zoo). You usually need to run hyperparameter optimization for that step.\n___\n### The CartPole-v1 environment\n\n> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n\n\n\nSo, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n\nThe episode ends if:\n- The pole Angle is greater than ¬±12¬∞\n- Cart Position is greater than ¬±2.4\n- Episode length is greater than 500\n\nWe get a reward üí∞ of +1 every timestep the Pole stays in the equilibrium.","metadata":{"id":"AH_TaLKFXo_8"}},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\n# Create the env\nenv = gym.make(env_id)\n\n# Create the evaluation env\neval_env = gym.make(env_id)\n\n# Get the state space and action space\ns_size = env.observation_space.shape[0]  # Ë°®Á§∫ÁöÑÊòØËæìÂÖ•Áä∂ÊÄÅÁöÑÊï∞Èáè.\na_size = env.action_space.n              # Ë°®Á§∫ÁöÑÊòØÂä®‰ΩúÁä∂ÊÄÅÁöÑÊï∞Èáè","metadata":{"id":"POOOk15_K6KA","execution":{"iopub.status.busy":"2023-07-06T08:00:22.328266Z","iopub.execute_input":"2023-07-06T08:00:22.329274Z","iopub.status.idle":"2023-07-06T08:00:22.344761Z","shell.execute_reply.started":"2023-07-06T08:00:22.329229Z","shell.execute_reply":"2023-07-06T08:00:22.343770Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint('action_size ', a_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"id":"FMLFrjiBNLYJ","execution":{"iopub.status.busy":"2023-07-06T08:01:38.125008Z","iopub.execute_input":"2023-07-06T08:01:38.125422Z","iopub.status.idle":"2023-07-06T08:01:38.132029Z","shell.execute_reply.started":"2023-07-06T08:01:38.125388Z","shell.execute_reply":"2023-07-06T08:01:38.130915Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"_____OBSERVATION SPACE_____ \n\nThe State Space is:  4\naction_size  2\nSample observation [ 8.6825538e-01  1.2542409e+38  2.8198969e-01 -1.1636652e+38]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"id":"Lu6t4sRNNWkN","execution":{"iopub.status.busy":"2023-07-06T08:02:11.521641Z","iopub.execute_input":"2023-07-06T08:02:11.522093Z","iopub.status.idle":"2023-07-06T08:02:11.528451Z","shell.execute_reply.started":"2023-07-06T08:02:11.522054Z","shell.execute_reply":"2023-07-06T08:02:11.527414Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\n _____ACTION SPACE_____ \n\nThe Action Space is:  2\nAction Space Sample 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Let's build the Reinforce Architecture\nThis implementation is based on two implementations:\n- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n- [Improvement of the integration by Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>","metadata":{"id":"7SJMJj3WaFOz"}},{"cell_type":"markdown","source":"So we want:\n- Two fully connected layers (fc1 and fc2).\n- Using ReLU as activation function of fc1\n- Using Softmax to output a probability distribution over actions","metadata":{"id":"49kogtxBODX8"}},{"cell_type":"markdown","source":"### Solution","metadata":{"id":"rOMrdwSYOWSC"}},{"cell_type":"markdown","source":"I make a mistake, can you guess where?\n\n- To find out let's make a forward pass:","metadata":{"id":"ZTGWL4g2eM5B"}},{"cell_type":"markdown","source":"- Here we see that the error says `ValueError: The value argument to log_prob must be a Tensor`\n\n- It means that `action` in `m.log_prob(action)` must be a Tensor **but it's not.**\n\n- Do you know why? Check the act function and try to see why it does not work. \n\nAdvice üí°: Something is wrong in this implementation. Remember that we act function **we want to sample an action from the probability distribution over actions**.\n","metadata":{"id":"14UYkoxCPaor"}},{"cell_type":"markdown","source":"### (Real) Solution","metadata":{"id":"gfGJNZBUP7Vn"}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        print(state,1111111111)\n        state=state[0]\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)","metadata":{"id":"Ho_UHf49N9i4","execution":{"iopub.status.busy":"2023-07-06T08:49:04.218640Z","iopub.execute_input":"2023-07-06T08:49:04.219012Z","iopub.status.idle":"2023-07-06T08:49:04.228011Z","shell.execute_reply.started":"2023-07-06T08:49:04.218982Z","shell.execute_reply":"2023-07-06T08:49:04.226896Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"By using CartPole, it was easier to debug since **we know that the bug comes from our integration and not from our simple environment**.","metadata":{"id":"rgJWQFU_eUYw"}},{"cell_type":"markdown","source":"- Since **we want to sample an action from the probability distribution over actions**, we can't use `action = np.argmax(m)` since it will always output the action that have the highest probability.\n\n- We need to replace with `action = m.sample()` that will sample an action from the probability distribution P(.|s)","metadata":{"id":"c-20i7Pk0l1T"}},{"cell_type":"markdown","source":"### Let's build the Reinforce Training Algorithm\nThis is the Reinforce algorithm pseudocode:\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n  ","metadata":{"id":"4MXoqetzfIoW"}},{"cell_type":"markdown","source":"- When we calculate the return Gt (line 6) we see that we calculate the sum of discounted rewards **starting at timestep t**.\n\n- Why? Because our policy should only **reinforce actions on the basis of the consequences**: so rewards obtained before taking an action are useless (since they were not because of the action), **only the ones that come after the action matters**.\n\n- Before coding this you should read this section [don't let the past distract you](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) that explains why we use reward-to-go policy gradient.\n\nWe use an interesting technique coded by [Chris1nexus](https://github.com/Chris1nexus) to **compute the return at each timestep efficiently**. The comments explained the procedure. Don't hesitate also [to check the PR explanation](https://github.com/huggingface/deep-rl-class/pull/95)\nBut overall the idea is to **compute the return at each timestep efficiently**.","metadata":{"id":"QmcXG-9i2Qu2"}},{"cell_type":"markdown","source":"The second question you may ask is **why do we minimize the loss**? You talked about Gradient Ascent not Gradient Descent?\n\n- We want to maximize our utility function $J(\\theta)$ but in PyTorch like in Tensorflow it's better to **minimize an objective function.**\n    - So let's say we want to reinforce action 3 at a certain timestep. Before training this action P is 0.25.\n    - So we want to modify $\\theta$ such that $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n    - Because all P must sum to 1, max $\\pi_\\theta(a_3|s; \\theta)$ will **minimize other action probability.**\n    - So we should tell PyTorch **to min $1 - \\pi_\\theta(a_3|s; \\theta)$.**\n    - This loss function approaches 0 as $\\pi_\\theta(a_3|s; \\theta)$ nears 1.\n    - So we are encouraging the gradient to max $\\pi_\\theta(a_3|s; \\theta)$\n","metadata":{"id":"O554nUGPpcoq"}},{"cell_type":"markdown","source":"#### Solution","metadata":{"id":"YB0Cxrw1StrP"}},{"cell_type":"code","source":"def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n    # Help us to calculate the score during the training\n    scores_deque = deque(maxlen=100)\n    scores = []\n    # Line 3 of pseudocode\n    for i_episode in range(1, n_training_episodes+1):\n        saved_log_probs = []    # ‰øùÂ≠òÊØè‰∏ÄÊ≠•ÁöÑlogÊ¶ÇÁéá.\n        rewards = []            # ‰øùÂ≠òÊØè‰∏ÄÊ≠•ÁöÑÂ•ñÂä±.\n        state = env.reset()  #=======Ëé∑ÂæóÂàùÂßãÁéØÂ¢É.\n        state=state[0]\n       \n        # Line 4 of pseudocode  ====ÂØπÂ∫î‰∏äÂõæ‰º™‰ª£Á†ÅÁöÑÁ¨¨ÂõõË°å.\n        for t in range(max_t): # max_t Ë°®Á§∫ÊØè‰∏ÄÊ¨°Ê∏∏Áé©,ÊúÄÂ§ßÂ§öÂ∞ëÊ≠•.\n            action, log_prob = policy.act(state) #Êàë‰ª¨‰ΩøÁî®Êàë‰ª¨ÁöÑÂΩìÂâçpolicyÊù•ÁîüÊàê‰∏Ä‰∏™trajactory.\n            saved_log_probs.append(log_prob)\n           \n       \n            state, reward, done, _ ,_= env.step(action)\n            rewards.append(reward)\n            if done:\n                break \n        scores_deque.append(sum(rewards))\n        scores.append(sum(rewards))\n        \n        \n        \n        # Line 6 of pseudocode: calculate the return\n        returns = deque(maxlen=max_t) \n        n_steps = len(rewards)        \n        for t in range(n_steps)[::-1]:\n            disc_return_t = (returns[0] if len(returns)>0 else 0)\n            returns.appendleft( gamma*disc_return_t + rewards[t]   )     #È´òÊïàÁöÑËÆ°ÁÆó.G_tËÆæÁΩÆ‰∏∫returns.\n            \n            \n            \n        ## standardization of the returns is employed to make training more stable\n        eps = np.finfo(np.float32).eps.item() # floatËÉΩË°®Á§∫ÁöÑÊúÄÂ∞èÂÄº.\n        ## eps is the smallest representable float, which is \n        # added to the standard deviation of the returns to avoid numerical instabilities        \n        returns = torch.tensor(returns)\n        returns = (returns - returns.mean()) / (returns.std() + eps) # ÂΩí‰∏ÄÂåñ.\n        \n        # Line 7:\n        policy_loss = []\n        for log_prob, disc_return in zip(saved_log_probs, returns):\n            policy_loss.append(-log_prob * disc_return)\n        policy_loss = torch.cat(policy_loss).sum()\n        \n        # Line 8: PyTorch prefers gradient descent \n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n        \n        if i_episode % print_every == 0:\n            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n        \n    return scores","metadata":{"id":"NCNvyElRStWG","execution":{"iopub.status.busy":"2023-07-06T08:53:41.029709Z","iopub.execute_input":"2023-07-06T08:53:41.030248Z","iopub.status.idle":"2023-07-06T08:53:41.042808Z","shell.execute_reply.started":"2023-07-06T08:53:41.030210Z","shell.execute_reply":"2023-07-06T08:53:41.041746Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"##  Train it\n- We're now ready to train our agent.\n- But first, we define a variable containing all the training hyperparameters.\n- You can change the training parameters (and should üòâ)","metadata":{"id":"RIWhQyJjfpEt"}},{"cell_type":"code","source":"cartpole_hyperparameters = {\n    \"h_size\": 16,\n    \"n_training_episodes\": 1000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 1000,\n    \"gamma\": 1.0,\n    \"lr\": 1e-2,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"id":"utRe1NgtVBYF","execution":{"iopub.status.busy":"2023-07-06T08:52:42.203315Z","iopub.execute_input":"2023-07-06T08:52:42.203777Z","iopub.status.idle":"2023-07-06T08:52:42.209543Z","shell.execute_reply.started":"2023-07-06T08:52:42.203744Z","shell.execute_reply":"2023-07-06T08:52:42.208241Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Create policy and place it to the device\ncartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\ncartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])","metadata":{"id":"D3lWyVXBVfl6","execution":{"iopub.status.busy":"2023-07-06T08:46:28.059834Z","iopub.execute_input":"2023-07-06T08:46:28.060210Z","iopub.status.idle":"2023-07-06T08:46:28.068455Z","shell.execute_reply.started":"2023-07-06T08:46:28.060179Z","shell.execute_reply":"2023-07-06T08:46:28.066426Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"scores = reinforce(cartpole_policy,\n                   cartpole_optimizer,\n                   cartpole_hyperparameters[\"n_training_episodes\"], \n                   cartpole_hyperparameters[\"max_t\"],\n                   cartpole_hyperparameters[\"gamma\"], \n                   100)","metadata":{"id":"uGf-hQCnfouB","execution":{"iopub.status.busy":"2023-07-06T08:53:46.217486Z","iopub.execute_input":"2023-07-06T08:53:46.217885Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Episode 100\tAverage Score: 677.11\nEpisode 200\tAverage Score: 975.79\nEpisode 300\tAverage Score: 887.99\nEpisode 400\tAverage Score: 910.04\n","output_type":"stream"}]},{"cell_type":"code","source":"print('ËÆ≠ÂÆå1000ËÆ∫')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define evaluation method üìù\n- Here we define the evaluation method that we're going to use to test our Reinforce agent.","metadata":{"id":"Qajj2kXqhB3g"}},{"cell_type":"code","source":"def evaluate_agent(env, max_steps, n_eval_episodes, policy): #=ÂÜôËØÑÊµãÂáΩÊï∞.?????????‰∏∫Âï•‰∏çÊ®°ÂûãevalÂë¢.\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The Reinforce agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n    \n    for step in range(max_steps):\n      action, _ = policy.act(state)\n      new_state, reward, done, info = env.step(action)\n      total_rewards_ep += reward\n        \n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n  print('jieguo',mean_reward, std_reward)\n  return mean_reward, std_reward","metadata":{"id":"3FamHmxyhBEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate our agent üìà","metadata":{"id":"xdH2QCrLTrlT"}},{"cell_type":"code","source":"evaluate_agent(eval_env, \n               cartpole_hyperparameters[\"max_t\"], \n               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n               cartpole_policy)","metadata":{"id":"ohGSXDyHh0xx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Publish our trained model on the Hub üî•\nNow that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n\nHere's an example of a Model Card:\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>","metadata":{"id":"7CoeLkQ7TpO8"}},{"cell_type":"markdown","source":"### Push to the Hub\n#### Do not modify this code","metadata":{"id":"Jmhs1k-cftIq"}},{"cell_type":"code","source":"from huggingface_hub import HfApi, snapshot_download\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport json\nimport imageio\n\nimport tempfile\n\nimport os","metadata":{"id":"LIVsvlW_8tcw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def record_video(env, policy, out_directory, fps=30):\n  \"\"\"\n  Generate a replay video of the agent\n  :param env\n  :param Qtable: Qtable of our agent\n  :param out_directory\n  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n  \"\"\"\n  images = []  \n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _ = policy.act(state)\n    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"id":"Lo4JH45if81z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def push_to_hub(repo_id, \n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the Hub\n\n  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n  :param model: the pytorch model we want to save\n  :param hyperparameters: training hyperparameters\n  :param eval_env: evaluation environment\n  :param video_fps: how many frame per seconds to record our video replay \n  \"\"\"\n\n  _, repo_name = repo_id.split(\"/\")\n  api = HfApi()\n  \n  # Step 1: Create the repo\n  repo_url = api.create_repo(\n        repo_id=repo_id,\n        exist_ok=True,\n  )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    local_directory = Path(tmpdirname)\n  \n    # Step 2: Save the model\n    torch.save(model, local_directory / \"model.pt\")\n\n    # Step 3: Save the hyperparameters to JSON\n    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n      json.dump(hyperparameters, outfile)\n    \n    # Step 4: Evaluate the model and build JSON\n    mean_reward, std_reward = evaluate_agent(eval_env, \n                                            hyperparameters[\"max_t\"],\n                                            hyperparameters[\"n_evaluation_episodes\"], \n                                            model)\n    # Get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n          \"env_id\": hyperparameters[\"env_id\"], \n          \"mean_reward\": mean_reward,\n          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n          \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(local_directory / \"results.json\", \"w\") as outfile:\n        json.dump(evaluate_data, outfile)\n\n    # Step 5: Create the model card\n    env_name = hyperparameters[\"env_id\"]\n    \n    metadata = {}\n    metadata[\"tags\"] = [\n          env_name,\n          \"reinforce\",\n          \"reinforcement-learning\",\n          \"custom-implementation\",\n          \"deep-rl-class\"\n      ]\n\n    # Add metrics\n    eval = metadata_eval_result(\n        model_pretty_name=repo_name,\n        task_pretty_name=\"reinforcement-learning\",\n        task_id=\"reinforcement-learning\",\n        metrics_pretty_name=\"mean_reward\",\n        metrics_id=\"mean_reward\",\n        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n        dataset_pretty_name=env_name,\n        dataset_id=env_name,\n      )\n\n    # Merges both dictionaries\n    metadata = {**metadata, **eval}\n\n    model_card = f\"\"\"\n  # **Reinforce** Agent playing **{env_id}**\n  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n  \"\"\"\n\n    readme_path = local_directory / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n          readme = f.read()\n    else:\n      readme = model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n      f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n    # Step 6: Record a video\n    video_path =  local_directory / \"replay.mp4\"\n    record_video(env, model, video_path, video_fps)\n\n    # Step 7. Push everything to the Hub\n    api.upload_folder(\n          repo_id=repo_id,\n          folder_path=local_directory,\n          path_in_repo=\".\",\n    )\n\n    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")","metadata":{"id":"_TPdq47D7_f_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### .\n\nBy using `push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the Hub**.\n\nThis way:\n- You can **showcase our work** üî•\n- You can **visualize your agent playing** üëÄ\n- You can **share with the community an agent that others can use** üíæ\n- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n","metadata":{"id":"w17w8CxzoURM"}},{"cell_type":"markdown","source":"To be able to share your model with the community there are three more steps to follow:\n\n1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n\n2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n","metadata":{"id":"cWnFC0iZooTw"}},{"cell_type":"code","source":"notebook_login()","metadata":{"id":"QB5nIcxR8paT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login` (or `login`)","metadata":{"id":"GyWc1x3-o3xG"}},{"cell_type":"markdown","source":"3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()` function","metadata":{"id":"F-D-zhbRoeOm"}},{"cell_type":"code","source":"repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(repo_id,\n                cartpole_policy, # The model we want to save\n                cartpole_hyperparameters, # Hyperparameters\n                eval_env, # Evaluation environment\n                video_fps=30\n                )","metadata":{"id":"UNwkTS65Uq3Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we try the robustness of our implementation, let's try a more complex environment: PixelCopter üöÅ\n\n\n","metadata":{"id":"jrnuKH1gYZSz"}},{"cell_type":"markdown","source":"## Second agent: PixelCopter üöÅ\n\n### Study the PixelCopter environment üëÄ\n- [The Environment documentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n","metadata":{"id":"JNLVmKKVKA6j"}},{"cell_type":"code","source":"env_id = \"Pixelcopter-PLE-v0\"\nenv = gym.make(env_id)\neval_env = gym.make(env_id)\ns_size = env.observation_space.shape[0]\na_size = env.action_space.n","metadata":{"id":"JBSc8mlfyin3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"id":"L5u_zAHsKBy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"id":"D7yJM9YXKNbq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The observation space (7) üëÄ:\n- player y position\n- player velocity\n- player distance to floor\n- player distance to ceiling\n- next block x distance to player\n- next blocks top y location\n- next blocks bottom y location\n\nThe action space(2) üéÆ:\n- Up (press accelerator) \n- Do nothing (don't press accelerator) \n\nThe reward function üí∞: \n- For each vertical block it passes through it gains a positive reward of +1. Each time a terminal state reached it receives a negative reward of -1.","metadata":{"id":"NNWvlyvzalXr"}},{"cell_type":"markdown","source":"### Define the new Policy üß†\n- We need to have a deeper neural network since the environment is more complex","metadata":{"id":"aV1466QP8crz"}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        # Define the three layers here\n\n    def forward(self, x):\n        # Define the forward process here\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)","metadata":{"id":"I1eBkCiX2X_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Solution","metadata":{"id":"47iuAFqV8Ws-"}},{"cell_type":"code","source":"class Policy(nn.Module):\n    def __init__(self, s_size, a_size, h_size):\n        super(Policy, self).__init__()\n        self.fc1 = nn.Linear(s_size, h_size)\n        self.fc2 = nn.Linear(h_size, h_size*2)\n        self.fc3 = nn.Linear(h_size*2, a_size)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return F.softmax(x, dim=1)\n    \n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.forward(state).cpu()\n        m = Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)","metadata":{"id":"wrNuVcHC8Xu7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define the hyperparameters ‚öôÔ∏è\n- Because this environment is more complex.\n- Especially for the hidden size, we need more neurons.","metadata":{"id":"SM1QiGCSbBkM"}},{"cell_type":"code","source":"pixelcopter_hyperparameters = {\n    \"h_size\": 64,\n    \"n_training_episodes\": 50000,\n    \"n_evaluation_episodes\": 10,\n    \"max_t\": 10000,\n    \"gamma\": 0.99,\n    \"lr\": 1e-4,\n    \"env_id\": env_id,\n    \"state_space\": s_size,\n    \"action_space\": a_size,\n}","metadata":{"id":"y0uujOR_ypB6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Train it\n- We're now ready to train our agent üî•.","metadata":{"id":"wyvXTJWm9GJG"}},{"cell_type":"code","source":"# Create policy and place it to the device\n# torch.manual_seed(50)\npixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\npixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])","metadata":{"id":"7mM2P_ckysFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = reinforce(pixelcopter_policy,\n                   pixelcopter_optimizer,\n                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n                   pixelcopter_hyperparameters[\"max_t\"],\n                   pixelcopter_hyperparameters[\"gamma\"], \n                   1000)","metadata":{"id":"v1HEqP-fy-Rf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Publish our trained model on the Hub üî•","metadata":{"id":"8kwFQ-Ip85BE"}},{"cell_type":"code","source":"repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\npush_to_hub(repo_id,\n                pixelcopter_policy, # The model we want to save\n                pixelcopter_hyperparameters, # Hyperparameters\n                eval_env, # Evaluation environment\n                video_fps=30\n                )","metadata":{"id":"6PtB7LRbTKWK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some additional challenges üèÜ\nThe best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. But also trying to find better parameters.\n\nIn the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n\nHere are some ideas to achieve so:\n* Train more steps\n* Try different hyperparameters by looking at what your classmates have done üëâ https://huggingface.co/models?other=reinforce\n* **Push your new trained model** on the Hub üî•\n* **Improving the implementation for more complex environments** (for instance, what about changing the network to a Convolutional Neural Network to handle\nframes as observation)?","metadata":{"id":"7VDcJ29FcOyb"}},{"cell_type":"markdown","source":"________________________________________________________________________\n\n**Congrats on finishing this unit**!¬†There was a lot of information.\nAnd congrats on finishing the tutorial. You've just coded your first Deep Reinforcement Learning agent from scratch using PyTorch and shared it on the Hub ü•≥.\n\nDon't hesitate to iterate on this unit **by improving the implementation for more complex environments** (for instance, what about changing the network to a Convolutional Neural Network to handle\nframes as observation)?\n\nIn the next unit, **we're going to learn more about Unity MLAgents**, by training agents in Unity environments. This way, you will be ready to participate in the **AI vs AI challenges where you'll train your agents\nto compete against other agents in a snowball fight and a soccer game.**\n\nSounds fun? See you next time!\n\nFinally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then, please üëâ  [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)\n\nSee you in Unit 5! üî•\n\n### Keep Learning, stay awesome ü§ó\n\n","metadata":{"id":"x62pP0PHdA-y"}}]}