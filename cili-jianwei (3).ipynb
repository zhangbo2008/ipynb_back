{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!huggingface-cli login --token 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 --add-to-git-credential","metadata":{"execution":{"iopub.status.busy":"2023-07-28T07:29:44.632694Z","iopub.execute_input":"2023-07-28T07:29:44.633657Z","iopub.status.idle":"2023-07-28T07:29:46.636310Z","shell.execute_reply.started":"2023-07-28T07:29:44.633615Z","shell.execute_reply":"2023-07-28T07:29:46.634962Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"zhangbo2008/cas_data\", use_auth_token=True , cache_dir='.')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T07:29:46.642549Z","iopub.execute_input":"2023-07-28T07:29:46.644969Z","iopub.status.idle":"2023-07-28T07:30:09.934820Z","shell.execute_reply.started":"2023-07-28T07:29:46.644928Z","shell.execute_reply":"2023-07-28T07:30:09.933640Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset text/zhangbo2008--cas_data to ./text/zhangbo2008--cas_data-c01d11ce25ec5f93/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ac2d0e52174fa18f5a8ce6fa39eb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/53.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8b0674891b45dcb931f7de9c448494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"560b2937299a43b3bb99e31fc5b78e26"}},"metadata":{}},{"name":"stdout","text":"Dataset text downloaded and prepared to ./text/zhangbo2008--cas_data-c01d11ce25ec5f93/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af60bc08d5fc414b86d90628cc8dff87"}},"metadata":{}}]},{"cell_type":"code","source":"cd /kaggle/working/downloads/extracted/b8eba04a54b30799c6ac0df44d1ea95b0e2fb920b6228dd49a718d4632d3d1f0/shuju/jiabi","metadata":{"execution":{"iopub.status.busy":"2023-07-28T07:30:09.936511Z","iopub.execute_input":"2023-07-28T07:30:09.937214Z","iopub.status.idle":"2023-07-28T07:30:09.945905Z","shell.execute_reply.started":"2023-07-28T07:30:09.937177Z","shell.execute_reply":"2023-07-28T07:30:09.944814Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/downloads/extracted/b8eba04a54b30799c6ac0df44d1ea95b0e2fb920b6228dd49a718d4632d3d1f0/shuju/jiabi\n","output_type":"stream"}]},{"cell_type":"code","source":"ls -al","metadata":{"execution":{"iopub.status.busy":"2023-07-28T07:30:09.948274Z","iopub.execute_input":"2023-07-28T07:30:09.949415Z","iopub.status.idle":"2023-07-28T07:30:10.947891Z","shell.execute_reply.started":"2023-07-28T07:30:09.949364Z","shell.execute_reply":"2023-07-28T07:30:10.946609Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"total 2768\ndrwxr-xr-x 2 root root  4096 Jul 28 07:30 \u001b[0m\u001b[01;34m.\u001b[0m/\ndrwxr-xr-x 4 root root  4096 Jul 28 07:30 \u001b[01;34m..\u001b[0m/\n-rw-r--r-- 1 root root 37168 Jul 28 07:30 0.txt\n-rw-r--r-- 1 root root 37163 Jul 28 07:30 1.txt\n-rw-r--r-- 1 root root 37193 Jul 28 07:30 10.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 11.txt\n-rw-r--r-- 1 root root 37191 Jul 28 07:30 12.txt\n-rw-r--r-- 1 root root 37192 Jul 28 07:30 13.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 14.txt\n-rw-r--r-- 1 root root 37191 Jul 28 07:30 15.txt\n-rw-r--r-- 1 root root 37189 Jul 28 07:30 16.txt\n-rw-r--r-- 1 root root 37191 Jul 28 07:30 17.txt\n-rw-r--r-- 1 root root 37187 Jul 28 07:30 18.txt\n-rw-r--r-- 1 root root 37185 Jul 28 07:30 19.txt\n-rw-r--r-- 1 root root 37167 Jul 28 07:30 2.txt\n-rw-r--r-- 1 root root 37170 Jul 28 07:30 20.txt\n-rw-r--r-- 1 root root 37179 Jul 28 07:30 21.txt\n-rw-r--r-- 1 root root 37171 Jul 28 07:30 22.txt\n-rw-r--r-- 1 root root 37174 Jul 28 07:30 23.txt\n-rw-r--r-- 1 root root 37172 Jul 28 07:30 24.txt\n-rw-r--r-- 1 root root 37180 Jul 28 07:30 25.txt\n-rw-r--r-- 1 root root 37177 Jul 28 07:30 26.txt\n-rw-r--r-- 1 root root 37183 Jul 28 07:30 27.txt\n-rw-r--r-- 1 root root 37185 Jul 28 07:30 28.txt\n-rw-r--r-- 1 root root 37194 Jul 28 07:30 29.txt\n-rw-r--r-- 1 root root 37165 Jul 28 07:30 3.txt\n-rw-r--r-- 1 root root 37194 Jul 28 07:30 30.txt\n-rw-r--r-- 1 root root 37197 Jul 28 07:30 31.txt\n-rw-r--r-- 1 root root 37194 Jul 28 07:30 32.txt\n-rw-r--r-- 1 root root 37197 Jul 28 07:30 33.txt\n-rw-r--r-- 1 root root 37195 Jul 28 07:30 34.txt\n-rw-r--r-- 1 root root 37194 Jul 28 07:30 35.txt\n-rw-r--r-- 1 root root 37196 Jul 28 07:30 36.txt\n-rw-r--r-- 1 root root 37196 Jul 28 07:30 37.txt\n-rw-r--r-- 1 root root 37195 Jul 28 07:30 38.txt\n-rw-r--r-- 1 root root 37191 Jul 28 07:30 39.txt\n-rw-r--r-- 1 root root 37168 Jul 28 07:30 4.txt\n-rw-r--r-- 1 root root 37192 Jul 28 07:30 40.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 41.txt\n-rw-r--r-- 1 root root 37195 Jul 28 07:30 42.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 43.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 44.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 45.txt\n-rw-r--r-- 1 root root 37196 Jul 28 07:30 46.txt\n-rw-r--r-- 1 root root 37197 Jul 28 07:30 47.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 48.txt\n-rw-r--r-- 1 root root 37184 Jul 28 07:30 49.txt\n-rw-r--r-- 1 root root 37158 Jul 28 07:30 5.txt\n-rw-r--r-- 1 root root 37186 Jul 28 07:30 50.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 51.txt\n-rw-r--r-- 1 root root 37189 Jul 28 07:30 52.txt\n-rw-r--r-- 1 root root 37185 Jul 28 07:30 53.txt\n-rw-r--r-- 1 root root 37190 Jul 28 07:30 54.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 55.txt\n-rw-r--r-- 1 root root 37189 Jul 28 07:30 56.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 57.txt\n-rw-r--r-- 1 root root 37187 Jul 28 07:30 58.txt\n-rw-r--r-- 1 root root 37185 Jul 28 07:30 59.txt\n-rw-r--r-- 1 root root 37171 Jul 28 07:30 6.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 60.txt\n-rw-r--r-- 1 root root 37187 Jul 28 07:30 61.txt\n-rw-r--r-- 1 root root 37189 Jul 28 07:30 62.txt\n-rw-r--r-- 1 root root 37181 Jul 28 07:30 63.txt\n-rw-r--r-- 1 root root 37192 Jul 28 07:30 64.txt\n-rw-r--r-- 1 root root 37184 Jul 28 07:30 65.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 66.txt\n-rw-r--r-- 1 root root 37184 Jul 28 07:30 67.txt\n-rw-r--r-- 1 root root 37188 Jul 28 07:30 68.txt\n-rw-r--r-- 1 root root 37170 Jul 28 07:30 7.txt\n-rw-r--r-- 1 root root 37165 Jul 28 07:30 8.txt\n-rw-r--r-- 1 root root 37191 Jul 28 07:30 9.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nif 1:\n    #============加载数据.\n    data1='/kaggle/working/downloads/extracted/b8eba04a54b30799c6ac0df44d1ea95b0e2fb920b6228dd49a718d4632d3d1f0/shuju'\n    data=data1+'/jiabi'\n    import numpy as np\n    import pandas as pd\n    # data = np.loadtxt(data+'/0.txt',dtype = int,comments='1',delimiter='')\n    import glob\n    aa=glob.glob(data+'/*.txt')\n    aa=[pd.read_csv(i,sep='\\t',header=None).values[:400,:18].astype(np.float32) for i in aa]\n\n        \n\n    aa=np.stack(aa)\n    old_aa=aa\n    print()\n\n\n\n    data=data1+'/zhenbi'\n\n    import numpy as np\n    import pandas as pd\n    # data = np.loadtxt(data+'/0.txt',dtype = int,comments='1',delimiter='')\n    import glob\n    bb=glob.glob(data+'/*.txt')\n    bb=[pd.read_csv(i,sep='\\t',header=None).values[:400,:18].astype(np.float32) for i in bb]\n\n        \n\n    bb=np.stack(bb)\n    old_bb=bb\n    print()\n\n\n\n    np.save('jiabi.npy',old_aa)\n    np.save('zhenbi.npy',old_bb)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T08:08:44.203464Z","iopub.execute_input":"2023-07-28T08:08:44.203844Z","iopub.status.idle":"2023-07-28T08:08:58.396675Z","shell.execute_reply.started":"2023-07-28T08:08:44.203813Z","shell.execute_reply":"2023-07-28T08:08:58.395622Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nif 0:\n    #============加载数据.\n    data='/mnt/e/hengyin/shuju/jiabi'\n\n    import numpy as np\n    import pandas as pd\n    # data = np.loadtxt(data+'/0.txt',dtype = int,comments='1',delimiter='')\n    import glob\n    aa=glob.glob(data+'/*.txt')\n    aa=[pd.read_csv(i,sep='\\t',header=None).values[:400,:18].astype(np.float32) for i in aa]\n\n        \n\n    aa=np.stack(aa)\n    old_aa=aa\n    print()\n\n\n\n    data='/mnt/e/hengyin/shuju/zhenbi'\n\n    import numpy as np\n    import pandas as pd\n    # data = np.loadtxt(data+'/0.txt',dtype = int,comments='1',delimiter='')\n    import glob\n    bb=glob.glob(data+'/*.txt')\n    bb=[pd.read_csv(i,sep='\\t',header=None).values[:400,:18].astype(np.float32) for i in bb]\n\n        \n\n    bb=np.stack(bb)\n    old_bb=bb\n    print()\n\n\n\n    np.save('jiabi.npy',old_aa)\n    np.save('zhenbi.npy',old_bb)\nimport numpy as np\nif 1:\n    old_aa=np.load('jiabi.npy',)\n    old_bb=np.load('zhenbi.npy',)\n\n#==================大剑网路\n\ninput_chanl=1\n\n\n\nimport torch.nn as nn\nimport torch\n \n \n# 定义ResNet18/34的残差结构，为2个3x3的卷积\nclass BasicBlock(nn.Module):\n    # 判断残差结构中，主分支的卷积核个数是否发生变化，不变则为1\n    expansion = 1\n \n    # init()：进行初始化，申明模型中各层的定义\n    # downsample=None对应实线残差结构，否则为虚线残差结构\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n                               kernel_size=3, stride=stride, padding=1, bias=False)\n        # 使用批量归一化\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        # 使用ReLU作为激活函数\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n                               kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        self.downsample = downsample\n \n    # forward()：定义前向传播过程,描述了各层之间的连接关系\n    def forward(self, x):\n        # 残差块保留原始输入\n        identity = x\n        # 如果是虚线残差结构，则进行下采样\n        if self.downsample is not None:\n            identity = self.downsample(x)\n \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        # -----------------------------------------\n        out = self.conv2(out)\n        out = self.bn2(out)\n        # 主分支与shortcut分支数据相加\n        out += identity\n        out = self.relu(out)\n \n        return out\n \nimport torch.nn.functional as F\n# 定义ResNet50/101/152的残差结构，为1x1+3x3+1x1的卷积\nclass Bottleneck(nn.Module):\n    # expansion是指在每个小残差块内，减小尺度增加维度的倍数，如64*4=256\n    # Bottleneck层输出通道是输入的4倍\n    expansion = 4\n \n    # init()：进行初始化，申明模型中各层的定义\n    # downsample=None对应实线残差结构，否则为虚线残差结构，专门用来改变x的通道数\n    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n                 groups=1, width_per_group=64):\n        super(Bottleneck, self).__init__()\n \n        width = int(out_channel * (width_per_group / 64.)) * groups\n \n        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n                               kernel_size=1, stride=1, bias=False)\n        # 使用批量归一化\n        self.bn1 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n                               kernel_size=3, stride=stride, bias=False, padding=1)\n        self.bn2 = nn.BatchNorm2d(width)\n        # -----------------------------------------\n        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion,\n                               kernel_size=1, stride=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)\n        # 使用ReLU作为激活函数\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n \n    # forward()：定义前向传播过程,描述了各层之间的连接关系\n    def forward(self, x):\n        # 残差块保留原始输入\n        identity = x\n        # 如果是虚线残差结构，则进行下采样\n        if self.downsample is not None:\n            identity = self.downsample(x)\n \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n \n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n \n        out = self.conv3(out)\n        out = self.bn3(out)\n        # 主分支与shortcut分支数据相加\n        out += identity\n        out = self.relu(out)\n \n        return out\n \n \n# 定义ResNet类\nclass ResNet(nn.Module):\n    # 初始化函数\n    def __init__(self,\n                 block,\n                 blocks_num,\n                 num_classes=1000,\n                 include_top=True,\n                 groups=1,\n                 width_per_group=64):\n        super(ResNet, self).__init__()\n        self.include_top = include_top\n        # maxpool的输出通道数为64，残差结构输入通道数为64\n        self.in_channel = 64\n \n        self.groups = groups\n        self.width_per_group = width_per_group\n \n        self.conv1 = nn.Conv2d(input_chanl, self.in_channel, kernel_size=7, stride=2,\n                               padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_channel)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        # 浅层的stride=1，深层的stride=2\n        # block：定义的两种残差模块\n        # block_num：模块中残差块的个数\n        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        if self.include_top:\n            # 自适应平均池化，指定输出（H，W），通道数不变\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n            # 全连接层\n            self.fc = nn.Linear(512 * block.expansion, num_classes)\n        # 遍历网络中的每一层\n        # 继承nn.Module类中的一个方法:self.modules(), 他会返回该网络中的所有modules\n        for m in self.modules():\n            # isinstance(object, type)：如果指定对象是指定类型，则isinstance()函数返回True\n            # 如果是卷积层\n            if isinstance(m, nn.Conv2d):\n                # kaiming正态分布初始化，使得Conv2d卷积层反向传播的输出的方差都为1\n                # fan_in：权重是通过线性层（卷积或全连接）隐性确定\n                # fan_out：通过创建随机矩阵显式创建权重\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n \n    # 定义残差模块，由若干个残差块组成\n    # block：定义的两种残差模块，channel：该模块中所有卷积层的基准通道数。block_num：模块中残差块的个数\n    def _make_layer(self, block, channel, block_num, stride=1):\n        downsample = None\n        # 如果满足条件，则是虚线残差结构\n        if stride != 1 or self.in_channel != channel * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(channel * block.expansion))\n \n        layers = []\n        layers.append(block(self.in_channel,\n                            channel,\n                            downsample=downsample,\n                            stride=stride,\n                            groups=self.groups,\n                            width_per_group=self.width_per_group))\n        self.in_channel = channel * block.expansion\n \n        for _ in range(1, block_num):\n            layers.append(block(self.in_channel,\n                                channel,\n                                groups=self.groups,\n                                width_per_group=self.width_per_group))\n        # Sequential：自定义顺序连接成模型，生成网络结构\n        return nn.Sequential(*layers)\n \n    # forward()：定义前向传播过程,描述了各层之间的连接关系\n    def forward(self, x):\n        # 无论哪种ResNet，都需要的静态层\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        # 动态层\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n \n        if self.include_top:\n            x = self.avgpool(x)\n            x = torch.flatten(x, 1)\n            x = self.fc(x)\n        #===========my head: 自己改写一个head.\n        # x = self.avgpool(x)\n        # x=x.view(x.shape[0],x.shape[1]) # 打到512向量.\n        #=======\n        #=====接一个l2norm头\n        x=F.normalize( x, p=2, dim=1)\n        return x\n \n# ResNet()中block参数对应的位置是BasicBlock或Bottleneck\n# ResNet()中blocks_num[0-3]对应[3, 4, 6, 3]，表示残差模块中的残差数\n# 34层的resnet\ndef resnet34(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n \n \n# 50层的resnet\ndef resnet50(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n \n \n# 101层的resnet\ndef resnet101(num_classes=1000, include_top=True):\n    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\ndef resnet18(num_classes=1000, include_top=True):\n    \"\"\"Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    # model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    # if pretrained:\n    #     model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n    # return model\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, include_top=include_top)\n\n\nmodel=resnet34(num_classes=128, include_top=True)\n# aaa= np.expand_dims(aa[0],axis=0)    # 拓展channel\n# aaa= np.expand_dims(aaa,axis=0)        # 拓展batchsize\n# print(model(torch.tensor(aaa)))\n\n\n\n\n\n\n\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport pdb\nimport random\nold_all=np.vstack([old_aa,old_bb])\nold_all=torch.tensor( np.expand_dims(old_all,axis=1) )\nold_all_saver_for_test=old_all\n\n\n#===================setp3训练策略: 让我们一个batch里面任意2个的距离都尽量大.\nif 1:\n    net = model\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    net.to(device)\n    net.train()\n    loss_func = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n    for epoch in range(100):\n        running_loss = 0.0\n        bs=200\n\n#==========数据shuffle\n        dex=list(range(len(old_all)))\n        random.shuffle(dex)\n        old_all=old_all[dex]\n\n        for i in range(len(old_all)//bs):\n            tmp=old_all[i*bs:(i+1)*bs]\n            inputs = tmp\n            optimizer.zero_grad()\n            outputs = net(inputs.to(device))\n            loss = torch.sum(outputs@  (outputs.T)  )/(bs**2) # bs是下界,我们loss最后到0为最好.\n            #注意这里必须加item,否则爆显存.\n            print(loss.item())\n            loss.backward()\n            optimizer.step()\n\n            ","metadata":{"execution":{"iopub.status.busy":"2023-07-28T08:32:59.720507Z","iopub.execute_input":"2023-07-28T08:32:59.720890Z","iopub.status.idle":"2023-07-28T08:40:15.221440Z","shell.execute_reply.started":"2023-07-28T08:32:59.720859Z","shell.execute_reply":"2023-07-28T08:40:15.220106Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"0.90375155210495\n0.2940177023410797\n0.12286044657230377\n0.08332879841327667\n0.046696655452251434\n0.019104128703475\n0.014086286537349224\n0.008597646839916706\n0.007184889167547226\n0.005520147271454334\n0.006440510507673025\n0.005769256502389908\n0.004064974375069141\n0.003426178591325879\n0.0026583243161439896\n0.0059604826383292675\n0.002128676977008581\n0.004646757151931524\n0.005453667137771845\n0.008552257902920246\n0.0026391632854938507\n0.0076703960075974464\n0.0049624876119196415\n0.0068195173516869545\n0.008139329962432384\n0.0013848352245986462\n0.006242237985134125\n0.010195699520409107\n0.003941891249269247\n0.007741148117929697\n0.001591928768903017\n0.007148173172026873\n0.0019905115477740765\n0.004694155417382717\n0.01065282616764307\n0.0014118626713752747\n0.0008122565923258662\n0.0007532262825407088\n0.0037014433182775974\n0.005105614196509123\n0.000822761794552207\n0.0009000135469250381\n0.0009447833872400224\n0.0005908901221118867\n0.001572679029777646\n0.0024152141995728016\n0.002172899665310979\n0.004266035743057728\n0.0030832041520625353\n0.0015211694408208132\n0.0015915153780952096\n0.002941917162388563\n0.0007087293197400868\n0.0014675784623250365\n0.0012126981746405363\n0.0005956071545369923\n0.0018986129434779286\n0.0012694410979747772\n0.0008814830798655748\n0.007020388729870319\n0.002633564407005906\n0.001295948983170092\n0.0013392820255830884\n0.00056120241060853\n0.0055870721116662025\n0.002170809544622898\n0.004162583965808153\n0.0004402305348776281\n0.0010882937349379063\n0.000462274911114946\n0.0022410147357732058\n0.004516144283115864\n0.00046929396921768785\n0.0011665058555081487\n0.0015324660344049335\n0.0006900957087054849\n0.0009627395193092525\n0.003627675585448742\n0.002885623835027218\n0.001292299828492105\n0.0004315500264056027\n0.0004519416834227741\n0.009747656062245369\n0.0035546431317925453\n0.0034941607154905796\n0.002385982545092702\n0.0015992553671821952\n0.0018167078960686922\n0.0027891993522644043\n0.002573091769590974\n0.0014366561081260443\n0.004235552158206701\n0.002447146223857999\n0.01019511092454195\n0.0009219791390933096\n0.000793744286056608\n0.0010463206563144922\n0.0009205985115841031\n0.0006216243491508067\n0.0039542485028505325\n0.0009411481441929936\n0.0006840831483714283\n0.0007825446664355695\n0.002779969945549965\n0.009186500683426857\n0.0010250243358314037\n0.0004192688502371311\n0.00649748370051384\n0.002797164488583803\n0.0015678447671234608\n0.0019294265657663345\n0.0005408744909800589\n0.00031692275661043823\n0.0013978624483570457\n0.0011940982658416033\n0.0006111866096034646\n0.0005822342936880887\n0.0022942188661545515\n0.006668316666036844\n0.003178813960403204\n0.0007561662350781262\n0.0061077275313436985\n0.009351011365652084\n0.0032704288605600595\n0.0012533791596069932\n0.0015150717226788402\n0.0008370594587177038\n0.002919659251347184\n0.0022306928876787424\n0.00031019828747957945\n0.007962607778608799\n0.001249166438356042\n0.0018569919047877192\n0.009817557409405708\n0.0006515514105558395\n0.0027650368865579367\n0.0029085029382258654\n0.0018554256530478597\n0.0010428701061755419\n0.007445535622537136\n0.0009798735845834017\n0.003380269045010209\n0.00033990154042840004\n0.005534777883440256\n0.0031972266733646393\n0.0015388568863272667\n0.0018853633664548397\n0.006353903096169233\n0.003917669411748648\n0.0019353701500222087\n0.0013024583458900452\n0.0004299995198380202\n0.0005743434885516763\n0.00181968801189214\n0.010675059631466866\n0.0013959440402686596\n0.0024583321064710617\n0.00515326764434576\n0.0008249453385360539\n0.0008784210658632219\n0.002560327062383294\n0.0003823358565568924\n0.002138493349775672\n0.002711160574108362\n0.0007656270754523575\n0.0052081323228776455\n0.0004118026699870825\n0.0014283622149378061\n0.000580371473915875\n0.0005545623716898263\n0.004135271534323692\n0.0018790406174957752\n0.0033313469029963017\n0.0018513923278078437\n0.0008067046874202788\n0.0003006563929375261\n0.0011788104893639684\n0.00554203474894166\n0.0047544799745082855\n0.0025011831894516945\n0.007042459677904844\n0.0003118461463600397\n0.003274315968155861\n0.0011277578305453062\n0.00059427012456581\n0.0020731566473841667\n0.0005037530791014433\n0.0004566503339447081\n0.004933116026222706\n0.009837847203016281\n0.0030703090596944094\n0.001051451894454658\n0.001267388928681612\n0.0022854418493807316\n0.003042693715542555\n0.0010550909209996462\n0.005775166675448418\n0.0015808860771358013\n0.0008085545268841088\n0.0013219142565503716\n0.004609748721122742\n0.00045539808343164623\n0.003977398853749037\n0.002346767345443368\n0.00048087825416587293\n0.0020562317222356796\n0.001789366127923131\n0.0024910944048315287\n0.0007764747133478522\n0.0020878666546195745\n0.00040631694719195366\n0.0003620483330450952\n0.00015558833547402173\n0.003198118880391121\n0.0006980777834542096\n0.0011426539858803153\n0.0035278243012726307\n0.0015394118381664157\n0.0007969428552314639\n0.000440947333117947\n0.0021487243939191103\n0.0005981010617688298\n0.0018191562267020345\n0.0012101096799597144\n0.0024490971118211746\n0.0008065703441388905\n0.0010952202137559652\n0.0013978935312479734\n0.0012114541605114937\n0.00087480474030599\n0.004535824526101351\n0.0010357979917898774\n0.0037575094029307365\n0.0014349027769640088\n0.00019094314484391361\n0.0008887061849236488\n0.00033568532671779394\n0.0012627483811229467\n0.0008571463404223323\n0.001497104181908071\n0.0009298906079493463\n0.00087311293464154\n0.0007892692228779197\n0.005284096579998732\n0.0002993469242937863\n0.0013082794612273574\n0.0007575106574222445\n0.0008603103342466056\n0.007493462413549423\n0.0014325956581160426\n0.002782208379358053\n0.000980969169177115\n0.003104213159531355\n0.008984019979834557\n0.0009886164916679263\n0.000572605524212122\n0.0007503792294301093\n0.001844395766966045\n0.0008192784152925014\n0.0006077682483009994\n0.0015692062443122268\n0.004363855812698603\n0.0004895894671790302\n0.0017899152589961886\n0.0013494737213477492\n0.0007228442700579762\n0.0005284991930238903\n0.0019277686951681972\n0.0051198601722717285\n0.0005883616977371275\n0.0003764989669434726\n0.0012904857285320759\n0.0009679123759269714\n0.0036970069631934166\n0.014252080582082272\n0.0011066222796216607\n0.001830406952649355\n0.006313850171864033\n0.009382033720612526\n0.007538436912000179\n0.0021577770821750164\n0.002231868216767907\n0.0005289940745569766\n0.0009940454037860036\n0.0008799663046374917\n0.0005787808331660926\n0.00939683336764574\n0.002191345440223813\n0.0015251531731337309\n0.002778220223262906\n0.0004056082107126713\n0.002454507863149047\n0.0007741617155261338\n0.0010812857653945684\n0.0037835966795682907\n0.001403073314577341\n0.0005410377634689212\n0.0007061317446641624\n0.00035416841274127364\n0.0022171151358634233\n0.000545841408893466\n0.0008277561864815652\n0.0002825553820002824\n0.012870428152382374\n0.001129153766669333\n0.0023867697454988956\n0.0005045694415457547\n0.0007646026206202805\n0.000976337818428874\n0.0009848900372162461\n0.0006099791498854756\n0.003142137546092272\n0.002870001131668687\n0.0003190284769516438\n0.0007964065298438072\n0.0013607273576781154\n0.0005930944462306798\n0.0006541017210111022\n0.001865350641310215\n0.0022528422996401787\n0.0013091338332742453\n0.0007246158202178776\n0.004813999403268099\n0.002171616768464446\n0.0019052982097491622\n0.0016882852651178837\n0.002525568939745426\n0.002164883306249976\n0.0017828242853283882\n0.0017206220654770732\n0.001350930891931057\n0.000628899724688381\n0.007665193174034357\n0.0035059526562690735\n0.0014854188775643706\n0.002698018681257963\n0.0017738788155838847\n0.004074771422892809\n0.0005904111894778907\n0.002023515058681369\n0.0012645283713936806\n0.0009182149660773575\n0.002379372715950012\n0.0013875606236979365\n0.0013195540523156524\n0.0018830130575224757\n0.0065987929701805115\n0.0019186302088201046\n0.0003733862831722945\n0.0007402417832054198\n0.0018099164590239525\n0.0023315369617193937\n0.005632120184600353\n0.0014058417873457074\n0.0019512784201651812\n0.0006857956759631634\n0.0010834622662514448\n0.000743506767321378\n0.004607583396136761\n0.0004510631551966071\n0.0029482082463800907\n0.0008927794988267124\n0.0004597534134518355\n0.0012894722167402506\n0.004643506836146116\n0.0021207667887210846\n0.0010054870508611202\n0.0008041610708460212\n0.00029979090322740376\n0.0036966376937925816\n0.0014663884649053216\n0.0018274652538821101\n0.0017610794166103005\n0.0011372450971975923\n0.0022426238283514977\n0.003114772029221058\n0.002299486193805933\n0.001721491222269833\n0.00444237794727087\n0.0011256864527240396\n0.00129440997261554\n0.0026474925689399242\n0.0017394229071214795\n0.0017228543292731047\n0.001840340904891491\n0.003398663131520152\n0.005235824268311262\n0.00230081332847476\n0.0006062156753614545\n0.0019519550260156393\n0.005864076316356659\n0.0026571531780064106\n0.004346504341810942\n0.002906383713707328\n0.0007923447410576046\n0.0014016960049048066\n0.0006756276125088334\n0.002181574236601591\n0.0015969675732776523\n0.001953614642843604\n0.0006030173972249031\n0.0016208818415179849\n0.001431034761480987\n0.0010118131758645177\n0.0008598369313403964\n0.0016329879872500896\n0.0037401290610432625\n0.0011450443416833878\n0.0010710107162594795\n0.002511620754376054\n0.0026726913638412952\n0.0019846931099891663\n0.003677044529467821\n0.0022702652495354414\n0.0019953905139118433\n0.0015179460169747472\n0.001333400490693748\n0.002272204961627722\n0.0035103983245790005\n0.004206399898976088\n0.007208163384348154\n0.0018207307439297438\n0.0007182731642387807\n0.0003135679289698601\n0.007485805545002222\n0.0021429050248116255\n0.00555822579190135\n0.0011114990338683128\n0.00444822246208787\n0.0026108312886208296\n0.0012778707314282656\n0.0014602679293602705\n0.0016090337885543704\n0.0019523192895576358\n0.0012193042784929276\n0.001665070652961731\n0.0006808166508562863\n0.0020019137300550938\n0.000377336866222322\n0.0006194293964654207\n0.002017261227592826\n0.005612495820969343\n0.0020026792772114277\n0.004558762069791555\n0.0027489957865327597\n0.001005724654532969\n0.000905174994841218\n0.0008469423046335578\n0.0004939652280882001\n0.004187155980616808\n0.004978112876415253\n0.00786187220364809\n0.003338230075314641\n0.0013173421612009406\n0.0008167532505467534\n0.0018845672020688653\n0.0009596679592505097\n0.004192131105810404\n0.004847538657486439\n0.0007232024800032377\n0.0011831542942672968\n0.0015437687980011106\n0.002235651481896639\n0.002063667168840766\n0.0008781650103628635\n0.000924637948628515\n0.0013644619612023234\n0.0021096926648169756\n0.0013880369951948524\n0.003754950128495693\n0.0008379410719498992\n0.0011098836548626423\n0.007079835515469313\n0.0019282599678263068\n0.001227254862897098\n0.007730460725724697\n0.0004724647442344576\n0.001034350018016994\n0.0013785442570224404\n0.0011123536387458444\n0.006984129082411528\n0.0015970286913216114\n0.0022088410332798958\n0.00304472167044878\n0.0004348111979197711\n0.0001434761070413515\n0.0006194282323122025\n0.002697775373235345\n0.0003374814987182617\n0.0005614610272459686\n0.0002645868225954473\n0.0006132129346951842\n0.0010941653745248914\n0.0002485860895831138\n0.0016889905091375113\n0.0026779589243233204\n0.0019405997591093183\n0.0013224451104179025\n0.00013595503696706146\n0.0023232295643538237\n0.0008725527441129088\n0.0016429161187261343\n0.00029511633329093456\n0.0014944903086870909\n0.0003135758452117443\n0.0015392398927360773\n0.002687514526769519\n0.0021488997153937817\n0.00018344134150538594\n0.00032771890982985497\n0.0005241665639914572\n0.003712211037054658\n0.001289147767238319\n0.002017486374825239\n0.0002700402110349387\n0.002218506531789899\n0.0016747170593589544\n0.0023474441841244698\n0.0023539878893643618\n0.0013230838812887669\n0.0004307014460209757\n0.005460696294903755\n0.0004381980688776821\n0.010513079352676868\n0.0017659378936514258\n0.0006577585591003299\n0.0009171167039312422\n0.000761801318731159\n0.0002074584917863831\n0.0019053098512813449\n0.001201447332277894\n0.002231031423434615\n0.0015607872046530247\n0.00237517268396914\n0.003407439449802041\n0.0011495111975818872\n0.0030042582657188177\n0.0007910127169452608\n0.0005726708332076669\n0.0009352882043458521\n0.0037988582625985146\n0.000419259158661589\n0.0034081381745636463\n0.0004679124103859067\n0.0002627929497975856\n0.0009333834168501198\n0.00025693225325085223\n0.0008992358925752342\n0.002289994154125452\n0.0015182483475655317\n0.0002116250980179757\n0.0020384734962135553\n0.0013732422376051545\n0.0018589765531942248\n0.0010304494062438607\n0.0005409353179857135\n0.00252913823351264\n0.0009472946985624731\n0.0023212581872940063\n0.0007453429861925542\n0.00113096390850842\n0.0041247461922466755\n0.0016409523086622357\n0.0008097924874164164\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 329\u001b[0m\n\u001b[1;32m    327\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(outputs\u001b[38;5;241m@\u001b[39m  (outputs\u001b[38;5;241m.\u001b[39mT)  )\u001b[38;5;241m/\u001b[39m(bs\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# bs是下界,我们loss最后到0为最好.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m#注意这里必须加item,否则爆显存.\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    330\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    331\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}