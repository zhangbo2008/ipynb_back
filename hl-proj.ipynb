{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/zhangbo2008/hl_detec","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-28T09:36:42.031399Z","iopub.execute_input":"2023-06-28T09:36:42.031713Z","iopub.status.idle":"2023-06-28T09:36:45.703062Z","shell.execute_reply.started":"2023-06-28T09:36:42.031686Z","shell.execute_reply":"2023-06-28T09:36:45.701612Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'hl_detec'...\nremote: Enumerating objects: 3044, done.\u001b[K\nremote: Counting objects: 100% (3044/3044), done.\u001b[K\nremote: Compressing objects: 100% (2107/2107), done.\u001b[K\nremote: Total 3044 (delta 933), reused 3044 (delta 933), pack-reused 0\u001b[K\nReceiving objects: 100% (3044/3044), 12.58 MiB | 11.87 MiB/s, done.\nResolving deltas: 100% (933/933), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd hl_detec","metadata":{"execution":{"iopub.status.busy":"2023-06-28T09:36:45.705418Z","iopub.execute_input":"2023-06-28T09:36:45.705801Z","iopub.status.idle":"2023-06-28T09:36:45.717903Z","shell.execute_reply.started":"2023-06-28T09:36:45.705766Z","shell.execute_reply":"2023-06-28T09:36:45.716683Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/hl_detec\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm tvsum50_ver_1_1.tgz -rf  # 先删除旧文件就非常robost\n!wget https://huggingface.co/datasets/zhangbo2008/tvsum/resolve/main/tvsum50_ver_1_1.tgz","metadata":{"execution":{"iopub.status.busy":"2023-06-28T09:36:45.719788Z","iopub.execute_input":"2023-06-28T09:36:45.720558Z","iopub.status.idle":"2023-06-28T09:37:02.633265Z","shell.execute_reply.started":"2023-06-28T09:36:45.720501Z","shell.execute_reply":"2023-06-28T09:37:02.631765Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2023-06-28 09:36:48--  https://huggingface.co/datasets/zhangbo2008/tvsum/resolve/main/tvsum50_ver_1_1.tgz\nResolving huggingface.co (huggingface.co)... 108.138.94.52, 108.138.94.97, 108.138.94.45, ...\nConnecting to huggingface.co (huggingface.co)|108.138.94.52|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/repos/cf/2f/cf2f2fea5cd14619dff8af32aded7579b33ded418dc4bb1f84b5099b96a85cd0/407d340bcd06fdc6d17374ebe6760b4a96816bcace228559c8283d9fb2520dea?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tvsum50_ver_1_1.tgz%3B+filename%3D%22tvsum50_ver_1_1.tgz%22%3B&Expires=1688204209&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NmLzJmL2NmMmYyZmVhNWNkMTQ2MTlkZmY4YWYzMmFkZWQ3NTc5YjMzZGVkNDE4ZGM0YmIxZjg0YjUwOTliOTZhODVjZDAvNDA3ZDM0MGJjZDA2ZmRjNmQxNzM3NGViZTY3NjBiNGE5NjgxNmJjYWNlMjI4NTU5YzgyODNkOWZiMjUyMGRlYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgyMDQyMDl9fX1dfQ__&Signature=A9qpODB2hmNUSMle2bp6VOeC0NKtwyPhg23lyps1G6BGlXsTRAj8oYI2j46zO3Xxn7ixDCttVUermAhEa9hHO%7ESeM2%7EzErQfZSLEFLFfEYrFFVpdrLxt1AROGypI-A2hcq20xijP6sQSBPVCXzOHpcvUVM42P6ldWGZE5fmuopA4Sro83GNVRDNCiz1Fc2xXlfOVo1Wzfb6F1WK5w9GJBF-PLOJJMaX07GrIOvV08p8o0SgYDuQG-uUtCOYPYEg0ZHmi7lTpklIG5ZCosrA7jg0sqyVDY717U6Bub9B%7EVimPscSKpQf5DDIJCdt4Fm-Ozb7rr4ULcM5FT4bGsBbgSA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n--2023-06-28 09:36:48--  https://cdn-lfs.huggingface.co/repos/cf/2f/cf2f2fea5cd14619dff8af32aded7579b33ded418dc4bb1f84b5099b96a85cd0/407d340bcd06fdc6d17374ebe6760b4a96816bcace228559c8283d9fb2520dea?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27tvsum50_ver_1_1.tgz%3B+filename%3D%22tvsum50_ver_1_1.tgz%22%3B&Expires=1688204209&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2NmLzJmL2NmMmYyZmVhNWNkMTQ2MTlkZmY4YWYzMmFkZWQ3NTc5YjMzZGVkNDE4ZGM0YmIxZjg0YjUwOTliOTZhODVjZDAvNDA3ZDM0MGJjZDA2ZmRjNmQxNzM3NGViZTY3NjBiNGE5NjgxNmJjYWNlMjI4NTU5YzgyODNkOWZiMjUyMGRlYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgyMDQyMDl9fX1dfQ__&Signature=A9qpODB2hmNUSMle2bp6VOeC0NKtwyPhg23lyps1G6BGlXsTRAj8oYI2j46zO3Xxn7ixDCttVUermAhEa9hHO%7ESeM2%7EzErQfZSLEFLFfEYrFFVpdrLxt1AROGypI-A2hcq20xijP6sQSBPVCXzOHpcvUVM42P6ldWGZE5fmuopA4Sro83GNVRDNCiz1Fc2xXlfOVo1Wzfb6F1WK5w9GJBF-PLOJJMaX07GrIOvV08p8o0SgYDuQG-uUtCOYPYEg0ZHmi7lTpklIG5ZCosrA7jg0sqyVDY717U6Bub9B%7EVimPscSKpQf5DDIJCdt4Fm-Ozb7rr4ULcM5FT4bGsBbgSA__&Key-Pair-Id=KVTP0A1DKRTAX\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.238.192.50, 18.238.192.34, 18.238.192.98, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.238.192.50|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 671779858 (641M) [binary/octet-stream]\nSaving to: ‘tvsum50_ver_1_1.tgz’\n\ntvsum50_ver_1_1.tgz 100%[===================>] 640.66M  50.7MB/s    in 13s     \n\n2023-06-28 09:37:02 (48.7 MB/s) - ‘tvsum50_ver_1_1.tgz’ saved [671779858/671779858]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar -xvzf tvsum50_ver_1_1.tgz","metadata":{"execution":{"iopub.status.busy":"2023-06-28T09:37:02.636338Z","iopub.execute_input":"2023-06-28T09:37:02.637822Z","iopub.status.idle":"2023-06-28T09:37:08.513992Z","shell.execute_reply.started":"2023-06-28T09:37:02.637784Z","shell.execute_reply":"2023-06-28T09:37:08.512800Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"./WebscopeReadMe.txt\n./ydata-tvsum50-v1_1/\n./ydata-tvsum50-v1_1/README\n./ydata-tvsum50-v1_1/ydata-tvsum50-data.zip\n./ydata-tvsum50-v1_1/ydata-tvsum50-matlab.zip\n./ydata-tvsum50-v1_1/ydata-tvsum50-thumbnail.zip\n./ydata-tvsum50-v1_1/ydata-tvsum50-video.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip ./ydata-tvsum50-v1_1/ydata-tvsum50-video.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-28T09:37:08.515588Z","iopub.execute_input":"2023-06-28T09:37:08.515970Z","iopub.status.idle":"2023-06-28T09:37:14.432199Z","shell.execute_reply.started":"2023-06-28T09:37:08.515932Z","shell.execute_reply":"2023-06-28T09:37:14.430752Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Archive:  ./ydata-tvsum50-v1_1/ydata-tvsum50-video.zip\n   creating: video/\n  inflating: video/-esJrBWj2d8.mp4   \n  inflating: video/0tmA_C6XwfM.mp4   \n  inflating: video/37rzWOQsNIw.mp4   \n  inflating: video/3eYKfiOEJNs.mp4   \n  inflating: video/4wU_LUjG5Ic.mp4   \n  inflating: video/91IHQYk1IQM.mp4   \n  inflating: video/98MoyGZKHXc.mp4   \n  inflating: video/_xMr-HKMfVA.mp4   \n  inflating: video/akI8YFjEmUw.mp4   \n  inflating: video/AwmHb44_ouw.mp4   \n  inflating: video/b626MiF1ew4.mp4   \n  inflating: video/Bhxk-O1Y7Ho.mp4   \n  inflating: video/byxOvuiIJV0.mp4   \n  inflating: video/cjibtmSLxQ4.mp4   \n  inflating: video/E11zDS9XGzg.mp4   \n  inflating: video/EE-bNr36nyA.mp4   \n  inflating: video/eQu1rNs0an0.mp4   \n  inflating: video/EYqVtI9YWJA.mp4   \n  inflating: video/fWutDQy1nnY.mp4   \n  inflating: video/GsAD1KT1xo8.mp4   \n  inflating: video/gzDbaEs1Rlg.mp4   \n  inflating: video/Hl-__g2gn_A.mp4   \n  inflating: video/HT5vyqe0Xaw.mp4   \n  inflating: video/i3wAGJaaktw.mp4   \n  inflating: video/iVt07TCkFM0.mp4   \n  inflating: video/J0nA4VgnoCo.mp4   \n  inflating: video/jcoYJXDG9sw.mp4   \n  inflating: video/JgHubY5Vw3Y.mp4   \n  inflating: video/JKpqYvAdIsw.mp4   \n  inflating: video/kLxoNp-UchI.mp4   \n  inflating: video/LRw_obCPUt0.mp4   \n  inflating: video/NyBmCxDoHJU.mp4   \n  inflating: video/oDXZc0tZe04.mp4   \n  inflating: video/PJrm840pAUI.mp4   \n  inflating: video/qqR6AEXwxoQ.mp4   \n  inflating: video/RBCABdttQmI.mp4   \n  inflating: video/Se3oxnaPsz0.mp4   \n  inflating: video/sTEELN-vY30.mp4   \n  inflating: video/uGu_10sucQo.mp4   \n  inflating: video/vdmoEJ5YbrQ.mp4   \n  inflating: video/VuWGsYPqAX8.mp4   \n  inflating: video/WG0MBPpPC6I.mp4   \n  inflating: video/WxtbjNsCQ8A.mp4   \n  inflating: video/XkqCExn6_Us.mp4   \n  inflating: video/xmEERLqJ2kU.mp4   \n  inflating: video/xwqBXPGE9pQ.mp4   \n  inflating: video/xxdtq8mxegs.mp4   \n  inflating: video/XzYM3PfTM4w.mp4   \n  inflating: video/Yi4Ij2NM7U4.mp4   \n  inflating: video/z_6gVvQb2d0.mp4   \n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install av\n!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-06-28T09:37:14.433988Z","iopub.execute_input":"2023-06-28T09:37:14.434364Z","iopub.status.idle":"2023-06-28T09:37:40.451018Z","shell.execute_reply.started":"2023-06-28T09:37:14.434328Z","shell.execute_reply":"2023-06-28T09:37:40.449811Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-10.0.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# =======vid path :  /mnt/e/ydata-tvsum50-v1_1/video\n\n\n\nimport av\nimport torch\nimport numpy as np\n\nfrom transformers import AutoProcessor, AutoModel\nfrom huggingface_hub import hf_hub_download\ndevice = torch.device(\"cuda\" if torch.cuda.is_available()else \"cpu\") \nnp.random.seed(0)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\nmodel = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")\n\n\ndef read_video_pyav(container, indices):\n    '''\n    Decode the video with PyAV decoder.\n    Args:\n        container (`av.container.input.InputContainer`): PyAV container.\n        indices (`List[int]`): List of frame indices to decode.\n    Returns:\n        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n    '''\n    frames = []\n    container.seek(0)\n    start_index = indices[0]\n    end_index = indices[-1]\n    for i, frame in enumerate(container.decode(video=0)):\n        if i > end_index:\n            break\n        if i >= start_index and i in indices:\n            frames.append(frame)\n    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n#=============做数据集\n\nimport numpy as np\n\ndatap='video'\nimport pandas as pd\nanno='ydata-tvsum50-anno.tsv'\ninfo='ydata-tvsum50-info.tsv'\n\n\nall_video=[]\nall_label=[]\nchulitvsum=1\nidex=0\ninfo = pd.read_csv(info, sep=\"\\t\")\nif chulitvsum:\n print()\n\n\n    \n\n\nprint()\n\n#================ferz\n\n\nprint('开始训练')\nepoch=10\nfrom torch import nn, optim\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\nimport torch.optim as optim\n\nfor p in model.parameters():\n    p.requires_grad=False\n\nfor p in model.base_model.last.parameters():\n    p.requires_grad=True\nprint()\n\n\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\nmodel=model.to(device)\nfor i in range(epoch):\n\n#   for out3, out4 in zip(all_video,all_label):\n#     #  getdata\n  for idex in range(len(info)):\n    # idex=0\n    \n    time=[]\n    for i in range(len(info)):\n        tmp=info.iloc[i].length\n        time.append(int(tmp.split(':')[0])*60+int(tmp.split(':')[1]))\n\n    import glob\n    files=glob.glob(datap+'/*.mp4')\n\n\n    print(1)\n    dummy=datap+'/'+info.iloc[idex].video_id+'.mp4'  #取第一个做dummytest\n    dummy_shipinchang=time[idex] # diyige shipinchnag .\n    dummy_fps=1\n    import av\n    container = av.open(dummy)\n    container.streams.video[0].average_rate\n    time=container.streams.video[0].duration/10000\n    time=dummy_shipinchang\n    frames=container.streams.video[0].frames\n    fps=frames/time\n    print(1)\n\n    indices=[]\n    jiange=int(fps*2/8) # 每8个一组\n    for i in range(0,frames,jiange):\n        indices.append(i)\n\n    print(1) \n    t=[]\n    out=[]\n    for i in indices:\n\n        t.append(i)\n        if len(t)==8:\n                out.append(t)\n                t=[]\n    # if t:\n    #     out.append(t)\n    hout=out\n    print(2)\n    #-------每一组的平分.\n\n\n    # import numpy as np \n    # df = pd.read_csv(anno, sep=\"\\t\")  # 用pd速度快.\n    # df.iloc[0]\n    with open(anno) as f:\n        tmp=f.readlines()\n\n    tmp=[''.join(i.strip().split('\\t')[2:]).split(',') for i in tmp]\n    tmp2=[]\n    for i in range(len(tmp)):\n        tmp2.append([int(i) for i in tmp[i]])\n    tmp=tmp2 \n    all=[]\n#     for i in tmp:\n#         print(len(i))\n    for i in range(0,len(tmp),20):\n        \n        t=(np.array(tmp[i])+np.array(tmp[i+1])+np.array(tmp[i+2]))/3\n#         print()\n        all.append(t)\n    #========改变out平分.\n#     print()\n    all2=[]\n    frames2 = []\n    container.seek(0)\n    out=sum(out,[])\n    for i, frame in enumerate(container.decode(video=0)):\n        if i in out:\n            frames2.append(frame)\n#     print()\n    t=[]\n    out2=[]\n    for i in frames2:\n\n        t.append(i)\n        if len(t)==8:\n                out2.append(t)\n                t=[]\n    # if t:\n    #     out2.append(t)\n#     print(2)\n    out3=[]\n    for i in out2:\n\n        out3.append(np.stack([x.to_ndarray(format=\"rgb24\") for x in i]))\n    print(len(out3),'numberofkeyframe')\n    print()\n    fenshu=all[idex]\n    hout\n    out4=[]\n    for i in hout:\n        ttt=fenshu[i]\n        t=np.mean(ttt)/5 #===============guiyihua\n        out4.append(t)\n    print()\n    # all_video.append(out3)\n    # all_label.append(out4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n    bs=100\n    for i in range(0,len(out3),bs):\n        tmp1=out3[i:i+bs]\n        tmp2=out4[i:i+bs]\n\n\n\n        optimizer.zero_grad()\n        \n        video=tmp1\n\n        tmp2=torch.tensor(tmp2).to(device)\n\n        # Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.\n        inputs = processor(\n            text=[\"\"],\n            videos=list(video),\n            return_tensors=\"pt\",\n            padding=True,\n        )\n        for i in inputs:\n            inputs[i]=inputs[i].to(device)\n\n\n\n\n\n        #=============收集所有的数据和标签.\n\n    #     print(inputs,66666666666666666)\n\n\n\n        # forward pass\n        model.train()\n        outputs,loss = model(**inputs,return_loss=True,label=tmp2)\n\n\n        print(loss,'当前损失')\n        loss.backward()\n        optimizer.step()\n\n\n\n\nprint('over_train')\n# logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n# probs = logits_per_video.softmax(dim=1)  # we can take the softmax to get the label probabilities\n# print(probs)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-28T09:37:40.453261Z","iopub.execute_input":"2023-06-28T09:37:40.453682Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/309 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e21f6b419b584d2d9552e69cb3a6391d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/965 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953f163663f749fcb98b0849118c0458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7da4858cecd241a6a952e68943a31e22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc1e49bebb44345ac5eeb2d512b0615"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cb625d44fe4b1da3420b1678f9c2e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a008694a31644d63b3b20318afd4ef1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd809f5abe344e1193fe4ef9c90b981b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/787M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4465786394454e87af539f843c535895"}},"metadata":{}},{"name":"stderr","text":"Some weights of XCLIPModel were not initialized from the model checkpoint at microsoft/xclip-base-patch32 and are newly initialized: ['last.weight', 'last.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n\n开始训练\n\n1\n1\n1\n2\n189 numberofkeyframe\n\n\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/hl_detec/transformers/models/x_clip/modeling_x_clip.py:1597: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  label = torch.tensor(label).to(torch.float32)\n/kaggle/working/hl_detec/transformers/models/x_clip/modeling_x_clip.py:1599: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  loss=nn.MSELoss()(ans,torch.tensor(label).reshape(-1,1))\n","output_type":"stream"},{"name":"stdout","text":"tensor(0.3233, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2677, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n97 numberofkeyframe\n\n\ntensor(0.3831, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n292 numberofkeyframe\n\n\ntensor(0.3840, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2724, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.3032, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n150 numberofkeyframe\n\n\ntensor(0.2746, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.3254, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n59 numberofkeyframe\n\n\ntensor(0.1854, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n172 numberofkeyframe\n\n\ntensor(0.2716, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.4007, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n79 numberofkeyframe\n\n\ntensor(0.2381, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n176 numberofkeyframe\n\n\ntensor(0.3320, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.3845, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n125 numberofkeyframe\n\n\ntensor(0.3327, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.4469, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n71 numberofkeyframe\n\n\ntensor(0.3071, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n84 numberofkeyframe\n\n\ntensor(0.3138, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n241 numberofkeyframe\n\n\ntensor(0.2834, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.3340, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2434, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n73 numberofkeyframe\n\n\ntensor(0.3313, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n101 numberofkeyframe\n\n\ntensor(0.3060, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.1167, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n77 numberofkeyframe\n\n\ntensor(0.2984, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n198 numberofkeyframe\n\n\ntensor(0.2296, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2303, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n121 numberofkeyframe\n\n\ntensor(0.2694, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.1647, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n202 numberofkeyframe\n\n\ntensor(0.2719, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2543, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.0417, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n102 numberofkeyframe\n\n\ntensor(0.2791, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.0186, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n130 numberofkeyframe\n\n\ntensor(0.2693, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2768, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n346 numberofkeyframe\n\n\ntensor(0.2605, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2835, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2315, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2841, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n118 numberofkeyframe\n\n\ntensor(0.2756, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2439, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n100 numberofkeyframe\n\n\ntensor(0.2776, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n77 numberofkeyframe\n\n\ntensor(0.2926, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n137 numberofkeyframe\n\n\ntensor(0.2257, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2014, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n59 numberofkeyframe\n\n\ntensor(0.2486, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n195 numberofkeyframe\n\n\ntensor(0.1453, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.1750, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n147 numberofkeyframe\n\n\ntensor(0.1992, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2009, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n313 numberofkeyframe\n\n\ntensor(0.2235, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2305, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.1737, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.1595, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n100 numberofkeyframe\n\n\ntensor(0.1804, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n112 numberofkeyframe\n\n\ntensor(0.1993, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2963, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n79 numberofkeyframe\n\n\ntensor(0.1986, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n238 numberofkeyframe\n\n\ntensor(0.1162, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.2197, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\ntensor(0.3524, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n77 numberofkeyframe\n\n\ntensor(0.1825, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n79 numberofkeyframe\n\n\ntensor(0.2272, device='cuda:0', grad_fn=<MseLossBackward0>) 当前损失\n1\n1\n1\n2\n142 numberofkeyframe\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!fuser -v /dev/nvidia*  #清空显存.\n!kill 15821    # 这里面输入号. 号会运行这块之后出现.\n!nvidia-smi\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}