{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# nvidia p100 kaggle上运行.\n\n!git clone https://github.com/zhangbo2008/firefly\n%cd firefly\n!pip install bitsandbytes\n!pip install peft\n!pip install loguru","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-18T02:23:29.149907Z","iopub.execute_input":"2023-08-18T02:23:29.150186Z","iopub.status.idle":"2023-08-18T02:24:11.861741Z","shell.execute_reply.started":"2023-08-18T02:23:29.150155Z","shell.execute_reply":"2023-08-18T02:24:11.860246Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'firefly' already exists and is not an empty directory.\n/kaggle/working/firefly\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.41.1\nCollecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nCollecting loguru\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loguru\nSuccessfully installed loguru-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-18T02:24:11.864918Z","iopub.execute_input":"2023-08-18T02:24:11.865546Z","iopub.status.idle":"2023-08-18T02:24:12.888879Z","shell.execute_reply.started":"2023-08-18T02:24:11.865480Z","shell.execute_reply":"2023-08-18T02:24:12.887581Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#-=========参数写到这里得了:\n\n\n\n\n'''\n多卡设置\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node={num_gpus} train_qlora.py --train_args_file train_args/qlora/baichuan-7b-sft-qlora.json\\\n\n\nCUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2  6.py\n\n'''\n\n#=======设置单卡, 应该单卡也够用.\nimport os\nos.system('CUDA_VISIBLE_DEVICES=0')\n\n\nar='tmp.json'\naaa=r\"\"\"\n{\n    \"output_dir\": \"output/firefly-chatglm2-6b\",\n    \"model_name_or_path\": \"THUDM/chatglm2-6b\",\n    \"train_file\": \"./data/dummy_data.jsonl\",\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 2,\n    \"gradient_accumulation_steps\": 1,\n    \"learning_rate\": 2e-4,\n    \"max_seq_length\": 1024,\n    \"logging_steps\": 300,\n    \"save_steps\": 500,\n    \"save_total_limit\": 1,\n    \"lr_scheduler_type\": \"constant_with_warmup\",\n    \"warmup_steps\": 3000,\n    \"lora_rank\": 64,\n    \"lora_alpha\": 16,\n    \"lora_dropout\": 0.05,\n\n    \"gradient_checkpointing\": true,\n    \"disable_tqdm\": false,\n    \"optim\": \"paged_adamw_32bit\",\n    \"seed\": 42,\n    \"fp16\": true,\n    \"report_to\": \"tensorboard\",\n    \"dataloader_num_workers\": 0,\n    \"save_strategy\": \"steps\",\n    \"weight_decay\": 0,\n    \"max_grad_norm\": 0.3,\n    \"remove_unused_columns\": false\n}\n\n\n\n\n\n\n\n\"\"\"\nwith open('tmp.json','w') as f:\n    f.write(aaa)\nimport bitsandbytes\n\n\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, AdaLoraConfig,prepare_model_for_kbit_training\nfrom transformers import (\n    set_seed,\n    HfArgumentParser,\n    TrainingArguments,\n    AutoModelForCausalLM\n)\nimport argparse\nfrom loguru import logger\nimport os\nfrom os.path import join\nimport torch\nimport bitsandbytes as bnb\nfrom collections import defaultdict\n\nfrom component.collator import SFTDataCollator\nfrom component.dataset import SFTDataset, ChatGLM2SFTDataset\nfrom component.argument import QLoRAArguments\nfrom component.trainer import LoRATrainer\nfrom component.loss import TargetLMLoss\n\n\nif 1:\n\n\n\n    # 进行一些配置和检查\n\n\n\n    train_args_file = ar\n    # 读取训练的参数配置\n    parser = HfArgumentParser((QLoRAArguments, TrainingArguments))\n    # 解析得到自定义参数，以及自带参数\n\n    \n\n\n\n\n\n\n\n\n\n\n\n    args, training_args = parser.parse_json_file(json_file=train_args_file)\n    # 创建输出目录\n    if not os.path.exists(training_args.output_dir):\n        os.makedirs(training_args.output_dir)\n    # logger.add(join(training_args.output_dir, 'train.log'))\n    # logger.info(\"train_args:{}\".format(training_args))\n    # 设置随机种子\n    set_seed(training_args.seed)\n    # args, training_args = setup_everything()\n    # 加载各种组件\n\n\n    logger.info('Initializing components...')\n    # 下面的设置至关重要，否则无法多卡训练\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    print('是否ddp',ddp)\n    training_args.ddp_find_unused_parameters = False\n    device_map = \"auto\"\n    # if we are in a distributed setting, we need to set the device map and max memory per device\n    if os.environ.get('LOCAL_RANK') is not None:\n        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n        device_map = {'': local_rank}\n    device_map = {'': 0}\n    # 加载模型\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        device_map=device_map,\n        load_in_4bit=True,           #########???????????????这么加载训练精度很低吧.....\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            llm_int8_threshold=6.0,\n            llm_int8_has_fp16_weight=False,\n        ),\n    )\n    \n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T02:24:12.891841Z","iopub.execute_input":"2023-08-18T02:24:12.892297Z","iopub.status.idle":"2023-08-18T02:27:20.800364Z","shell.execute_reply.started":"2023-08-18T02:24:12.892256Z","shell.execute_reply":"2023-08-18T02:27:20.799370Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[32m2023-08-18 02:24:27.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mInitializing components...\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"是否ddp False\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309545275903456fa63be63e8ea81e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21125e45da0436083183a93c5dee6a3"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a5b9d457bf4aafb398aefb4bc6e98d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb271bf96e747408eefeeecd98b9a66"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ccb50d0398c4e47ac04db4a8801887c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5011aecd50924472a0d27a34af3bde6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26dfadefdba24646abb3d61730cae8d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c855eb54a176410e8ffee8816fb8d8cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f2bc4bf88d4038abd5b41d94d7457c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99e11884048f455f80d0211e318ae649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee689d7a3267414aab59463e6009445a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1dad3130f3460dbf32b6781bf5174e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4310ac1fcf143878142896d6f0b199f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39fc418c67b444eb9791870e51a1db3"}},"metadata":{}}]},{"cell_type":"code","source":"\nif 1:\n# 加载tokenzier\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        trust_remote_code=True,\n        # llama不支持fast\n        use_fast=False if model.config.model_type == 'llama' else True\n    )\n    # QWenTokenizer比较特殊，pad_token_id、bos_token_id、eos_token_id均为None。eod_id对应的token为<|endoftext|>\n    if tokenizer.__class__.__name__ == 'QWenTokenizer':\n        tokenizer.pad_token_id = tokenizer.eod_id\n        tokenizer.bos_token_id = tokenizer.eod_id\n        tokenizer.eos_token_id = tokenizer.eod_id\n    # ChatGLMTokenizer不需要设置，仅设置其他tokenizer\n    elif tokenizer.__class__.__name__ != 'ChatGLMTokenizer':\n        assert tokenizer.eos_token_id is not None\n        assert tokenizer.bos_token_id is not None\n        tokenizer.pad_token_id = tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n\n    # # 部分tokenizer没有pad_token_id\n    # if tokenizer.pad_token_id is None:\n    #     tokenizer.pad_token_id = tokenizer.unk_token_id\n    # # 部分tokenizer的pad_token_id与eos_token_id相同，如InternLM，会导致无法计算eos_token_id的loss。将pad_token_id设为unk_token_id\n    # if tokenizer.pad_token_id == tokenizer.eos_token_id and tokenizer.unk_token_id is not None:\n    #     tokenizer.pad_token_id = tokenizer.unk_token_id\n    # # 如果两者相同，模型训练时不会计算eos_token_id的loss\n    # if tokenizer.pad_token_id == tokenizer.eos_token_id:\n    #     raise Exception('pad_token_id should not be equal to eos_token_id')\n\n    # casts all the non int8 modules to full precision (fp32) for stability\n    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=training_args.gradient_checkpointing)\n\n    \n    print(f'memory footprint of model: {model.get_memory_footprint()/(1024*1024*1024)} GB')\n    # 找到所有需要插入adapter的全连接层\n    # target_modules = find_all_linear_names(model)\n\n    \n    \n    config = LoraConfig(\n    task_type='CAUSAL_LM', inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n    )\n    \n    \n    \n    \n    model = get_peft_model(model, config)\n    model.print_trainable_parameters()\n    model.config.torch_dtype = torch.float16\n\n    import torch\n    import torch.nn as nn\n\n    class Loss(object):\n        \"\"\"\n        所有loss的类父类\n        \"\"\"\n        def __call__(self, model, inputs, training_args, return_outputs=False):\n            \"\"\"\n            todo label smoothing\n            用于计算loss。\n            看源码发现，return_outputs=True为train时调用，return_outputs=False为eval和predict调用\n            :param model: 模型\n            :param inputs: 模型输入，dict\n            :param training_args: 训练配置参数\n            :param return_outputs:是否返回模型的输出\n            :return:\n            \"\"\"\n            raise NotImplemented\n    class TargetLMLoss(Loss):\n\n        def __init__(self, ignore_index):\n            super().__init__()\n            self.ignore_index = ignore_index\n            self.loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n        def __call__(self, model, inputs, training_args, return_outputs=False):\n            input_ids = inputs['input_ids']\n            attention_mask = inputs['attention_mask']\n            target_mask = inputs['target_mask']\n            # 模型前馈预测\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n            logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs[0]\n\n            # 将labels中不属于target的部分，设为ignore_index，只计算target部分的loss\n            labels = torch.where(target_mask == 1, input_ids, self.ignore_index)\n            shift_logits = logits[..., :-1, :].contiguous() # 因为我们生成的logits最后一个\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            return (loss, outputs) if return_outputs else loss\n\n\n\n\n    # 初始化损失函数 ########################!!!!!!!!!!!!!!!!!!!!!\n    loss_func = TargetLMLoss(ignore_index=-100)\n\n    # 指加载训练集\n    if model.config.model_type == 'chatglm':\n        train_dataset = ChatGLM2SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    else:\n        train_dataset = SFTDataset(args.train_file, tokenizer, args.max_seq_length)\n    data_collator = SFTDataCollator(tokenizer, args.max_seq_length)\n\n    # 初始化Trainer\n    trainer = LoRATrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_loss=loss_func\n    )\nif 1:\n    logger.info(\"*** starting training ***\")\n    train_result = trainer.train()\n    # 保存最好的checkpoint\n    final_save_path = join(training_args.output_dir, 'final')\n    print('保存模型')\n    trainer.save_model(final_save_path)  # Saves the tokenizer too\n    # 保存训练指标\n    metrics = train_result.metrics\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T02:27:20.806569Z","iopub.execute_input":"2023-08-18T02:27:20.807290Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8a1a5fc9264c178076c703d3cd7a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b80e6e88384e49bcf4fb8e18b64c05"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1405d7c360941169f5a97f6c92a7f85"}},"metadata":{}},{"name":"stdout","text":"memory footprint of model: 4.644905149936676 GB\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m2023-08-18 02:27:26.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mLoading data: ./data/dummy_data.jsonl\u001b[0m\n\u001b[32m2023-08-18 02:27:26.165\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcomponent.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mthere are 33 data in dataset\u001b[0m\n\u001b[32m2023-08-18 02:27:26.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m119\u001b[0m - \u001b[1m*** starting training ***\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,949,696 || all params: 3,390,261,248 || trainable%: 0.05750872447219737\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 7/17 00:56 < 01:53, 0.09 it/s, Epoch 0.35/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]}]}