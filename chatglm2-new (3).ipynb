{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 导入常用模块\nimport numpy as np\nimport pandas as pd \nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset,DataLoader \n\n\n# 配置参数\nfrom argparse import Namespace\ncfg = Namespace()\n\n#dataset\ncfg.prompt_column = 'prompt'\ncfg.response_column = 'response'\ncfg.history_column = None\ncfg.source_prefix = '' #添加到每个prompt开头的前缀引导语\n\ncfg.max_source_length = 128 \ncfg.max_target_length = 128\n\n#model\ncfg.model_name_or_path = 'THUDM/chatglm2-6b'  #远程'THUDM/chatglm-6b' \ncfg.quantization_bit = None #仅仅预测时可以选 4 or 8 \n\n\n#train\ncfg.epochs = 100 \ncfg.lr = 5e-3\ncfg.batch_size = 1\ncfg.gradient_accumulation_steps = 2 #梯度累积","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T08:51:37.346686Z","iopub.execute_input":"2023-08-17T08:51:37.347149Z","iopub.status.idle":"2023-08-17T08:51:37.358024Z","shell.execute_reply.started":"2023-08-17T08:51:37.347108Z","shell.execute_reply":"2023-08-17T08:51:37.356804Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install torchkeras","metadata":{"execution":{"iopub.status.busy":"2023-08-17T08:51:37.360324Z","iopub.execute_input":"2023-08-17T08:51:37.360868Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchkeras in /opt/conda/lib/python3.10/site-packages (3.9.3)\nRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from torchkeras) (0.20.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchkeras) (4.65.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->torchkeras) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->torchkeras) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->torchkeras) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->torchkeras) (6.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->torchkeras) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.20.3->torchkeras) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.3->torchkeras) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.3->torchkeras) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.3->torchkeras) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.3->torchkeras) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate>=0.20.3->torchkeras) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate>=0.20.3->torchkeras) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate>=0.20.3->torchkeras) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq\n\n\nconfig = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    cfg.model_name_or_path, trust_remote_code=True)\n\nmodel = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,\n                                  trust_remote_code=True, device_map='auto').half() \n\n#先量化瘦身\nif cfg.quantization_bit is not None:\n    print(f\"Quantized to {cfg.quantization_bit} bit\")\n    model = model.quantize(cfg.quantization_bit)\n    \n#再移动到GPU上\n\n\n# 通过注册jupyter魔法命令可以很方便地在jupyter中测试ChatGLM \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchkeras.chat import ChatGLM \nchatglm = ChatGLM(model,tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定义一条知识样本~\n\nkeyword = '梦中情炉'\n\ndescription = '''梦中情炉一般指的是炼丹工具torchkeras。\n这是一个通用的pytorch模型训练模版工具。\ntorchkeras是一个三好炼丹炉：好看，好用，好改。\n她有torch的灵动，也有keras的优雅，并且她的美丽，无与伦比。\n所以她的作者一个有毅力的吃货给她取了一个别名叫做梦中情炉。'''\n\n#对prompt使用一些简单的数据增强的方法，以便更好地收敛。\ndef get_prompt_list(keyword):\n    return [f'{keyword}', \n            f'你知道{keyword}吗?',\n            f'{keyword}是什么？',\n            f'介绍一下{keyword}',\n            f'你听过{keyword}吗?',\n            f'啥是{keyword}？',\n            f'{keyword}是何物？',\n            f'何为{keyword}？',\n           ]\n\ndata =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\ndfdata = pd.DataFrame(data)\ndisplay(dfdata) \nimport datasets \n#训练集和验证集一样\nds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)\n\n\n#这是支持 history列处理，并且按照batch预处理数据的方法。\n\ndef preprocess(examples):\n    max_seq_length = cfg.max_source_length + cfg.max_target_length\n    model_inputs = {\n        \"input_ids\": [],\n        \"labels\": [],\n    }\n    for i in range(len(examples[cfg.prompt_column])):\n        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n\n            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n            prompt = tokenizer.build_prompt(query, history)\n\n            prompt = cfg.source_prefix + prompt\n            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n                                     max_length=cfg.max_source_length)\n            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n                                     max_length=cfg.max_target_length)\n\n            context_length = len(a_ids)\n            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n\n            pad_len = max_seq_length - len(input_ids)\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            labels = labels + [tokenizer.pad_token_id] * pad_len\n            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n            model_inputs[\"input_ids\"].append(input_ids)\n            model_inputs[\"labels\"].append(labels)\n    return model_inputs\n\n\nds_train = ds_train_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_train_raw.column_names\n)\n\nds_val = ds_val_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_val_raw.column_names\n)\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=None,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=None,\n    padding=False\n)\n\ndl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = True, collate_fn = data_collator \n                     )\ndl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = False, collate_fn = data_collator \n                     )\n\n\nfor batch in dl_train:\n    break\n\nprint(len(dl_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import get_peft_model, AdaLoraConfig, TaskType\n\n#训练时节约GPU占用\nmodel.config.use_cache=False\nmodel.supports_gradient_checkpointing = True  #\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\npeft_config = AdaLoraConfig(\n    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n)\n\npeft_model = get_peft_model(model, peft_config)\n\npeft_model.is_parallelizable = True\npeft_model.model_parallel = True\npeft_model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchkeras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchkeras import KerasModel \nfrom accelerate import Accelerator \n\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n    \n    def __call__(self, batch):\n        \n        #loss\n        with self.accelerator.autocast():\n            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\n    \nKerasModel.StepRunner = StepRunner \n\n\n#仅仅保存lora相关的可训练参数\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \n\n\n\noptimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer) \nckpt_path = 'single_chatglm2'\n\nkeras_model.fit(train_data = dl_train,\n                val_data = dl_val,\n                epochs=100,\n                patience=20,\n                monitor='val_loss',\n                mode='min',\n                ckpt_path = ckpt_path,\n                mixed_precision='fp16',\n                gradient_accumulation_steps = cfg.gradient_accumulation_steps\n               )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel  #验证模型.\n\n# model_old = AutoModel.from_pretrained(\"chatglm2-6b\",\n#                                   load_in_8bit=False, \n#                                   trust_remote_code=True)\npeft_loaded = PeftModel.from_pretrained(model,ckpt_path)\nmodel_new = peft_loaded.merge_and_unload() #合并lora权重\n\n\nchatglm = ChatGLM(model_new,tokenizer,max_chat_rounds=20) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%chatglm\n你好","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat /tmp/ipykernel_28/2485524056.py\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}