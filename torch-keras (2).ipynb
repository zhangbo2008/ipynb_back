{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchkeras peft","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:58:30.022772Z","iopub.execute_input":"2023-08-15T07:58:30.023602Z","iopub.status.idle":"2023-08-15T07:58:44.821382Z","shell.execute_reply.started":"2023-08-15T07:58:30.023568Z","shell.execute_reply":"2023-08-15T07:58:44.820216Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchkeras\n  Downloading torchkeras-3.9.3-py3-none-any.whl (6.5 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from torchkeras) (0.20.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchkeras) (4.65.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nInstalling collected packages: torchkeras, peft\nSuccessfully installed peft-0.4.0 torchkeras-3.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n############ÂÖàÊòØÊâÄÊúâÁöÑÈÖçÁΩÆÂèÇÊï∞.\n\nimport os\n\n# ÂØºÂÖ•Â∏∏Áî®Ê®°Âùó\nimport numpy as np\n\nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset,DataLoader \n\n\n# ÈÖçÁΩÆÂèÇÊï∞\nfrom argparse import Namespace\ncfg = Namespace()\n\n#dataset\ncfg.prompt_column = 'prompt'\ncfg.response_column = 'response'\ncfg.history_column =None\ncfg.source_prefix = '' #Ê∑ªÂä†Âà∞ÊØè‰∏™promptÂºÄÂ§¥ÁöÑÂâçÁºÄÂºïÂØºËØ≠\n\ncfg.max_source_length = 128 \ncfg.max_target_length = 128\n\n#model\ncfg.model_name_or_path = 'THUDM/chatglm2-6b'  #ËøúÁ®ã'THUDM/chatglm-6b' \ncfg.quantization_bit = None #‰ªÖ‰ªÖÈ¢ÑÊµãÊó∂ÂèØ‰ª•ÈÄâ 4 or 8 \n\n\n#train\ncfg.epochs = 100 \ncfg.lr = 5e-3\ncfg.batch_size = 2\ncfg.gradient_accumulation_steps = 1 #Ê¢ØÂ∫¶Á¥ØÁßØ\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available()else \"cpu\") \n\n\n\n\n\n\n#==========ÂÆö‰πâÁü•ËØÜÊ†∑Êú¨.######ÂÖàÂ§ÑÁêÜÊàë‰ª¨ÁöÑÊï∞ÊçÆ.\nfrom torch.utils.data import Dataset,DataLoader \nimport transformers\nfrom transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq\ntokenizer = AutoTokenizer.from_pretrained(\n    cfg.model_name_or_path, trust_remote_code=True)\nimport transformers\nfrom transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq\n\n\nimport pandas as pd \nkeyword = 'Ê¢¶‰∏≠ÊÉÖÁÇâ'\n\ndescription = '''Ê¢¶‰∏≠ÊÉÖÁÇâ‰∏ÄËà¨ÊåáÁöÑÊòØÁÇº‰∏πÂ∑•ÂÖ∑torchkeras„ÄÇ\nËøôÊòØ‰∏Ä‰∏™ÈÄöÁî®ÁöÑpytorchÊ®°ÂûãËÆ≠ÁªÉÊ®°ÁâàÂ∑•ÂÖ∑„ÄÇ\ntorchkerasÊòØ‰∏Ä‰∏™‰∏âÂ•ΩÁÇº‰∏πÁÇâÔºöÂ•ΩÁúãÔºåÂ•ΩÁî®ÔºåÂ•ΩÊîπ„ÄÇ\nÂ•πÊúâtorchÁöÑÁÅµÂä®Ôºå‰πüÊúâkerasÁöÑ‰ºòÈõÖÔºåÂπ∂‰∏îÂ•πÁöÑÁæé‰∏ΩÔºåÊó†‰∏é‰º¶ÊØî„ÄÇ\nÊâÄ‰ª•Â•πÁöÑ‰ΩúËÄÖ‰∏Ä‰∏™ÊúâÊØÖÂäõÁöÑÂêÉË¥ßÁªôÂ•πÂèñ‰∫Ü‰∏Ä‰∏™Âà´ÂêçÂè´ÂÅöÊ¢¶‰∏≠ÊÉÖÁÇâ„ÄÇ'''\n\n\n\n\n#ÂØπprompt‰ΩøÁî®‰∏Ä‰∫õÁÆÄÂçïÁöÑÊï∞ÊçÆÂ¢ûÂº∫ÁöÑÊñπÊ≥ïÔºå‰ª•‰æøÊõ¥Â•ΩÂú∞Êî∂Êïõ„ÄÇ\ndef get_prompt_list(keyword):\n    return [f'{keyword}', \n            f'‰Ω†Áü•ÈÅì{keyword}Âêó?',\n            f'{keyword}ÊòØ‰ªÄ‰πàÔºü',\n            f'‰ªãÁªç‰∏Ä‰∏ã{keyword}',\n            f'‰Ω†Âê¨Ëøá{keyword}Âêó?',\n            f'Âï•ÊòØ{keyword}Ôºü',\n            f'{keyword}ÊòØ‰ΩïÁâ©Ôºü',\n            f'‰Ωï‰∏∫{keyword}Ôºü',\n           ]\n\ndata =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\ndfdata = pd.DataFrame(data)\n\n\n\n\n\nimport datasets \n#ËÆ≠ÁªÉÈõÜÂíåÈ™åËØÅÈõÜ‰∏ÄÊ†∑\nds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)\n#ËøôÊòØÊîØÊåÅ historyÂàóÂ§ÑÁêÜÔºåÂπ∂‰∏îÊåâÁÖßbatchÈ¢ÑÂ§ÑÁêÜÊï∞ÊçÆÁöÑÊñπÊ≥ï„ÄÇ\n\ndef preprocess(examples):\n    max_seq_length = cfg.max_source_length + cfg.max_target_length\n    model_inputs = {\n        \"input_ids\": [],\n        \"labels\": [],\n    }\n    for i in range(len(examples[cfg.prompt_column])):\n        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n\n            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n            prompt = tokenizer.build_prompt(query, history)\n\n            prompt = cfg.source_prefix + prompt\n            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n                                     max_length=cfg.max_source_length)\n            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n                                     max_length=cfg.max_target_length)\n\n            context_length = len(a_ids)\n            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n\n            pad_len = max_seq_length - len(input_ids)\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            labels = labels + [tokenizer.pad_token_id] * pad_len\n            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n            model_inputs[\"input_ids\"].append(input_ids)\n            model_inputs[\"labels\"].append(labels)\n    return model_inputs\n\n\nds_train = ds_train_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_train_raw.column_names\n)\n\nds_val = ds_val_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_val_raw.column_names\n)\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=None,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=None,\n    padding=False\n)\n\ndl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = True, collate_fn = data_collator \n                     )\ndl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = False, collate_fn = data_collator \n                     )\n\n\n\n\nprint(len(dl_train))\n\n\n\n\nconfig = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)\n\n\n\nmodel = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,\n                                  trust_remote_code=True, device_map='auto').half() #==========16‰ΩçÁî®Êù•gpuËÆ≠ÁªÉ.ËÆæÂ§á‰∏ÄÂÆöÂÜôauto,Ëá™Âä®ÈÖçÁΩÆÊòæÂç°ÂíåÂÜÖÂ≠ò.\n\n#ÂÖàÈáèÂåñÁò¶Ë∫´  =======ÊµãËØïÊó∂ÂÄôÂèØ‰ª•Áî®Ëøô‰∏™. ‰∏çËßÅ‰∏ÄÂºÄÂêØ.Èô§ÈùûÈÖçÁΩÆ ÁâπÂà´Â∑Æ.\nif cfg.quantization_bit is not None:\n    print(f\"Quantized to {cfg.quantization_bit} bit\")\n    model = model.quantize(cfg.quantization_bit)\n    \n#ÂÜçÁßªÂä®Âà∞GPU‰∏ä\n# model = model.cuda();\n\n\n# # ÈÄöËøáÊ≥®ÂÜåjupyterÈ≠îÊ≥ïÂëΩ‰ª§ÂèØ‰ª•ÂæàÊñπ‰æøÂú∞Âú®jupyter‰∏≠ÊµãËØïChatGLM \n# from torchkeras.chat import ChatGLM \n# chatglm = ChatGLM(model,tokenizer)\n\nprint('ÊµãËØï‰∏Ä‰∏ãÊòØÂê¶Âä†ËΩΩÊàêÂäü')\nresponse,history= model.chat(tokenizer,query='‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØ‰ªÄ‰πàÔºü',history=[])\nprint(response)\n\n\n\n\n#ÂÆö‰πâ‰∏ÄÊù°Áü•ËØÜÊ†∑Êú¨~#===========================\n\n\nfrom peft import get_peft_model, AdaLoraConfig, TaskType\n\n#ËÆ≠ÁªÉÊó∂ËäÇÁ∫¶GPUÂç†Áî®\nmodel.config.use_cache=False\nmodel.supports_gradient_checkpointing = True  #\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\npeft_config = AdaLoraConfig(\n    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n)\n\npeft_model = get_peft_model(model, peft_config)\n\npeft_model.is_parallelizable = True\npeft_model.model_parallel = True\npeft_model.print_trainable_parameters()\n\n\n\n\n#================over.\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-15T07:58:44.825651Z","iopub.execute_input":"2023-08-15T07:58:44.826026Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a23fb65e1b4e4faf0bf0efec7907e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe9f3808542141f58e673662a23fd898"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffbf41646cb24574b306525f40a00a65"}},"metadata":{}},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd1115a523a4e7aa0cb0c3e97e93067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e3b7e56f694baaae2da8aa49e9527e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475b9572e6af4a7a9b5fb5fe27eec619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8301146e0104f4e8d864d7769475c9c"}},"metadata":{}},{"name":"stdout","text":"        ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba0c8e7197ca4ef3bef2c0d9db338571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6583b16718874cacb33a7782980a6746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f297bbb722a54c65b4060ca67b6626e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b7592f465f4ef8a54d65d8d98d49e8"}},"metadata":{}},{"name":"stdout","text":"4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02cb3f89b37e468cb921e244463ec39d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b667b9344ca14506b67ce6564418261b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320336b88a794777955b6222efb0e16d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f94a01a5024b4391b83777b974deda1b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b818c19982fd4a19aa37502535ae9e54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"606a33d8c36f4f3ca815b88b8d4b6d6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf09f9a91e6840d89dc62d84faa5a636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (‚Ä¶)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2094c492e634f54b449c32a67c767ed"}},"metadata":{}}]},{"cell_type":"code","source":"\nimport sys,datetime\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\n\n#=========ËÆæÁΩÆÊâìÂç∞‰ø°ÊÅØÁöÑ.\nclass EpochRunner:\n    def __init__(self,steprunner,quiet=False):\n        self.steprunner = steprunner\n        self.stage = steprunner.stage\n        self.accelerator = steprunner.accelerator\n        self.net = steprunner.net\n        self.quiet = quiet\n        \n    def __call__(self,dataloader):\n        n = dataloader.size  if hasattr(dataloader,'size') else len(dataloader)\n        loop = tqdm(enumerate(dataloader,start=1), \n                    total=n,\n                    file=sys.stdout,\n                    disable=not self.accelerator.is_local_main_process or self.quiet,\n                    ncols=100\n                   )\n        epoch_losses = {}\n        \n        for step, batch in loop: \n            with self.accelerator.accumulate(self.net):\n                step_losses,step_metrics = self.steprunner(batch)   \n                step_log = dict(step_losses,**step_metrics)\n\n                for k,v in step_losses.items():\n                    epoch_losses[k] = epoch_losses.get(k,0.0)+v\n                \n          #=============ÊâìÂç∞ËÆ≠ÁªÉÊó•Âøó.\n                print('ÂΩìÂâçstep')\n                if step<n:\n                    loop.set_postfix(**step_log)\n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**step_log)\n                        self.progress.set_postfix(**post_log)\n\n                elif step==n:\n                    epoch_metrics = step_metrics\n                    epoch_metrics.update({self.stage+\"_\"+name:metric_fn.compute().item() \n                                     for name,metric_fn in self.steprunner.metrics_dict.items()})\n                    epoch_losses = {k:v/step for k,v in epoch_losses.items()}\n                    epoch_log = dict(epoch_losses,**epoch_metrics)\n                    loop.set_postfix(**epoch_log)\n            \n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**epoch_log)\n                        self.progress.set_postfix(**post_log)\n                    \n                    for name,metric_fn in self.steprunner.metrics_dict.items():\n                        metric_fn.reset()  \n                else:\n                    break\n        print(55555,epoch_log)\n        return epoch_log\n\n\n#===============‰øÆÊîπ‰∏ãÈù¢‰ª£Á†Å‰∏∫Ëá™Â∑±Ë∑ë. Êù•‰ºòÂåñÊÄßËÉΩ:\n\nfrom accelerate import Accelerator \n#============torchkerasÊù•ÂÜôËÆ≠ÁªÉ‰ª£Á†ÅÊûúÁÑ∂ÁâõÈÄº,ÂõæÊ†áÂ§™ÁâõÈÄº‰∫Ü.\n#======Á¨¨‰∏ÄÊ≠•ËÆæÁΩÆÂ•ΩËá™ÂÆö‰πâÁöÑKerasModel\nflag=0\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n        self.flag=0\n    \n    def __call__(self, batch):\n        \n        #loss\n        global flag\n        if 0:\n#           if not flag: #=======Êàë‰ª¨ÊâìÂç∞Á¨¨‰∏Ä‰∏™ËæìÂÖ•ÂèòÈáèÁöÑÊï∞ÊçÆ,Êñπ‰æøÁêÜËß£Êï∞ÊçÆÈõÜ.\n            print('Êü•ÁúãÁ¨¨‰∏Ä‰∏™batch',batch)\n            flag=1\n        with self.accelerator.autocast():\n            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n#=========================‰ªéËøôÂæÄ‰∏ãÁöÑÂÖ®ÊòØÂõ∫ÂÆöÂÜôÊ≥ï‰∏çÁî®Âä®.\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\nclass KerasModel(torch.nn.Module):\n    \n    StepRunner,EpochRunner = StepRunner,EpochRunner\n    \n    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None,tokenizer=None):\n        super().__init__()\n        self.net,self.loss_fn,self.metrics_dict = net, loss_fn, torch.nn.ModuleDict(metrics_dict) \n        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n            self.net.parameters(), lr=3e-4)\n        self.lr_scheduler = lr_scheduler\n        self.from_scratch = True     #Ê≤°ÊúâÂä†ËΩΩÂä†ËΩΩÈ¢ÑÂÖàÁöÑÊùÉÈáç.#ÂàùÂßãÂåñÊó∂ÂÄôÊ≤°Âä†ËΩΩ, scratcchÊòØËçâÂõæÁöÑÊÑèÊÄùË°®Á§∫Ê≤°ÊúâÊùÉÈáçÂú®ÁΩëÁªúÈáåÈù¢.\n    #########=============‰∏ÄËà¨‰∏çÁî®‰∏ãÈù¢Ëøô2‰∏™‰øùÂ≠òÂä†ËΩΩ, ÈÄÇÈÖçÊÄß‰∏çÂ§ü.\n    def save_ckpt(self, ckpt_path=None, accelerator= None):\n        accelerator = accelerator if accelerator is not None else self.accelerator\n        net_dict = accelerator.get_state_dict(self.net)\n        accelerator.save(net_dict,ckpt_path if ckpt_path is not None else self.ckpt_path)\n      \n    def load_ckpt(self, ckpt_path=None):\n        self.net.load_state_dict(\n            torch.load(ckpt_path if ckpt_path is not None else self.ckpt_path,\n            map_location='cpu'))\n        self.from_scratch = False\n\n    def forward(self, x):\n        return self.net.forward(x)\n    \n    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=False,  wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1,dfhistorypath='dfhistory.csv'):\n        from torchkeras.utils import colorful,is_jupyter\n        self.__dict__.update(locals())\n        self.accelerator = Accelerator(mixed_precision=mixed_precision,cpu=cpu,\n            gradient_accumulation_steps=gradient_accumulation_steps)\n        device = str(self.accelerator.device)\n        device_type = 'üêå'  if 'cpu' in device else ('‚ö°Ô∏è' if 'cuda' in device else 'üöÄ')\n        self.accelerator.print(\n            colorful(\"<<<<<< \"+device_type +\" \"+ device +\" is used >>>>>>\"))\n    \n        self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler= self.accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler)\n        \n        train_dataloader,val_dataloader = self.accelerator.prepare(train_data,val_data)\n        train_dataloader.size = train_data.size if hasattr(train_data,'size') else len(train_data)\n        train_dataloader.size = min(train_dataloader.size,len(train_dataloader))\n        \n        if val_data:\n            val_dataloader.size = val_data.size if hasattr(val_data,'size') else len(val_data)\n            val_dataloader.size = min(val_dataloader.size,len(val_dataloader))\n        \n        self.history = {}\n        callbacks = callbacks if callbacks is not None else []\n        \n        if bool(plot):\n            from torchkeras.kerascallbacks import VisProgress,VisMetric\n            callbacks = [VisMetric(),VisProgress()]+callbacks\n            \n        if wandb!=False:\n            from torchkeras.kerascallbacks import WandbCallback\n            project = wandb if isinstance(wandb,str) else 'torchkeras'\n            callbacks.append(WandbCallback(project=project))\n            \n        self.callbacks = [self.accelerator.prepare(x) for x in callbacks]\n        \n        if self.accelerator.is_local_main_process:\n            [cb.on_fit_start(model = self) for cb in self.callbacks if hasattr(cb,'on_fit_start')]\n                \n        start_epoch = 1 if self.from_scratch else 0\n        \n        if bool(plot) or quiet is None:\n            quiet = True\n        \n        quiet_fn = (lambda epoch:quiet) if isinstance(quiet,bool) else (\n            (lambda epoch:epoch>quiet) if isinstance(quiet,int) else quiet)\n        #==========================ËÆ≠ÁªÉ.\n        for epoch in range(start_epoch,epochs+1):\n            if 0:\n                should_quiet = quiet_fn(epoch)\n            \n                if not should_quiet:\n                    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                    self.accelerator.print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n                    self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs)+\"\\n\")\n\n            # 1Ôºåtrain -------------------------------------------------  \n            train_step_runner = self.StepRunner(    #ËÆ≠ÁªÉ‰∏Ä‰∏™step\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"train\",\n                    metrics_dict=deepcopy(self.metrics_dict),\n                    optimizer = self.optimizer if epoch>0 else None,\n                    lr_scheduler = self.lr_scheduler if epoch>0 else None\n            )\n            should_quiet=True\n            train_epoch_runner = self.EpochRunner(train_step_runner,should_quiet)\n            train_metrics = {'epoch':epoch}\n            print('111111')\n            train_metrics.update(train_epoch_runner(train_dataloader))\n            print(train_metrics)\n            for name, metric in train_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n            #==================Ë∞ÉÁî®callbackÂáΩÊï∞!!!!!!!!!\n            if 0:\n                if self.accelerator.is_local_main_process: #=================420ÂáΩÊï∞ÁöÑÂê´‰πâÂ∞±ÊòØË∞ÉÁî®ÂÖ®ÈÉ®ÁöÑself.callbacksÂáΩÊï∞!!!!!!!!\n                    [cb.on_train_epoch_end(model = self) for cb in self.callbacks \n                    if hasattr(cb,'on_train_epoch_end')]\n                    \n            # 2Ôºåvalidate -------------------------------------------------\n            if val_dataloader is not None:\n                val_step_runner = self.StepRunner(\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"val\",\n                    metrics_dict= deepcopy(self.metrics_dict)\n                )\n                val_epoch_runner = self.EpochRunner(val_step_runner,should_quiet)\n                with torch.no_grad():\n                    val_metrics = val_epoch_runner(val_dataloader)\n\n                for name, metric in val_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n                \n            if self.accelerator.is_local_main_process:\n                [cb.on_validation_epoch_end(model = self) for cb in self.callbacks \n                 if hasattr(cb,'on_validation_epoch_end')]\n\n            # 3Ôºåearly-stopping -------------------------------------------------\n            if 1: #======ËøôÈÉ®ÂàÜÈÄªËæë‰∏çÂ§™ÂØπÂïä.#‰øùÂ≠òÂ§™ÂØÜÈõÜ‰∫Ü.Êàë‰øÆÊîπÊéâ‰øùÂ≠òÁöÑ.\n                self.accelerator.wait_for_everyone()\n                arr_scores = self.history[monitor]\n                best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n\n\n\n                if len(arr_scores)-best_score_idx>patience:\n                    break\n                \n        if self.accelerator.is_local_main_process:   \n            dfhistory = pd.DataFrame(self.history)\n            # [cb.on_fit_end(model = self) for cb in self.callbacks \n            #      if hasattr(cb,'on_fit_end')]\n            if epoch<epochs:\n                self.accelerator.print(colorful(\n                        \"<<<<<< {} without improvement in {} epoch,\"\"early stopping >>>>>> \\n\"\n                    ).format(monitor,patience))\n            # self.net = self.accelerator.unwrap_model(self.net)\n            # self.net.cpu()\n\n            dfhistory = pd.DataFrame(model.history)\n            dfhistory.to_csv(self.dfhistorypath,index=None)\n            # self.load_ckpt(ckpt_path)\n            return dfhistory\n    def predict(self,batch):\n\n        accelerator = Accelerator() if not hasattr(self,'accelerator') else self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n        val_data = accelerator.prepare(val_data)\n        with torch.no_grad():\n            a=self.StepRunner.net(input_ids=batch[\"input_ids\"])\n\n\n        return a\n\n    def evaluate(self, val_data, quiet=False):\n        accelerator = Accelerator() if not hasattr(self,'accelerator') else self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n        val_data = accelerator.prepare(val_data)\n        val_step_runner = self.StepRunner(net = self.net,stage=\"val\",\n                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n                    accelerator = accelerator)\n        val_epoch_runner = self.EpochRunner(val_step_runner,quiet=quiet)\n        with torch.no_grad():\n            val_metrics = val_epoch_runner(val_data)\n        return val_metrics\n    \n    def fit_ddp(self,num_processes,train_data,\n            val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=True, wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1\n           ):\n        from accelerate import notebook_launcher\n        args = (train_data,val_data,epochs,ckpt_path,patience,monitor,mode,\n            callbacks,plot,wandb,quiet,mixed_precision,cpu,gradient_accumulation_steps)\n        notebook_launcher(self.fit, args, num_processes=num_processes)\n    \n    def evaluate_ddp(self, num_processes, val_data, quiet=False):\n        from accelerate import notebook_launcher\n        args = (val_data,quiet)\n        notebook_launcher(self.evaluate, args, num_processes=num_processes)\n\n\n\n\n\n\n\n\n\n    \nKerasModel.StepRunner = StepRunner \n\n\n#‰ªÖ‰ªÖ‰øùÂ≠òloraÁõ∏ÂÖ≥ÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \noptimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n\n#########Á¨¨‰∫åÊ≠•ÂÆû‰æãÂåñmodel\nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer) \nckpt_path = 'chatglm2_my' #===========‰øùÂ≠òÁöÑË∑ØÂæÑ.\n#=========Á¨¨‰∏âÈÉ®‰∏ãÈù¢ÂáΩÊï∞Ëá™Âä®ËÆ≠ÁªÉ, ÁîªÂõæ, ÂíåÂ≠òÊ®°Âûã.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys,datetime\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom accelerate import Accelerator\n\n#=========ËÆæÁΩÆÊâìÂç∞‰ø°ÊÅØÁöÑ.\nclass EpochRunner:\n    def __init__(self,steprunner,quiet=False):\n        self.steprunner = steprunner\n        self.stage = steprunner.stage\n        self.accelerator = steprunner.accelerator\n        self.net = steprunner.net\n        self.quiet = quiet\n        \n    def __call__(self,dataloader):\n        n = dataloader.size  if hasattr(dataloader,'size') else len(dataloader)\n        loop = tqdm(enumerate(dataloader,start=1), \n                    total=n,\n                    file=sys.stdout,\n                    disable=not self.accelerator.is_local_main_process or self.quiet,\n                    ncols=100\n                   )\n        epoch_losses = {}\n        \n        for step, batch in loop: \n            with self.accelerator.accumulate(self.net):\n                step_losses,step_metrics = self.steprunner(batch)   \n                step_log = dict(step_losses,**step_metrics)\n                print(step_losses.items())\n                for k,v in step_losses.items():\n                    epoch_losses[k] = epoch_losses.get(k,0.0)+v\n          #=============ÊâìÂç∞ËÆ≠ÁªÉÊó•Âøó.\n                if step<n:\n                    loop.set_postfix(**step_log)\n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**step_log)\n                        self.progress.set_postfix(**post_log)\n\n                elif step==n:\n    \n                    epoch_metrics = step_metrics\n                    epoch_metrics.update({self.stage+\"_\"+name:metric_fn.compute().item() \n                                     for name,metric_fn in self.steprunner.metrics_dict.items()})\n                    epoch_losses = {k:v/step for k,v in epoch_losses.items()}\n                    epoch_log = dict(epoch_losses,**epoch_metrics)\n                    loop.set_postfix(**epoch_log)\n            \n                    \n                    if hasattr(self,'progress') and self.accelerator.is_local_main_process:\n                        post_log = dict(**{'i':step,'n':n},**epoch_log)\n                        self.progress.set_postfix(**post_log)\n                    \n                    for name,metric_fn in self.steprunner.metrics_dict.items():\n                        metric_fn.reset()  \n                else:\n                    break\n        return epoch_log\n\n\n#===============‰øÆÊîπ‰∏ãÈù¢‰ª£Á†Å‰∏∫Ëá™Â∑±Ë∑ë. Êù•‰ºòÂåñÊÄßËÉΩ:\n\nfrom accelerate import Accelerator \n#============torchkerasÊù•ÂÜôËÆ≠ÁªÉ‰ª£Á†ÅÊûúÁÑ∂ÁâõÈÄº,ÂõæÊ†áÂ§™ÁâõÈÄº‰∫Ü.\n#======Á¨¨‰∏ÄÊ≠•ËÆæÁΩÆÂ•ΩËá™ÂÆö‰πâÁöÑKerasModel\nflag=0\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n        self.flag=0\n\n    \n    def __call__(self, batch):\n        \n        #loss\n        global flag\n        if not flag: #=======Êàë‰ª¨ÊâìÂç∞Á¨¨‰∏Ä‰∏™ËæìÂÖ•ÂèòÈáèÁöÑÊï∞ÊçÆ,Êñπ‰æøÁêÜËß£Êï∞ÊçÆÈõÜ.\n            \n            flag=1\n        with self.accelerator.autocast():\n            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n#=========================‰ªéËøôÂæÄ‰∏ãÁöÑÂÖ®ÊòØÂõ∫ÂÆöÂÜôÊ≥ï‰∏çÁî®Âä®.\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\nclass KerasModel(torch.nn.Module):\n    \n    StepRunner,EpochRunner = StepRunner,EpochRunner\n    \n    def __init__(self,net,loss_fn,metrics_dict=None,optimizer=None,lr_scheduler = None,tokenizer=None,mixed_precision=None,cpu=None,gradient_accumulation_steps=None):\n        super().__init__()\n        self.net,self.loss_fn,self.metrics_dict = net, loss_fn, torch.nn.ModuleDict(metrics_dict) \n        self.optimizer = optimizer if optimizer is not None else torch.optim.Adam(\n            self.net.parameters(), lr=3e-4)\n        self.lr_scheduler = lr_scheduler\n        self.from_scratch = True     #Ê≤°ÊúâÂä†ËΩΩÂä†ËΩΩÈ¢ÑÂÖàÁöÑÊùÉÈáç.#ÂàùÂßãÂåñÊó∂ÂÄôÊ≤°Âä†ËΩΩ, scratcchÊòØËçâÂõæÁöÑÊÑèÊÄùË°®Á§∫Ê≤°ÊúâÊùÉÈáçÂú®ÁΩëÁªúÈáåÈù¢.\n        \n        self.accelerator= Accelerator(mixed_precision=mixed_precision,cpu=cpu,\n            gradient_accumulation_steps=gradient_accumulation_steps)\n    #########=============‰∏ÄËà¨‰∏çÁî®‰∏ãÈù¢Ëøô2‰∏™‰øùÂ≠òÂä†ËΩΩ, ÈÄÇÈÖçÊÄß‰∏çÂ§ü.\n    def save_ckpt(self, ckpt_path=None, accelerator= None):\n        accelerator = accelerator if accelerator is not None else self.accelerator\n        net_dict = accelerator.get_state_dict(self.net)\n        accelerator.save(net_dict,ckpt_path if ckpt_path is not None else self.ckpt_path)\n      \n    def load_ckpt(self, ckpt_path=None):\n        self.net.load_state_dict(\n            torch.load(ckpt_path if ckpt_path is not None else self.ckpt_path,\n            map_location='cpu'))\n        self.from_scratch = False\n\n    def forward(self, x):\n        return self.net.forward(x)\n    \n    def fit(self, train_data, val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=False,  wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1,dfhistorypath='dfhistory.csv'):\n        from torchkeras.utils import colorful,is_jupyter\n        self.__dict__.update(locals())\n\n        device = str(self.accelerator.device)\n        device_type = 'üêå'  if 'cpu' in device else ('‚ö°Ô∏è' if 'cuda' in device else 'üöÄ')\n        self.accelerator.print(\n            colorful(\"<<<<<< \"+device_type +\" \"+ device +\" is used >>>>>>\"))\n    \n        self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler= self.accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict,self.optimizer,self.lr_scheduler)\n        \n        train_dataloader,val_dataloader = self.accelerator.prepare(train_data,val_data)\n        train_dataloader.size = train_data.size if hasattr(train_data,'size') else len(train_data)\n        train_dataloader.size = min(train_dataloader.size,len(train_dataloader))\n        \n        if val_data:\n            val_dataloader.size = val_data.size if hasattr(val_data,'size') else len(val_data)\n            val_dataloader.size = min(val_dataloader.size,len(val_dataloader))\n        \n        self.history = {}\n        callbacks = callbacks if callbacks is not None else []\n        \n        if bool(plot):\n            from torchkeras.kerascallbacks import VisProgress,VisMetric\n            callbacks = [VisMetric(),VisProgress()]+callbacks\n            \n        if wandb!=False:\n            from torchkeras.kerascallbacks import WandbCallback\n            project = wandb if isinstance(wandb,str) else 'torchkeras'\n            callbacks.append(WandbCallback(project=project))\n            \n        self.callbacks = [self.accelerator.prepare(x) for x in callbacks]\n        \n        if self.accelerator.is_local_main_process:\n            [cb.on_fit_start(model = self) for cb in self.callbacks if hasattr(cb,'on_fit_start')]\n                \n        start_epoch = 1 if self.from_scratch else 0\n        \n        if bool(plot) or quiet is None:\n            quiet = True\n        \n        quiet_fn = (lambda epoch:quiet) if isinstance(quiet,bool) else (\n            (lambda epoch:epoch>quiet) if isinstance(quiet,int) else quiet)\n        #==========================ËÆ≠ÁªÉ.\n        for epoch in range(start_epoch,epochs+1):\n            if 0:\n                should_quiet = quiet_fn(epoch)\n            \n                if not should_quiet:\n                    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n                    self.accelerator.print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n                    self.accelerator.print(\"Epoch {0} / {1}\".format(epoch, epochs)+\"\\n\")\n            should_quiet=True\n            # 1Ôºåtrain -------------------------------------------------  \n            train_step_runner = self.StepRunner(    #ËÆ≠ÁªÉ‰∏Ä‰∏™step\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"train\",\n                    metrics_dict=deepcopy(self.metrics_dict),\n                    optimizer = self.optimizer if epoch>0 else None,\n                    lr_scheduler = self.lr_scheduler if epoch>0 else None\n            )\n\n            train_epoch_runner = self.EpochRunner(train_step_runner,should_quiet)\n            train_metrics = {'epoch':epoch}\n            train_metrics.update(train_epoch_runner(train_dataloader))\n\n            for name, metric in train_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n            #==================Ë∞ÉÁî®callbackÂáΩÊï∞!!!!!!!!!\n            if 0:\n                if self.accelerator.is_local_main_process: #=================420ÂáΩÊï∞ÁöÑÂê´‰πâÂ∞±ÊòØË∞ÉÁî®ÂÖ®ÈÉ®ÁöÑself.callbacksÂáΩÊï∞!!!!!!!!\n                    [cb.on_train_epoch_end(model = self) for cb in self.callbacks \n                    if hasattr(cb,'on_train_epoch_end')]\n                    \n            # 2Ôºåvalidate -------------------------------------------------\n            if val_dataloader is not None:\n                val_step_runner = self.StepRunner(\n                    net = self.net,\n                    loss_fn = self.loss_fn,\n                    accelerator = self.accelerator,\n                    stage=\"val\",\n                    metrics_dict= deepcopy(self.metrics_dict)\n                )\n                val_epoch_runner = self.EpochRunner(val_step_runner,should_quiet)\n                with torch.no_grad():\n                    val_metrics = val_epoch_runner(val_dataloader)\n\n                for name, metric in val_metrics.items():\n                    self.history[name] = self.history.get(name, []) + [metric]\n                \n            if self.accelerator.is_local_main_process:\n                [cb.on_validation_epoch_end(model = self) for cb in self.callbacks \n                 if hasattr(cb,'on_validation_epoch_end')]\n\n            # 3Ôºåearly-stopping -------------------------------------------------\n            if 1: #======ËøôÈÉ®ÂàÜÈÄªËæë‰∏çÂ§™ÂØπÂïä.#‰øùÂ≠òÂ§™ÂØÜÈõÜ‰∫Ü.Êàë‰øÆÊîπÊéâ‰øùÂ≠òÁöÑ.\n                self.accelerator.wait_for_everyone()\n                arr_scores = self.history[monitor]\n                best_score_idx = np.argmax(arr_scores) if mode==\"max\" else np.argmin(arr_scores)\n\n\n\n                if len(arr_scores)-best_score_idx>patience:\n                    break\n                \n        if self.accelerator.is_local_main_process:   \n            dfhistory = pd.DataFrame(self.history)\n            # [cb.on_fit_end(model = self) for cb in self.callbacks \n            #      if hasattr(cb,'on_fit_end')]\n            if epoch<epochs:\n                self.accelerator.print(colorful(\n                        \"<<<<<< {} without improvement in {} epoch,\"\"early stopping >>>>>> \\n\"\n                    ).format(monitor,patience))\n            # self.net = self.accelerator.unwrap_model(self.net)\n            # self.net.cpu()\n\n#             dfhistory = pd.DataFrame(model.history)\n            dfhistory.to_csv(self.dfhistorypath,index=None)\n            # self.load_ckpt(ckpt_path)\n            return dfhistory\n#=====================!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!ÂÆûÁé∞È¢ÑÊµã‰ª£Á†Å.\n    def predict(self,q,max_length):\n\n        self.net.eval()\n        accelerator = self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n\n        with accelerator.autocast() , torch.no_grad():\n\n            a=self.net.chat(tokenizer,query=q,history=[],max_length=max_length)\n            print(a,'debug!!!!!!!!!!!')\n\n        return a\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    def evaluate(self, val_data, quiet=False):\n        accelerator = Accelerator() if not hasattr(self,'accelerator') else self.accelerator\n        self.net,self.loss_fn,self.metrics_dict = accelerator.prepare(\n            self.net,self.loss_fn,self.metrics_dict)\n        val_data = accelerator.prepare(val_data)\n        val_step_runner = self.StepRunner(net = self.net,stage=\"val\",\n                    loss_fn = self.loss_fn,metrics_dict=deepcopy(self.metrics_dict),\n                    accelerator = accelerator)\n        val_epoch_runner = self.EpochRunner(val_step_runner,quiet=quiet)\n        with torch.no_grad():\n            val_metrics = val_epoch_runner(val_data)\n        return val_metrics\n    \n    def fit_ddp(self,num_processes,train_data,\n            val_data=None, epochs=10, ckpt_path='checkpoint',\n            patience=5, monitor=\"val_loss\", mode=\"min\", callbacks=None, \n            plot=True, wandb=False, quiet=None, \n            mixed_precision='no', cpu=False, gradient_accumulation_steps=1\n           ):\n        from accelerate import notebook_launcher\n        args = (train_data,val_data,epochs,ckpt_path,patience,monitor,mode,\n            callbacks,plot,wandb,quiet,mixed_precision,cpu,gradient_accumulation_steps)\n        notebook_launcher(self.fit, args, num_processes=num_processes)\n    \n    def evaluate_ddp(self, num_processes, val_data, quiet=False):\n        from accelerate import notebook_launcher\n        args = (val_data,quiet)\n        notebook_launcher(self.evaluate, args, num_processes=num_processes)\n\n\n\n\n\n\n\n\n\n    \nKerasModel.StepRunner = StepRunner \n\n\n#‰ªÖ‰ªÖ‰øùÂ≠òloraÁõ∏ÂÖ≥ÁöÑÂèØËÆ≠ÁªÉÂèÇÊï∞\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \noptimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n\n#########Á¨¨‰∫åÊ≠•ÂÆû‰æãÂåñmodel\nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer, mixed_precision='fp16',cpu=False,\n            gradient_accumulation_steps=cfg.gradient_accumulation_steps) \nckpt_path = 'chatglm2_my' #===========‰øùÂ≠òÁöÑË∑ØÂæÑ.\n#=========Á¨¨‰∏âÈÉ®‰∏ãÈù¢ÂáΩÊï∞Ëá™Âä®ËÆ≠ÁªÉ, ÁîªÂõæ, ÂíåÂ≠òÊ®°Âûã.\n\nprint('ÈÖçÁΩÆÂÆåÊØï')\nif 1: # ÊµãËØï\n\n        print('ËÆ≠ÁªÉ‰πãÂâçÂºÄÂßãÊµãËØï')\n        print(keras_model.predict('Ê¢¶‰∏≠ÊÉÖÁÇâ',max_length=200)[0])\n        print(keras_model.predict('‰∏ñÁïå‰∏äÊúÄÈ´òÁöÑÂ±±Â≥∞ÊòØ‰ªÄ‰πà',max_length=200)[0])\n\nif 1:#ËÆ≠ÁªÉ\n    keras_model.fit(train_data = dl_train,\n                val_data = dl_train,\n                epochs=20,\n                patience=20,\n                monitor='val_loss',\n                mode='min',\n                ckpt_path = ckpt_path,\n\n                plot=False, # ‰∏çÁîªÁîªËäÇÁúÅÁ©∫Èó¥.\n          \n               )\nif 1: # ÊµãËØï\n\n        print('ËÆ≠ÁªÉ‰πãÂêéÂºÄÂßãÊµãËØï')\n        print(keras_model.predict('Ê¢¶‰∏≠ÊÉÖÁÇâ',max_length=200)[0])\n        \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#=========Êü•ÁúãÊï∞ÊçÆ.\nprint(len(dl_train))\n# for batch in dl_train:\n#     print(batch)\nfor i in ds_train_raw:\n    print(i)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}