{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 导入常用模块\nimport numpy as np\nimport pandas as pd \nimport torch\nfrom torch import nn \nfrom torch.utils.data import Dataset,DataLoader \n\n\n# 配置参数\nfrom argparse import Namespace\ncfg = Namespace()\n\n#dataset\ncfg.prompt_column = 'prompt'\ncfg.response_column = 'response'\ncfg.history_column = None\ncfg.source_prefix = '' #添加到每个prompt开头的前缀引导语\n\ncfg.max_source_length = 128 \ncfg.max_target_length = 128\n\n#model\ncfg.model_name_or_path = 'THUDM/chatglm-6b'  #远程'THUDM/chatglm-6b' \ncfg.quantization_bit = None #仅仅预测时可以选 4 or 8 \n\n\n#train\ncfg.epochs = 100 \ncfg.lr = 5e-3\ncfg.batch_size = 1\ncfg.gradient_accumulation_steps = 2 #梯度累积","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-17T08:25:04.559485Z","iopub.execute_input":"2023-08-17T08:25:04.560062Z","iopub.status.idle":"2023-08-17T08:25:04.571136Z","shell.execute_reply.started":"2023-08-17T08:25:04.560019Z","shell.execute_reply":"2023-08-17T08:25:04.570129Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq\n\n\nconfig = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    cfg.model_name_or_path, trust_remote_code=True)\n\nmodel = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,\n                                  trust_remote_code=True, device_map='auto').half() \n\n#先量化瘦身\nif cfg.quantization_bit is not None:\n    print(f\"Quantized to {cfg.quantization_bit} bit\")\n    model = model.quantize(cfg.quantization_bit)\n    \n#再移动到GPU上\n\n\n# 通过注册jupyter魔法命令可以很方便地在jupyter中测试ChatGLM \nfrom torchkeras.chat import ChatGLM \nchatglm = ChatGLM(model,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T08:25:04.572756Z","iopub.execute_input":"2023-08-17T08:25:04.573046Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab68d180fe66411591af337fcf8b2926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/4.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e1a9206c004d9b830e822bffd1126a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e8e57968c64f70af496ba308d391a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/17.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e9861df2951454abd3bdad72747852c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading ice_text.model:   0%|          | 0.00/2.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5902b9f24a5845428c83067c99652c88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/57.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8acdf21d46f44dc4b1f0697ec0b0c1ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e015a798974b5da8267aee9a99402e"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927cf1df299e4473a3e3097d0e904f1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0323abef3b4ec2a46b8d743310e698"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00008.bin:   0%|          | 0.00/1.74G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f6976edfa6426a886ee3b7926b1120"}},"metadata":{}}]},{"cell_type":"code","source":"#定义一条知识样本~\n\nkeyword = '梦中情炉'\n\ndescription = '''梦中情炉一般指的是炼丹工具torchkeras。\n这是一个通用的pytorch模型训练模版工具。\ntorchkeras是一个三好炼丹炉：好看，好用，好改。\n她有torch的灵动，也有keras的优雅，并且她的美丽，无与伦比。\n所以她的作者一个有毅力的吃货给她取了一个别名叫做梦中情炉。'''\n\n#对prompt使用一些简单的数据增强的方法，以便更好地收敛。\ndef get_prompt_list(keyword):\n    return [f'{keyword}', \n            f'你知道{keyword}吗?',\n            f'{keyword}是什么？',\n            f'介绍一下{keyword}',\n            f'你听过{keyword}吗?',\n            f'啥是{keyword}？',\n            f'{keyword}是何物？',\n            f'何为{keyword}？',\n           ]\n\ndata =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\ndfdata = pd.DataFrame(data)\ndisplay(dfdata) \nimport datasets \n#训练集和验证集一样\nds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)\n\n\n#这是支持 history列处理，并且按照batch预处理数据的方法。\n\ndef preprocess(examples):\n    max_seq_length = cfg.max_source_length + cfg.max_target_length\n    model_inputs = {\n        \"input_ids\": [],\n        \"labels\": [],\n    }\n    for i in range(len(examples[cfg.prompt_column])):\n        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n\n            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n            prompt = tokenizer.build_prompt(query, history)\n\n            prompt = cfg.source_prefix + prompt\n            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n                                     max_length=cfg.max_source_length)\n            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n                                     max_length=cfg.max_target_length)\n\n            context_length = len(a_ids)\n            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n\n            pad_len = max_seq_length - len(input_ids)\n            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n            labels = labels + [tokenizer.pad_token_id] * pad_len\n            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n            model_inputs[\"input_ids\"].append(input_ids)\n            model_inputs[\"labels\"].append(labels)\n    return model_inputs\n\n\nds_train = ds_train_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_train_raw.column_names\n)\n\nds_val = ds_val_raw.map(\n    preprocess,\n    batched=True,\n    num_proc=4,\n    remove_columns=ds_val_raw.column_names\n)\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=None,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=None,\n    padding=False\n)\n\ndl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = True, collate_fn = data_collator \n                     )\ndl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n                      num_workers = 2, shuffle = False, collate_fn = data_collator \n                     )\n\n\nfor batch in dl_train:\n    break\n\nprint(len(dl_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import get_peft_model, AdaLoraConfig, TaskType\n\n#训练时节约GPU占用\nmodel.config.use_cache=False\nmodel.supports_gradient_checkpointing = True  #\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\npeft_config = AdaLoraConfig(\n    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n    r=8,\n    lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query\", \"value\"]\n)\n\npeft_model = get_peft_model(model, peft_config)\n\npeft_model.is_parallelizable = True\npeft_model.model_parallel = True\npeft_model.print_trainable_parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchkeras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchkeras import KerasModel \nfrom accelerate import Accelerator \n\nclass StepRunner:\n    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n                 optimizer = None, lr_scheduler = None\n                 ):\n        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n        self.accelerator = accelerator if accelerator is not None else Accelerator() \n        if self.stage=='train':\n            self.net.train() \n        else:\n            self.net.eval()\n    \n    def __call__(self, batch):\n        \n        #loss\n        with self.accelerator.autocast():\n            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n\n        #backward()\n        if self.optimizer is not None and self.stage==\"train\":\n            self.accelerator.backward(loss)\n            if self.accelerator.sync_gradients:\n                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n            self.optimizer.step()\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step()\n            self.optimizer.zero_grad()\n            \n        all_loss = self.accelerator.gather(loss).sum()\n        \n        #losses (or plain metrics that can be averaged)\n        step_losses = {self.stage+\"_loss\":all_loss.item()}\n        \n        #metrics (stateful metrics)\n        step_metrics = {}\n        \n        if self.stage==\"train\":\n            if self.optimizer is not None:\n                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n            else:\n                step_metrics['lr'] = 0.0\n        return step_losses,step_metrics\n    \nKerasModel.StepRunner = StepRunner \n\n\n#仅仅保存lora相关的可训练参数\ndef save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n    unwrap_net = accelerator.unwrap_model(self.net)\n    unwrap_net.save_pretrained(ckpt_path)\n    \ndef load_ckpt(self, ckpt_path='checkpoint'):\n    self.net = self.net.from_pretrained(self.net.base_model.model,ckpt_path)\n    self.from_scratch = False\n    \nKerasModel.save_ckpt = save_ckpt \nKerasModel.load_ckpt = load_ckpt \n\n\n\noptimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \nkeras_model = KerasModel(peft_model,loss_fn = None,\n        optimizer=optimizer) \nckpt_path = 'single_chatglm2'\n\nkeras_model.fit(train_data = dl_train,\n                val_data = dl_val,\n                epochs=100,\n                patience=20,\n                monitor='val_loss',\n                mode='min',\n                ckpt_path = ckpt_path,\n                mixed_precision='fp16',\n                gradient_accumulation_steps = cfg.gradient_accumulation_steps\n               )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel  #验证模型.\n\n# model_old = AutoModel.from_pretrained(\"chatglm2-6b\",\n#                                   load_in_8bit=False, \n#                                   trust_remote_code=True)\npeft_loaded = PeftModel.from_pretrained(model,ckpt_path)\nmodel_new = peft_loaded.merge_and_unload() #合并lora权重\n\n\nchatglm = ChatGLM(model_new,tokenizer,max_chat_rounds=20) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%chatglm\n你好","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}